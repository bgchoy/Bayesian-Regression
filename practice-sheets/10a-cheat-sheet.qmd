---
title: "Cheat sheet: common things to do with BRMS"
subtitle: "Bayesian regression: theory & practice"
author: "Michael Franke"
format: html
execute:
  error: false
  warning: false
  message: false
  cache: true
callout-appearance: simple
editor:
  markdown:
    wrap: sentence
---

This document provides a cursory run-down of common operations and manipulations for working with the `brms` package.

# Preamble

{{< include 00-preamble.qmd >}}

# Running a regression model

As a running example, we fit a multi-level model.

```{r}
#| results: hide

data_MC <- aida::data_MC_preprocessed |> 
      mutate(condition = factor(as.character(block), 
                                levels = c("goNoGo", "reaction", "discrimination")))

fit_MC <- 
  brms::brm(
    # "brmsformula" object specifies the model to fit
    formula = RT ~ condition + (1 + condition + shape | submission_id),
    # data to fit model to
    data = data_MC,
    # "iter" is the number of iterations
    iter = 4000,
    # control parameters for MCMC
    control = list(adapt_delta = 0.9)
  )
```

## Updating a model

Using `stats::update()`, refit a model based on an existing model fit, keeping everything as is, except for what is explicitly set:

```{r}
#| eval: false

# take existing fit, refit on smaller data set, just take 100 samples (all else equal)
fit_first_five <- 
  stats::update(
    object = fit_MC,
    iter = 100,
    # use first five participants only
    newdata = data_MC |> filter(submission_id >= 8550)
  )

```

## Formula syntax

The basic form of a `brms` formula is: `response ~ pterms + (gterms | group)`

### Multi-level modeling

- `(gterms || group)` :  suppress correlation between gterms
- `(gterms | g1 + g2)` :  syntactic sugar for `(gterms | g1) + (gterms | g2)`
- `(gterms | g1 : g2)` :  all combinations of `g1` and `g2` (Cartesian product)
- `(gterms | g1 / g2)` :  nesting `g2` within `g1`; equals `(gterms | g1) + (gterms | g1 : g2)`
- `(gterms | IDx | group)` : correlation for all group-level categories with `IDx`
  - useful for multi-formula models (e.g., non-linear models)


# General information about the model fit

## Summaries

Standard summary of the model fit:

```{r}
summary(fit_MC)
```

Same in tidy format:

```{r}
tidybayes::summarise_draws(fit_MC)
```

Summary of just the fixed effects:

```{r}
brms::fixef(fit_MC)
```

Summary of just the random effects (this is huge, so output suppressed):

```{r}
#| eval: false
brms::ranef(fit_MC)
```

## Retrieve names of model variables

```{r }
tidybayes::get_variables(fit_MC)[1:10]
```

## Explore via `shinystan`

```{r}
#| eval: false
shinystan::launch_shinystan(fit_MC)
```

# MCMC diagnostics

Retrieve R-hat numerically (so you can calculate it or include it in a reproducible document):

```{r}
brms::rhat(fit_MC) |> head()
```

Retrieve ration of efficient samples numerically (so you can calculate it or include it in a reproducible document):

```{r}
brms::neff_ratio(fit_MC) |> head()
```

Retrieve all per-sample diagnostics from a fitted object:

```{r}
fit_MC |> 
  tidybayes::tidy_draws() |> 
  dplyr::select(ends_with("__"))
```

Retrieve all samples with divergent transitions:

```{r}
fit_MC |> 
  tidybayes::tidy_draws() |> 
  dplyr::select(ends_with("__")) |> 
  filter(divergent__ == TRUE)
```


[fill me]{style="color:darkgreen"}

# Extracting samples

## Tidy samples with `tidybayes`

Retrieve all samples with `tidybayes::tidy_draws()`:

```{r}
tidybayes::tidy_draws(fit_MC)
```

## Getting summaries for samples

To get (Bayesian) summary statistics for a vector of samples from a parameter you can do this:

```{r}
posterior_Intercept <- 
  tidybayes::tidy_draws(fit_MC) |> 
  dplyr::pull("b_Intercept")
```

The `tidybayes::hdi` function gives the upper and lower bound of a Bayesian credible interval:

```{r}
tidybayes::hdi(posterior_Intercept, credMass = 0.90)
```

The function `aida::summarize_sample_vector` does so, too.

```{r}
aida::summarize_sample_vector(posterior_Intercept, name = "Intercept")
```

Here is how you can do this for several vectors at once:

```{r}
tidybayes::tidy_draws(fit_MC) |> 
  dplyr::select(starts_with("b_")) |> 
  pivot_longer(cols = everything()) |> 
  group_by(name) |> 
  reframe(aida::summarize_sample_vector(value)[-1])
```

# Plotting posteriors

## Population-level parameters

To plot the posteriors over model paramters, you can use various plots from the `bayesplot` package (here information [here](http://mc-stan.org/bayesplot/)):

```{r}
posterior_draws <- brms::as_draws_matrix(fit_MC)[,c("b_conditionreaction", "b_conditiondiscrimination")]
bayesplot::mcmc_areas(posterior_draws)
```

Or, use the `tidybayes` package:

```{r}
fit_MC |> 
  tidy_draws() |> 
  select(starts_with("b_con")) |> 
  rename(reaction = b_conditionreaction,
         discrimination = b_conditiondiscrimination) |> 
  pivot_longer(cols = everything()) |> 
  ggplot(aes(x = value, y = name)) +
  tidybayes::stat_halfeye(fill = project_colors[1]) +
  ylab("") +
  geom_vline(aes(xintercept = 0), color = project_colors[2], size = 2)
```

## Group-level parameters

Here is an example of plotting the posterior for random effects (here: the by-subject random intercepts):

```{r}

ranef(fit_MC)$submission_id[,,"Intercept"] |> 
  as.data.frame() |> 
  rownames_to_column("submission_id") |> 
  ggplot(aes(y = submission_id, x = Estimate)) +
  geom_errorbar(aes(xmin = `Q2.5`, xmax = `Q97.5`), 
                color = project_colors[6], alpha = 0.7)+
  geom_vline(aes(xintercept = 0), color = project_colors[1], 
             size = 2, alpha = 0.8) +
  geom_point(color = project_colors[2], size = 2)

```


# Posterior predictives

## Visual PPCs

Use tools from the `bayesplot` package.
The basic `bayesplot::pp_check()` plots the distribution of `ndraws` samples from the posterior (data) predictive against the distribution of the data the model was trained on:

```{r}
bayesplot::pp_check(fit_MC, ndraws = 30)
```

There are many tweaks to `pp_check`:

```{r}
bayesplot::pp_check(fit_MC, ndraws = 30, type = "hist")
```

You can also directly use underlying functions like `ppc_stat`.
See `help("PPC-overview")` and `help("PPD-overview")` for what is available.

```{r}
predictive_samples <- brms::posterior_predict(fit_MC, ndraws = 1000)
predictive_samples[1:5, 1:5] 

bayesplot::ppc_stat(
  y    = data_MC$RT, 
  yrep = predictive_samples,
  # specify the test statistic of interest
  stat = function(x){quantile(x, 0.8)})
```

## Extracting samples from the posterior predictive distribution

There are three kinds of commonly relevant variables a generalized linear model predicts:

1.  the central tendency of data $y$ for some predictor $x$,
2.  the shape of the (hypothetical) data $y'$ for $x$, and
3.  a linear predictor value given values of $x$.

All of these measures can be obtained from a fitted model with different functions, e.g., from the `tidyverse` package.
Here, it does not matter whether the model was fitted to data or it is a "prior model", so to speak, fit with the flag `sample_prior = "only"`.

Here is an example for a logistic regression model (where all the three measures clearly show their conceptual difference).

```{r}
#| results: hide
fit_MT_logistic <- 
  brms::brm(
    formula = correct ~ group * condition,
    data    = aida::data_MT,
    family  = brms::bernoulli()
  )
```

```{r}
# 2 samples from the predicted central tendency
aida::data_MT |> 
  dplyr::select(group, condition) |> 
  unique() |> 
  tidybayes::add_epred_draws(
    fit_MT_logistic,
    ndraws = 2
    )

# 2 samples from the predictive distribution (data samples)
aida::data_MT |> 
  dplyr::select(group, condition) |> 
  unique() |> 
  tidybayes::add_predicted_draws(
    fit_MT_logistic,
    ndraws = 2
    )

# 2 samples for the linear predictor
aida::data_MT |> 
  dplyr::select(group, condition) |> 
  unique() |> 
  tidybayes::add_linpred_draws(
    fit_MT_logistic,
    ndraws = 2
    )
```

# Priors

## Inspecting default priors without running the model

```{r}
# define the model as a "brmsformula" object
myFormula <- brms::bf(RT ~ 1 + condition + (1 + condition | submission_id))

# get prior information
brms::get_prior(
  formula = myFormula,
  data    = data_MC,
  family  = exgaussian()
  )
```

## Setting priors

```{r}
#| eval: false

myPrior <- 
  brms::prior("normal(30,100)",  class = "b", coef = "conditiondiscrimination") +
  brms::prior("normal(-30,100)", class = "b", coef = "conditionreaction")

fit_with_specified_prior <- 
  brms::brm(
    formula = myformula,
    data    = data_MC,
    prior   = myPrior,
    family  = exgaussian()
  )
```

## Sampling from the prior

```{r}
#| eval: false

fit_samples_from_prior_only <- 
brms::brm(
  formula = myformula,
  data    = data_MC,
  prior   = myPrior,
  family  = exgaussian(),
  sample_prior = "only"
)
```

# Under the hood: Stan code, design matrices etc.

## Extract the Stan code

```{r}
brms::stancode(fit_MC)
```

## Extract Stan data

This is the data passed to Stan.
Useful for inspecting dimensions etc.

```{r}
brms::standata(fit_MC) |> names()
```

## Inspect design matrices

### Population-level effects

```{r}
X <- brms::standata(fit_MC)$X
X |> head()
```

### Group-level effects

The group-level design matrix is spread out over different variables (all names `Z_` followed by some indices), but retrievable like so:

```{r}
data4Stan <- brms::standata(fit_MC)
Z <- data4Stan[str_detect(data4Stan |> names(), "Z_")] |> as_tibble()
Z
```
