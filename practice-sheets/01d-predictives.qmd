---
title: "Prior and posterior predictives"
subtitle: "Bayesian regression: theory & practice"
author: "Michael Franke"
format: html
editor: visual
execute:
  error: false
  warning: false
  message: false
  cache: true
callout-appearance: simple
---

The tutorial introduces the concept of "predictive distribution". There usually three kinds of commonly relevant predictions: (i) linear predictor value, (ii) the central tendency, and (iii) the data. The tutorial shows how to collect samples for all of these. As predictive distributions are important from the prior and posterior point of view, the tutorial also shows how to use the prior predictive distributions as intuition-fuel for the choice of prior.

# Preamble

{{< include 00-preamble.qmd >}}

# Model predictions

Usually, when we think about what predictions a model makes, we think about the predictions it makes about the data, e.g., which data points $y'$ is deems likely after having been conditioned on some data $y$. These are the types of predictions that usually matter. They address the question: what will the future be like?

But complex models, with various latent variables, not only make predictions about data $y$ but also about all computational steps from parameters $\theta$ to data $y$, so to speak.

A vanilla linear model (whether before or after being conditioned on some data), makes two kinds of predictions, namely:

1.  the shape of the (hypothetical) data $y'$ for $x$, and
2.  the central tendency of data $y$ for some predictor $x$.

Generalized linear models often also disassociate a prediction of central tendency (point 2 above), from the linear predictor that is used to compute that prediction of central tendency. So, *all* linear models also predict:

3.  a linear predictor value given values of $x$,

but for some linear models (with the identity function as a link function), there is no difference between 1 and 3.

When we speak of the \*posterior predictive distribution\* we usually mean predictions about data $y'$, but the term can be (leniently) also applied to the latter two types of predictions.

# Predictive samples

Samples for all of the three types of posterior predictive distributions can be obtained from a fitted model, e.g., with different functions from the `tidyverse` package. Here, it does not matter whether the model was fitted to data or it is a "prior model", so to speak, fit with the flag `sample_prior = "only"`. We look at the posterior case first, then at the prior predicitives.

## Posterior predictives

Here is an example for a logistic regression model (where all the three measures clearly show their conceptual difference). Fit the model to some data first (here: predicting accuracy for two categorical factors with two levels each):

```{r}
#| results: hide
fit_MT_logistic <- 
  brms::brm(
    formula = correct ~ group * condition,
    data    = aida::data_MT,
    family  = brms::bernoulli()
  )
```

The posterior predictive (in the most general sense) makes predictions about the to-be-expected data, here a Boolean value of whether a response was correct.

```{r}
# 2 samples from the predictive distribution (data samples)
data_MT |> 
  select(group, condition) |> 
  unique() |> 
  tidybayes::add_predicted_draws(
    fit_MT_logistic,
    ndraws = 2
    )
```

A predicted central tendency for this logistic model is a probability of giving a correct answer.

```{r}

# 2 samples from the predicted central tendency
data_MT |> 
  select(group, condition) |> 
  unique() |> 
  tidybayes::add_epred_draws(
    fit_MT_logistic,
    ndraws = 2
    )
```

Predictions at the linear predictor level are sometimes not so easy to interpret. The interpretation depends on the kind of link function used (more on this under the topic of "generalized linear models"). For a logistic regression, this number is a log-odds ratio (which determines the predicted correctness-probability).

```{r}
# 2 samples for the linear predictor
data_MT |> 
  select(group, condition) |> 
  unique() |> 
  tidybayes::add_linpred_draws(
    fit_MT_logistic,
    ndraws = 2
    )
```

## Prior predictives

To sample from the prior predictive we first need to initialize the model, setting the option `sample_prior = "only"`. It is necessary to specify priors for all parameters that would otherwise be improper.

```{r}
#| results: hide
fit_MT_logistic_prior <- 
  brms::brm(
    formula = correct ~ group * condition,
    data    = aida::data_MT,
    family  = brms::bernoulli(),
    # rather unspecific priors
    prior   = prior(student_t(3,0,2.5)),
    # tell BRMS to not condition on the data
    sample_prior = "only"
  )
```

Obtaining samples from the prior predictive distributions is then the same as before (from this point on it doesn't matter whether the model was trained on data or not). For example, here are two samples from the prior predictive (data) distribution:

```{r}
# 2 samples from the predictive distribution (data samples)
data_MT |> 
  select(group, condition) |> 
  unique() |> 
  tidybayes::add_predicted_draws(
    fit_MT_logistic_prior,
    ndraws = 2
    )
```

Notice that *a posteriori* correct trials are predicted to be very likely, but *a priori* they are not more likely than incorrect ones. - Wait, how do we know? Let's poke a bit.

::: {.callout-caution collapse="false"}
## Exercise 1

Obtain sufficient samples from the prior predictive distribution for the central tendency (the predicted probability of correctness) to address the question of whether this model, as specified above, initially predicts correct and false answers to be equally likely. Use Bayesian summary statistics to corroborate your claim.

::: {.callout-tip collapse="true"}
### Solution

Let's obtain 4000 samples for each different quadruple of $x$ values and summarize the resulting samples:

```{r}
data_MT |> 
  select(group, condition) |> 
  tidybayes::add_epred_draws(
    fit_MT_logistic_prior,
    ndraws = 4000
    ) |> ungroup() |> 
  group_by(group, condition) |> 
  reframe(aida::summarize_sample_vector(.epred)[,-1])
```

The expected value of the posterior predictive is indeed around 0.5, but there is total uncertainty (about the accuracy).
:::
:::

::: {.callout-caution collapse="false"}
## Exercise \[SPECIAL\]

I wonder why the same code, but using `tidybayes::hdi` does not work as expected. If anyone knows or finds out, please share.

::: {.callout-tip collapse="true"}
### Solution

```{r}
data_MT |> 
  select(group, condition) |> 
  tidybayes::add_epred_draws(
    fit_MT_logistic_prior,
    ndraws = 4000
    ) |> ungroup() |>
  group_by(group, condition) |> 
  summarize(
    lower  = tidybayes::hdi(.epred, .width = 0.95)[1],
    mean   = mean(.epred),
    higher = tidybayes::hdi(.epred, .width = 0.95)[2]
  )
```
:::
:::

# Prior and posterior predictives

## Prior plausibility

Checking a model's prior predictive distributions is an integral part of the Bayesian workflow. It is not always apparent what a particular choice of prior entails for the model's other parameters, or its prior (data) predictive. Let's explore how we can test implications of prior choice visually.

We are using the fit of a linear model to the (scaled) average world temperature data for the year 2025 to 2024. The function `plot_predictPriPost` below allows you to specify a prior for the model's paramters for which it will show samples from the model's prior and posterior prediction of the measure of central tendency, as well as the data. Plotting exercises like these inform you about how strong or biased your priors are, whether they are reasonably in line with your intentions, and whether they seem to inform the posterior strongly (an informal, punctuated sensitivity analysis).

```{r}

plot_predictPriPost <- function(prior_spec, ndraws = 1000) {
  
  # get the posterior fit
  fit <- brm(
    avg_temp ~ year,
    prior = prior_spec,
    data = aida::data_WorldTemp,
    silent = TRUE,
    refresh = 0
  )
  
  # retrieve prior samples from the posterior fit
  fit_prior_only <- update(
    fit,
    silent = TRUE,
    refresh = 0,
    sample_prior = "only"
  )
  
  get_predictions <- function(fit_object, type = "prior prediction") {
    
    tidybayes::add_epred_draws(
      fit_object, 
      newdata = tibble(year = aida::data_WorldTemp$year),
      ndraws = ndraws,
      value = 'avg_tmp'
    ) |> 
      ungroup() |> 
      select(year, .draw, avg_tmp) |> 
      mutate(type = type)
    
  }
  
  get_predictions(fit, "posterior prediction") |> 
    rbind(get_predictions(fit_prior_only, "prior prediction")) |> 
    mutate(type = factor(type, levels = c("prior prediction", "posterior prediction"))) |> 
    ggplot() + 
    facet_grid(type ~ ., scales = "free") +
    geom_line(aes(x = year, y = avg_tmp, group = .draw), 
              color = "gray", alpha = 0.3) +
    geom_point(data = aida::data_WorldTemp, 
               aes(x = year, y = avg_temp), color = project_colors[2], size = 1, alpha = 0.8) +
    ylab("average temperature")
}

prior_baseline <- c(prior("normal(0, 0.02)", class = "b"),
                    prior("student_t(3, 8, 5)", class = "Intercept"))
plot_predictPriPost(prior_baseline)

prior_opinionated <- c(prior("normal(0.2, 0.05)", class = "b"),
                       prior("student_t(3, 8, 5)", class = "Intercept"))
plot_predictPriPost(prior_opinionated)

prior_crazy <- c(prior("normal(-1, 0.005)", class = "b"),
                 prior("student_t(3, 8, 5)", class = "Intercept"))
plot_predictPriPost(prior_crazy)

```

::: {.callout-caution collapse="false"}
## Exercise 2

Test different prior specifications, and inspect the resulting prior and posterior predictions. This is just to build your intuitions, and also to help you try out different kinds of prior probability distributions (try a lower- or upper-bounded distribution, if you dare).
:::

::: {.callout-caution collapse="false"}
## Exercise 3

Set up a similar plotting pipeline for another model, e.g., the logistic regression model used above, or (ideally) a model you really care about.
:::

## Visual predictive checks

Let's have a closer look at prior and posterior predictives, and the functions that we can use to explore them. Here, we fit a regression model with the "opinionated priors" from above, obtaining both posterior and prior samples for it.

```{r}
#| results: hide

fit_posterior <- brm(
    avg_temp ~ year,
    prior = prior_opinionated,
    data = aida::data_WorldTemp
  )
```

```{r}
#| results: hide

fit_prior <- stats::update(
    fit_posterior,
    sample_prior = "only"
  )
```

The `bayesplot` package has a number of visual predictive check functions nested inside the function `pp_check`. Here are examples.

Without additional argument `pp_check` compares the overal observed distribution of the repsonse variable to the prior/posterior predictive distribution. Check the observed distribution (marginal of $y$) first:

```{r}
aida::data_WorldTemp |> 
  ggplot(aes(x = avg_temp)) + geom_density()
```

The prior predictive check shows that this prior is way less "opinionated" or biased than its name may suggest:

```{r}
brms::pp_check(fit_prior, ndraws = 50)
```

The posterior predictive check can reveal systematic problems with the model, such as here: an inability to capture the bimodal-ish shape of the data.

```{r}
brms::pp_check(fit_posterior, ndraws = 50)
```

There are number of different plots `pp_check` is able to produce. For fine-grained plotting and exploring, the `bayesplot` package offers flexible plotting tools. These come in pairs: *predicitve distributions* only show the predictions, while *predictive checks* also show the data. See `help("PPC-overview")` and `help("PPD-overview")` for more information.

The general workflow is that you first extract samples from the relevant predictive distribution (in matrix form), like so:

```{r}
predictive_samples <- brms::posterior_predict(fit_posterior, ndraws = 1000)
predictive_samples[1:5, 1:5] 
```

And then you can, for example, compare the distribution of some test statistic (here: the standard deviation), using a function like `ppc_stat`:

```{r}
bayesplot::ppc_stat(
  y    = aida::data_WorldTemp$avg_temp, 
  yrep = predictive_samples,
  stat = sd)
```

::: {.callout-caution collapse="false"}
## Exercise 4

Interpret this plot.

::: {.callout-tip collapse="true"}
### Solution

The light blue histogram indicates the distribution of the values of the test statistics under the predictive distribution (here: posterior). The darker blue line indicates the value of the test statistic for the observed data.

In this case, the observed test value is rather central in the posterior predictive distribution, thus suggesting that, as far as the standard deviation is concerned, the model cannot be criticized for its posterior predictions.
:::
:::

::: {.callout-caution collapse="false"}
## Exercise 5

Try a similar `ppc_stat` plot for the prior predictive. Can you find a test statistic for which the model looks adequate?

::: {.callout-tip collapse="true"}
### Solution

Looking at the prior predicted mean is not too bad (at least visually).

```{r}
predictive_samples <- brms::posterior_predict(fit_prior, ndraws = 1000)
bayesplot::ppc_stat(
  y    = aida::data_WorldTemp$avg_temp, 
  yrep = predictive_samples,
  stat = mean)
```

That is because the predictions are very wide. There is nothing wrong about that! But, of course, another criterion that the prior predictive distribution blatantly fails is to predict the deviation in the data adequately (again this is, arguably, how it should be if we want to learn from the data):

```{r}
bayesplot::ppc_stat(
  y    = aida::data_WorldTemp$avg_temp, 
  yrep = predictive_samples,
  stat = sd)
```
:::
:::

## Bayesian $p$-values

Using model predictions, we can also compute Bayesian $p$-values as handy summary statistics for visual predictive checks. A single number never replaces the information we obtain from (good) plots, but is easier to communicate and may help interpretation (though should never solely dominate decision making).

Let's focus on the posterior model for the temperature data and apply a rigorous (data-informed) test statistic: the standard deviation for the data observation up to 1800.

```{r}
postPred_y <- 
  tidybayes::predicted_draws(
    object  = fit_posterior,
    newdata = aida::data_WorldTemp |> select(year) |> filter(year <= 1800),
    value   = "avg_temp",
    ndraws  = 4000) |> ungroup() |> 
  select(.draw,year, avg_temp)

sd_postPred <- postPred_y |> 
  group_by(.draw) |> 
  summarize(sd_post_pred = sd(avg_temp)) |> 
  pull(sd_post_pred)

sd_data <- aida::data_WorldTemp |> filter(year <= 1800) |> pull(avg_temp) |> sd()

mean(sd_data > sd_postPred)
```

::: {.callout-caution collapse="false"}
## Exercise 6: Predictive $p$-values w/ SD as test statistic

Make sure you understand how the code in the last code block works. Interpret the numerical result.

::: {.callout-tip collapse="true"}
### Solution

We see an estimated $p$-value of close to one, which is really bad (for the chosen test statistic). It means that the model never predicts data with a value of the test statistic that is that extreme. Notice that "extremeness" here means "very high or very low". So in this case, we would clearly have ground of accusing the model to fail to predict the aspect captured by this test statistic.
:::
:::

::: {.callout-caution collapse="false"}
## Exercise 7: \[ambitious\] Likelihood as test statistic

Use the code above to calculate a Bayesian $p$ value for the same data and model but assuming that the likelihood of the data is the test statistic. Note that `brms::log_lik` is a handy function for obtaining the likelihood of some $y'$ --be it observed, predicted or made up-- given a model (prior or posterior)

Interpret the result you get (also in relation to the results from the previous exercise).

::: {.callout-tip collapse="true"}
### Solution

```{r}
#| echo: false

get_LH <- function(avg_temp, ndraws = 1000) {
  LH_ys <- brms::log_lik(
    object  = fit_posterior,
    newdata = tibble(avg_temp = avg_temp, 
                     year = aida::data_WorldTemp$year),
    ndraws  = ndraws)
  mean(matrixStats::rowLogSumExps(LH_ys) - log(dim(LH_ys)[2]))
}

postPred_y <- 
  tidybayes::predicted_draws(
    object  = fit_posterior,
    newdata = aida::data_WorldTemp |> select(year),
    value   = "avg_temp",
    ndraws  = 100) |> ungroup() |> 
  select(.draw, year, avg_temp)

LH_postPred <- postPred_y |> 
  group_by(.draw) |> 
  summarize(LH_post_pred = get_LH(avg_temp)) |> 
  pull(LH_post_pred)

LH_data <- get_LH(aida::data_WorldTemp$avg_temp, ndraws = 1000)

mean(LH_data > LH_postPred)
```

[try yourself; solution will follow]{style="color:darkgreen"}
:::
:::

<!-- ## Snippets -->

<!-- Here is the mouse-tracking data set we used previously for simple linear regression. -->

<!-- ```{r} -->

<!-- dolphin <- aida::data_MT -->

<!-- # aggregate -->

<!-- dolphin_agg <- dolphin |>  -->

<!--   filter(correct == 1) |>  -->

<!--   group_by(subject_id) |>  -->

<!--   dplyr::summarize( -->

<!--             AUC = median(AUC, na.rm = TRUE), -->

<!--             MAD = median(MAD, na.rm = TRUE))  -->

<!-- ``` -->

<!-- Here is a plot to remind ourselves. -->

<!-- ```{r} -->

<!-- # plot temperature data -->

<!-- dolphin_agg |>  -->

<!--   ggplot(aes(x = MAD, y = AUC)) + -->

<!--   geom_point(color = project_colors[2]) -->

<!-- ``` -->

<!-- ::: callout-caution -->

<!-- **Exercise 3a** -->

<!-- Obtain a model fit for `AUC ~ MAD` with a prior for the slope coefficient as a Student-t distribution with 1 degree of freedom, mean 0 and standard deviation 500. -->

<!-- ::: -->

<!-- ```{r} -->

<!-- #| eval: true -->

<!-- #| code-fold: true -->

<!-- #| code-summary: "Show solution" -->

<!-- fit_dolphin_agg <- brm( -->

<!--   AUC ~ MAD,  -->

<!--   data = dolphin_agg, -->

<!--   prior = prior(student_t(1,0,500), class = "b") -->

<!--   ) -->

<!-- ``` -->

<!-- Here is how we can extract and plot three samples from the posterior predictive distribution. So, these are three "fake" data sets of the same size and for the same `MAD` values as in the original data. -->

<!-- ```{r} -->

<!-- # extract & plot posterior predictives -->

<!-- post_pred <- tidybayes::predicted_draws( -->

<!--   object = fit_dolphin_agg, -->

<!--   newdata = dolphin_agg |> select(MAD), -->

<!--   value = "AUC", -->

<!--   ndraws = 3 -->

<!-- ) |>  -->

<!--   ungroup() |>  -->

<!--   mutate(run = str_c("sample ", factor(.draw))) |>  -->

<!--   select(run, MAD, AUC)  -->

<!-- post_pred |> ggplot(aes(x = MAD, y = AUC)) + -->

<!--   geom_point(data = dolphin_agg, color = project_colors[2], alpha = 0.3) + -->

<!--   geom_point() +  -->

<!--   facet_grid(. ~ run) -->

<!-- ``` -->

<!-- ::: callout-caution -->

<!-- **Exercise 3b** -->

<!-- Change the input to the parameter `newdata` so that we get three samples for `MAD` values 400, 500 and 600. -->

<!-- ::: -->

<!-- ```{r} -->

<!-- #| eval: false -->

<!-- #| code-fold: true -->

<!-- #| code-summary: "Show solution" -->

<!-- # extract & plot posterior predictives -->

<!-- post_pred2 <- tidybayes::predicted_draws( -->

<!--   object = fit_dolphin_agg, -->

<!--   newdata = tibble(MAD = c(400, 500, 600)), -->

<!--   value = "AUC", -->

<!--   ndraws = 3 -->

<!-- ) |>  -->

<!--   ungroup() |>  -->

<!--   mutate(run = str_c("sample ", factor(.draw))) |>  -->

<!--   select(run, MAD, AUC)  -->

<!-- post_pred2 |> ggplot(aes(x = MAD, y = AUC)) + -->

<!--   geom_point(data = dolphin_agg, color = project_colors[2], alpha = 0.3) + -->

<!--   geom_point() +  -->

<!--   facet_grid(. ~ run) -->

<!-- ``` -->

<!-- We can also extract predictions for the linear predictor values like so: -->

<!-- ```{r} -->

<!-- # extract & plot posterior linear predictors -->

<!-- post_lin_pred <- tidybayes::linpred_draws( -->

<!--   object = fit_dolphin_agg, -->

<!--   newdata = dolphin_agg |> select(MAD), -->

<!--   value = "AUC", -->

<!--   ndraws = 3 -->

<!-- ) |>  -->

<!--   ungroup() |>  -->

<!--   mutate(run = str_c("sample ", factor(.draw))) |>  -->

<!--   select(run, MAD, AUC)  -->

<!-- post_lin_pred |> ggplot(aes(x = MAD, y = AUC)) + -->

<!--   geom_point(data = dolphin_agg, color = project_colors[2], alpha = 0.3) + -->

<!--   geom_line() +  -->

<!--   facet_grid(. ~ run) -->

<!-- ``` -->

<!-- ::: callout-caution -->

<!-- **Exercise 3c** -->

<!-- Extract 30 samples of linear predictor lines and plot them all in one plot. Make the line plots gray and use a low `alpha` value (slight transparency). -->

<!-- ::: -->

<!-- ```{r} -->

<!-- #| eval: false -->

<!-- #| code-fold: true -->

<!-- #| code-summary: "Show solution" -->

<!-- post_lin_pred2 <- tidybayes::linpred_draws( -->

<!--   object = fit_dolphin_agg, -->

<!--   newdata = dolphin_agg |> select(MAD), -->

<!--   value = "AUC", -->

<!--   ndraws = 30 -->

<!-- ) |>  -->

<!--   ungroup() |>  -->

<!--   mutate(run = str_c("sample ", factor(.draw))) |>  -->

<!--   select(run, MAD, AUC)  -->

<!-- post_lin_pred2 |> ggplot(aes(x = MAD, y = AUC)) + -->

<!--   geom_point(data = dolphin_agg, color = project_colors[2], alpha = 0.3) + -->

<!--   geom_line(aes(group = run), color = "gray", alpha = 0.2) -->

<!-- ``` -->

<!-- Finally, let's look at a posterior predictive check, based on the distribution of actual / predicted `AUC` values: -->

<!-- ```{r} -->

<!-- pp_check(fit_dolphin_agg, ndraws = 20) -->

<!-- ``` -->

<!-- ::: callout-caution -->

<!-- **Exercise 3d** -->

<!-- Repeat all the steps from the prior predictive point of view for model `fit_dolphin_agg`, -->

<!-- ::: -->

<!-- ```{r} -->

<!-- #| eval: false -->

<!-- #| code-fold: true -->

<!-- #| code-summary: "Show solution" -->

<!-- fit_dolphin_agg_prior <- stats::update(fit_dolphin_agg, sample_prior = 'only') -->

<!-- post_pred <- tidybayes::predicted_draws( -->

<!--   object = fit_dolphin_agg_prior, -->

<!--   newdata = dolphin_agg |> select(MAD), -->

<!--   value = "AUC", -->

<!--   ndraws = 3 -->

<!-- ) |>  -->

<!--   ungroup() |>  -->

<!--   mutate(run = str_c("sample ", factor(.draw))) |>  -->

<!--   select(run, MAD, AUC)  -->

<!-- post_pred |> ggplot(aes(x = MAD, y = AUC)) + -->

<!--   geom_point(data = dolphin_agg, color = project_colors[2], alpha = 0.3) + -->

<!--   geom_point(alpha = 0.5) +  -->

<!--   facet_grid(. ~ run) -->

<!-- # extract & plot posterior predictives -->

<!-- post_pred2 <- tidybayes::predicted_draws( -->

<!--   object = fit_dolphin_agg_prior, -->

<!--   newdata = tibble(MAD = c(400, 500, 600)), -->

<!--   value = "AUC", -->

<!--   ndraws = 3 -->

<!-- ) |>  -->

<!--   ungroup() |>  -->

<!--   mutate(run = str_c("sample ", factor(.draw))) |>  -->

<!--   select(run, MAD, AUC)  -->

<!-- post_pred2 |> ggplot(aes(x = MAD, y = AUC)) + -->

<!--   geom_point(data = dolphin_agg, color = project_colors[2], alpha = 0.3) + -->

<!--   geom_point(alpha = 0.8) +  -->

<!--   facet_grid(. ~ run) -->

<!-- # extract & plot posterior linear predictors -->

<!-- post_lin_pred <- tidybayes::linpred_draws( -->

<!--   object = fit_dolphin_agg_prior, -->

<!--   newdata = dolphin_agg |> select(MAD), -->

<!--   value = "AUC", -->

<!--   ndraws = 3 -->

<!-- ) |>  -->

<!--   ungroup() |>  -->

<!--   mutate(run = str_c("sample ", factor(.draw))) |>  -->

<!--   select(run, MAD, AUC)  -->

<!-- post_lin_pred |> ggplot(aes(x = MAD, y = AUC)) + -->

<!--   geom_point(data = dolphin_agg, color = project_colors[2], alpha = 0.3) + -->

<!--   geom_line() +  -->

<!--   facet_grid(. ~ run) -->

<!-- post_lin_pred2 <- tidybayes::linpred_draws( -->

<!--   object = fit_dolphin_agg_prior, -->

<!--   newdata = dolphin_agg |> select(MAD), -->

<!--   value = "AUC", -->

<!--   ndraws = 30 -->

<!-- ) |>  -->

<!--   ungroup() |>  -->

<!--   mutate(run = str_c("sample ", factor(.draw))) |>  -->

<!--   select(run, MAD, AUC)  -->

<!-- post_lin_pred2 |> ggplot(aes(x = MAD, y = AUC)) + -->

<!--   geom_point(data = dolphin_agg, color = project_colors[2], alpha = 0.3) + -->

<!--   geom_line(aes(group = run), color = "gray", alpha = 0.2) -->

<!-- ``` -->
