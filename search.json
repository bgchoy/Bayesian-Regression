[
  {
    "objectID": "practice-sheets/01f-catPreds-exercises.html",
    "href": "practice-sheets/01f-catPreds-exercises.html",
    "title": "02c: Categorical predictors (exercises)",
    "section": "",
    "text": "Preamble\nHere is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\n\n\nToggle code\n# install packages from CRAN (unless installed)\npckgs_needed <- c(\n  \"tidyverse\",\n  \"brms\",\n  \"rstan\",\n  \"rstanarm\",\n  \"remotes\",\n  \"tidybayes\",\n  \"bridgesampling\",\n  \"shinystan\",\n  \"mgcv\"\n)\npckgs_installed <- installed.packages()[,\"Package\"]\npckgs_2_install <- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]\nif(length(pckgs_2_install)) {\n  install.packages(pckgs_2_install)\n} \n\n# install additional packages from GitHub (unless installed)\nif (! \"aida\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/aida-package\")\n}\nif (! \"faintr\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/faintr\")\n}\nif (! \"cspplot\" %in% pckgs_installed) {\n  remotes::install_github(\"CogSciPrag/cspplot\")\n}\n\n# load the required packages\nx <- lapply(pckgs_needed, library, character.only = TRUE)\nlibrary(aida)\nlibrary(faintr)\nlibrary(cspplot)\n\n# these options help Stan run faster\noptions(mc.cores = parallel::detectCores())\n\n# use the CSP-theme for plotting\ntheme_set(theme_csp())\n\n# global color scheme from CSP\nproject_colors = cspplot::list_colors() |> pull(hex)\n# names(project_colors) <- cspplot::list_colors() |> pull(name)\n\n# setting theme colors globally\nscale_colour_discrete <- function(...) {\n  scale_colour_manual(..., values = project_colors)\n}\nscale_fill_discrete <- function(...) {\n   scale_fill_manual(..., values = project_colors)\n}\n\n\n\n\nToggle code\ndolphin <- aida::data_MT\n\n\n\n\nExercise 1: Regression w/ multiple categorical predictors\nWe want to regress log(RT) against the full combination of categorical factors group, condition, and prototype_label.\nlog(RT) ~ group * condition * prototype_label\nThe research hypotheses we would like to investigate are:\n\nTypical trials are faster than atypical ones.\nCoM trials are slower than the other kinds of trials (straight and curved) together, and respectively.\n‘straight’ trials are faster than ‘curved’ trials.\nClick trials are slower than touch trials.\n\nBut for this to work (without at least mildly informative priors), we would need to have a sufficient amount of observations in each cell. So, let’s check:\n\n\nToggle code\ndolphin |>\n  mutate(group = as_factor(group),\n         condition = as_factor(condition),\n         prototype_label = as_factor(prototype_label)) |>\n  count(group, condition, prototype_label, .drop = FALSE) |>\n  arrange(n)\n\n\n# A tibble: 20 × 4\n   group condition prototype_label     n\n   <fct> <fct>     <fct>           <int>\n 1 touch Atypical  dCoM2               0\n 2 touch Typical   dCoM2               0\n 3 touch Atypical  dCoM                9\n 4 click Atypical  dCoM2              11\n 5 click Typical   dCoM2              11\n 6 touch Typical   dCoM               14\n 7 touch Atypical  cCoM               21\n 8 touch Typical   cCoM               31\n 9 click Atypical  cCoM               32\n10 click Atypical  curved             36\n11 touch Atypical  curved             37\n12 click Typical   cCoM               48\n13 click Atypical  dCoM               50\n14 click Typical   dCoM               52\n15 touch Typical   curved             72\n16 click Typical   curved             84\n17 click Atypical  straight          189\n18 touch Atypical  straight          263\n19 click Typical   straight          494\n20 touch Typical   straight          598\n\n\nSo, there are cells for which we have no observations at all. For simplicity, we therefore just lump all “change of mind”-type trajectories into one category:\n\n\nToggle code\ndolphin_prepped <-\n  dolphin |>\n  mutate(\n    prototype_label = case_when(\n     prototype_label %in% c('curved', 'straight') ~ prototype_label,\n     TRUE ~ 'CoM'\n    ),\n    prototype_label = factor(prototype_label,\n                             levels = c('straight', 'curved', 'CoM')))\n\ndolphin_prepped |>\n  select(RT, prototype_label)\n\n\n# A tibble: 2,052 × 2\n      RT prototype_label\n   <dbl> <fct>          \n 1   950 straight       \n 2  1251 straight       \n 3   930 curved         \n 4   690 curved         \n 5   951 CoM            \n 6  1079 CoM            \n 7  1050 CoM            \n 8   830 straight       \n 9   700 straight       \n10   810 straight       \n# ℹ 2,042 more rows\n\n\nHere is a plot of the data to be analyzed:\n\n\nToggle code\ndolphin_prepped |>\n  ggplot(aes(x = log(RT), fill = condition)) +\n  geom_density(alpha = 0.4) +\n  facet_grid(group ~ prototype_label)\n\n\n\n\n\n\n\n\n\n\n\nExercise 1a\n\n\n\n\n\nUse brm() to run a linear regression model for the data set dolphin_prepped and the formula:\nlog(RT) ~ group * condition * prototype_label\nSet the prior for all population-level slope coefficients to a reasonable, weakly-informative but unbiased prior.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\nfit <- brm(\n  formula = log(RT) ~ group * condition * prototype_label,\n  prior   = prior(student_t(1, 0, 3), class = \"b\"),\n  data    = dolphin_prepped\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1b\n\n\n\n\n\nPlot the posteriors for population-level slope coefficients using the tidybayes package in order to:\n\ndetermine which combination of factor levels is the default cell\ncheck which coefficients have 95% CIs that do not include zero\ntry to use this latter information to address any of our research hypotheses (stated above)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\ntidybayes::summarise_draws(fit)\n\n\n# A tibble: 15 × 10\n   variable       mean   median      sd     mad       q5      q95  rhat ess_bulk\n   <chr>         <num>    <num>   <num>   <num>    <num>    <num> <num>    <num>\n 1 b_Interce…  7.63e+0  7.64e+0 0.0332  0.0328   7.58e+0  7.69e+0  1.00    1775.\n 2 b_groupto… -2.40e-1 -2.40e-1 0.0439  0.0434  -3.12e-1 -1.66e-1  1.00    1789.\n 3 b_conditi… -2.46e-1 -2.48e-1 0.0388  0.0377  -3.08e-1 -1.80e-1  1.00    1691.\n 4 b_prototy… -5.30e-2 -5.38e-2 0.0849  0.0855  -1.92e-1  8.55e-2  1.00    1589.\n 5 b_prototy…  1.77e-1  1.77e-1 0.0591  0.0605   8.05e-2  2.74e-1  1.00    1760.\n 6 b_groupto…  1.30e-2  1.35e-2 0.0517  0.0512  -7.23e-2  9.74e-2  1.00    1660.\n 7 b_groupto…  1.36e-1  1.38e-1 0.117   0.118   -5.76e-2  3.27e-1  1.00    1676.\n 8 b_groupto…  4.05e-3  4.81e-3 0.107   0.108   -1.72e-1  1.80e-1  1.00    1992.\n 9 b_conditi…  3.16e-2  3.36e-2 0.101   0.100   -1.34e-1  1.98e-1  1.00    1585.\n10 b_conditi…  4.35e-2  4.48e-2 0.0754  0.0769  -8.16e-2  1.65e-1  1.00    1907.\n11 b_groupto… -1.38e-1 -1.40e-1 0.143   0.146   -3.73e-1  9.60e-2  1.00    1667.\n12 b_groupto…  5.19e-2  5.10e-2 0.139   0.140   -1.77e-1  2.83e-1  1.00    2207.\n13 sigma       4.60e-1  4.60e-1 0.00739 0.00718  4.48e-1  4.73e-1  1.00    4181.\n14 lprior     -2.79e+1 -2.79e+1 0.0133  0.0107  -2.79e+1 -2.79e+1  1.00    1583.\n15 lp__       -1.35e+3 -1.35e+3 2.59    2.49    -1.35e+3 -1.34e+3  1.00    1459.\n# ℹ 1 more variable: ess_tail <num>\n\n\nToggle code\ntidybayes::gather_draws(fit, `b_.*`, regex = TRUE) |>\n  filter(.variable != \"b_Intercept\") |>\n  ggplot(aes(y = .variable, x = .value)) +\n  tidybayes::stat_halfeye() +\n  labs(x = \"\", y = \"\") +\n  geom_segment(x = 0, xend = 0, y = Inf, yend = -Inf,\n               lty = \"dashed\")\n\n\n\n\n\nThe default cell is for click-atypical-straight. The coeffiencents with 95% CIs that do not include zero are: grouptouch, conditionTypical, prototype_labelCoM. None of these give us direct information about our research hypotheses.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1c\n\n\n\n\n\nUse the faintr package to get information relevant for the current research hypotheses. Interpret each result with respect to what we may conclude from it.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\n# 1. Typical trials are faster than atypical ones.\n# -> There is overwhelming evidence that this is true\n#    (given the data and the model).\n\nfaintr::compare_groups(\n  fit,\n  lower  = condition == 'Typical',\n  higher = condition == 'Atypical'\n)\n\n\nOutcome of comparing groups: \n * higher:  condition == \"Atypical\" \n * lower:   condition == \"Typical\" \nMean 'higher - lower':  0.2291 \n95% HDI:  [ 0.1679 ; 0.2913 ]\nP('higher - lower' > 0):  1 \nPosterior odds:  Inf \n\n\nToggle code\n# 2. CoM trials are slower than the other kinds of trials\n#    (straight and curved) together, and respectively.\n# -> There is overwhelming evidence that this is true\n#    (given the data and the model).\n\nfaintr::compare_groups(\n  fit,\n  lower  = prototype_label != 'CoM',\n  higher = prototype_label == 'CoM'\n)\n\n\nOutcome of comparing groups: \n * higher:  prototype_label == \"CoM\" \n * lower:   prototype_label != \"CoM\" \nMean 'higher - lower':  0.2159 \n95% HDI:  [ 0.1448 ; 0.288 ]\nP('higher - lower' > 0):  1 \nPosterior odds:  Inf \n\n\nToggle code\nfaintr::compare_groups(\n  fit,\n  lower  = prototype_label == 'straight',\n  higher = prototype_label == 'CoM'\n)\n\n\nOutcome of comparing groups: \n * higher:  prototype_label == \"CoM\" \n * lower:   prototype_label == \"straight\" \nMean 'higher - lower':  0.214 \n95% HDI:  [ 0.1492 ; 0.2835 ]\nP('higher - lower' > 0):  1 \nPosterior odds:  Inf \n\n\nToggle code\n# 3. 'straight' trials are faster than 'curved' trials.\n# -> There is no evidence for this hypothesis\n#    (given the data and the model).\n\nfaintr::compare_groups(\n  fit,\n  lower  = prototype_label == 'straight',\n  higher = prototype_label == 'curved'\n)\n\n\nOutcome of comparing groups: \n * higher:  prototype_label == \"curved\" \n * lower:   prototype_label == \"straight\" \nMean 'higher - lower':  -0.003717 \n95% HDI:  [ -0.07183 ; 0.06501 ]\nP('higher - lower' > 0):  0.455 \nPosterior odds:  0.8349 \n\n\nToggle code\n# 4. Click trials are slower than touch trials.\n# -> There is overwhelming evidence that this is true\n#    (given the data and the model).\n\nfaintr::compare_groups(\n  fit,\n  lower  = group == 'touch',\n  higher = group == 'click'\n)\n\n\nOutcome of comparing groups: \n * higher:  group == \"click\" \n * lower:   group == \"touch\" \nMean 'higher - lower':  0.2014 \n95% HDI:  [ 0.1346 ; 0.2621 ]\nP('higher - lower' > 0):  1 \nPosterior odds:  Inf \n\n\n\n\n\n\n\n\n\n\nExercise 2: Regression w/ metric & categorical predictors\n\n\n\n\n\n\nExercise 2a\n\n\n\n\n\nCreate a new data frame that contains only the mean values of the RT, and MAD for each animal (exemplar) and for correct and incorrect responses. Print out the head of the new data frame.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\n# aggregate\ndolphin_agg <- dolphin |> \n  group_by(exemplar, correct) |> \n  dplyr::summarize(MAD = mean(MAD, na.rm = TRUE),\n                   RT = mean(RT, na.rm = TRUE))\n  \n# let's have a look\nhead(dolphin_agg)\n\n\n# A tibble: 6 × 4\n# Groups:   exemplar [3]\n  exemplar  correct   MAD    RT\n  <chr>       <dbl> <dbl> <dbl>\n1 alligator       0 271.  4246.\n2 alligator       1  87.4 1717.\n3 bat             0 176.  3334.\n4 bat             1 182.  2252.\n5 butterfly       0  15.0 1636.\n6 butterfly       1 145.  1761.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2b\n\n\n\n\n\nRun a linear regression using brms. MAD is the dependent variable (i.e. the measure) and both RT and correct are independent variables (MAD ~ RT + correct). (Hint: the coefficients might be really small, so make sure the output is printed with enough numbers after the comma.)\nTry to understand the coefficient table. There is one coefficient for RT and one coefficient for correct which gives you the change in MAD from incorrect to correct responses.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\n# specify the model \nmodel2 = brm(\n  # model formula\n  MAD ~ RT + correct, \n  # data\n  data = dolphin_agg\n  )\n\nprint(summary(model2), digits = 5)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: MAD ~ RT + correct \n   Data: dolphin_agg (Number of observations: 36) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n           Estimate Est.Error  l-95% CI u-95% CI    Rhat Bulk_ESS Tail_ESS\nIntercept  20.95857  35.61002 -49.23980 91.53278 1.00108     4177     2812\nRT          0.06221   0.01496   0.03210  0.09087 1.00117     4320     2797\ncorrect   -14.84838  26.82810 -68.90679 38.80311 1.00096     4334     2579\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI    Rhat Bulk_ESS Tail_ESS\nsigma 77.64625   9.95414 61.62940 99.51658 1.00238     3451     2497\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2c\n\n\n\n\n\nPlot a scatter plot of MAD ~ RT and color code it for correct responses. (Hint: Make sure that correct is treated as a factor and not a numeric vector). Draw two predicted lines into the scatterplot. One for correct responses (“lightblue”) and one for incorrect responses (“orange”).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\ndolphin_agg$correct <- as.factor(as.character(dolphin_agg$correct))\n\n# extract model parameters:\nmodel_intercept <- summary(model2)$fixed[1,1]\nmodel_RT <- summary(model2)$fixed[2,1]\nmodel_correct <- summary(model2)$fixed[3,1]\n\n# plot\nggplot(data = dolphin_agg, \n       aes(x = RT, \n           y = MAD,\n           color = correct)) + \n  geom_abline(intercept = model_intercept, slope = model_RT, color = \"orange\", size  = 2) +\n  geom_abline(intercept = model_intercept + model_correct , slope = model_RT, color = \"lightblue\",size  = 2) +\n  geom_point(size = 3, alpha = 0.3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2d\n\n\n\n\n\nExtract the posteriors for the coefficients of both RT and correct from the model output (use the spread_draws() function), calculate their means and a 67% Credible Interval. Print out the head of the aggregated dataframe.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\n# get posteriors for the relevant coefficients\nposteriors2 <- model2 |>\n  spread_draws(b_RT, b_correct) |>\n  select(b_RT, b_correct) |> \n  gather(key = \"parameter\", value = \"posterior\")\n\n# aggregate\nposteriors2_agg <- posteriors2 |> \n  group_by(parameter) |> \n  summarise(mean_posterior = mean(posterior),\n            `67lowerCrI` = HDInterval::hdi(posterior, credMass = 0.67)[1],\n            `67higherCrI` = HDInterval::hdi(posterior, credMass = 0.67)[2]\n            )\n\n# print out\nposteriors2_agg\n\n\n# A tibble: 2 × 4\n  parameter mean_posterior `67lowerCrI` `67higherCrI`\n  <chr>              <dbl>        <dbl>         <dbl>\n1 b_RT              0.0622       0.0480        0.0763\n2 b_correct       -14.8        -40.1          10.4   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2e\n\n\n\n\n\nPlot the scatterplot from 2c and plot 50 sample tuples for the regression lines for correct and incorrect responses.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\n# sample 50 random numbers from 4000 samples\nrandom_50 <- sample(1:4000, 50, replace = FALSE)\n  \n# wrangle data frame\nposteriors3 <- model2 |>\n  spread_draws(b_Intercept, b_RT, b_correct) |>\n  select(b_Intercept, b_RT, b_correct) |> \n  # filter by the row numbers in random_50\n  slice(random_50)\n  \n# plot\nggplot(data = dolphin_agg, \n       aes(x = RT, \n           y = MAD, \n           color = correct)) + \n  geom_abline(data = posteriors3,\n              aes(intercept = b_Intercept, slope = b_RT), \n              color = \"orange\", size  = 0.1) +\n  geom_abline(data = posteriors3,\n              aes(intercept = b_Intercept + b_correct, slope = b_RT), \n              color = \"lightblue\", size  = 0.1) +\n  geom_point(size = 3, alpha = 0.3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2f\n\n\n\n\n\nGiven our model and our data, calculate the evidence ratio of correct responses exhibiting larger MADs than incorrect responses. How would you interpret the result?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\nhypothesis(model2, 'correct > 0')\n\n\nHypothesis Tests for class b:\n     Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1 (correct) > 0   -14.85     26.83   -58.59    28.87       0.41      0.29     \n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\n\n\n\n\n\n\n\n\n\nExercise 3: Metric and categorical variables, and their interaction\nHere is an aggregated data set dolphin_agg for you.\n\n\nToggle code\n# aggregate\ndolphin_agg <- dolphin %>% \n  group_by(group, exemplar) %>% \n  dplyr::summarize(MAD = median(MAD, na.rm = TRUE),\n                   RT = median(RT, na.rm = TRUE)) %>% \n  mutate(log_RT = log(RT))\n\n\n\n\n\n\n\n\nExercise 3a\n\n\n\n\n\nStandardize (“z-transform”) log_RT such that the mean is at zero and 1 unit corresponds to the standard deviation. Name it log_RT_s. (Hint: use function scale().)\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\ndolphin_agg$log_RT_s <- scale(dolphin_agg$log_RT, scale = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 3b\n\n\n\n\n\nRun a linear model with brms that predicts MAD based on log_RT_s, group, and their two-way interaction.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\nmodel1 = brm(\n  MAD ~ log_RT_s * group, \n  data = dolphin_agg\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 3c\n\n\n\n\n\nPlot MAD (y) against log_RT_s (x) in a scatter plot and color-code for group. Plot the regression lines for the click and the touch group into the plot and don’t forget to take possible interactions into account.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\n# extract posterior means for model coefficients\nIntercept = summary(model1)$fixed[1,1]\nlog_RT = summary(model1)$fixed[2,1]\ngroup = summary(model1)$fixed[3,1]\ninteraction = summary(model1)$fixed[4,1]\n\n# plot\nggplot(data = dolphin_agg, \n       aes(x = log_RT_s, \n           y = MAD, \n           color = group)) + \n  geom_point(size = 3, alpha = 0.3) +\n  geom_vline(xintercept = 0, lty = \"dashed\") +\n  geom_abline(intercept = Intercept, slope = log_RT, \n              color = project_colors[1], size = 2) +\n  geom_abline(intercept = Intercept + group, slope = log_RT + interaction, \n              color = project_colors[2], size = 2) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 3d\n\n\n\n\n\nSpecify very skeptic priors for all three coefficients. Use a normal distribution with mean = 0, and sd = 10. Rerun the model with those priors.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\n# specify priors\npriors_model2 <- c(\n  set_prior(\"normal(0,10)\", class = \"b\", coef = \"log_RT_s\"),\n  set_prior(\"normal(0,10)\", class = \"b\", coef = \"grouptouch\"),\n  set_prior(\"normal(0,10)\", class = \"b\", coef = \"log_RT_s:grouptouch\")\n)\n\n# model\nmodel2 = brm(\n  MAD ~ log_RT_s * group, \n  data = dolphin_agg,\n  prior = priors_model2\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 3e\n\n\n\n\n\nCompare the model output of model1 to model2. What are the differences and what are the reasons for these differences?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\n# We can compare the model predictions by looking at the coefficients / plotting them:\nsummary(model1)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: MAD ~ log_RT_s * group \n   Data: dolphin_agg (Number of observations: 38) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept              66.25      7.60    51.21    80.82 1.00     3042     2614\nlog_RT_s               16.74      7.40     2.67    31.69 1.00     2265     2547\ngrouptouch            -34.96     11.11   -56.29   -12.92 1.00     3245     2610\nlog_RT_s:grouptouch   -11.11     11.01   -33.22    10.18 1.00     2537     2407\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    27.69      3.45    22.01    35.40 1.00     2630     2763\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nToggle code\nsummary(model2)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: MAD ~ log_RT_s * group \n   Data: dolphin_agg (Number of observations: 38) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept              60.37      6.04    48.11    71.93 1.00     4560     3159\nlog_RT_s               13.69      5.14     3.67    23.49 1.00     3759     3221\ngrouptouch            -17.71      7.65   -32.48    -2.15 1.00     4132     2778\nlog_RT_s:grouptouch    -0.92      7.31   -15.34    13.48 1.00     4186     3153\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    28.52      3.48    22.55    36.55 1.00     4016     3007\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nToggle code\n# extract posterior means for model coefficients\nIntercept = summary(model2)$fixed[1,1]\nlog_RT = summary(model2)$fixed[2,1]\ngroup = summary(model2)$fixed[3,1]\ninteraction = summary(model2)$fixed[4,1]\n\n# plot\nggplot(data = dolphin_agg, \n       aes(x = log_RT_s, \n           y = MAD, \n           color = group)) + \n  geom_point(size = 3, alpha = 0.3) +\n  geom_vline(xintercept = 0, lty = \"dashed\") +\n  geom_abline(intercept = Intercept, slope = log_RT, \n              color = project_colors[1], size = 2) +\n  geom_abline(intercept = Intercept + group, slope = log_RT + interaction, \n              color = project_colors[1], size = 2) \n\n\n\n\n\nThe magnitude of the coefficients is much smaller in model2, with the interaction term being close to zero. As a result, the lines in the plot are closer together and run in parallel. The reason for this change lies in the priors. We defined the priors of model2 rather narrowly, down-weighing data points larger or smaller than zero. This is a case of the prior dominating the posterior."
  },
  {
    "objectID": "practice-sheets/07a-nonLinear.html",
    "href": "practice-sheets/07a-nonLinear.html",
    "title": "XX: Non-linear models in brms",
    "section": "",
    "text": "It is possible to supply non-linear predictor terms in brms. These are of the form:\n\\[\n\\eta = X \\beta + F(X', \\theta_1, \\dots, \\theta_n)\n\\]\nwhere \\(X'\\) are the predictor terms feeding into non-linear function \\(F\\), with parameters \\(\\theta_1, \\dots, \\theta_n\\). These parameters can themselves be predicted by linear terms, e.g., in the form:\n\\[\n\\theta_i = X'' \\beta_{\\theta_i}\n\\]\n\nPreamble\nHere is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\n\n\nToggle code\n# install packages from CRAN (unless installed)\npckgs_needed <- c(\n  \"tidyverse\",\n  \"brms\",\n  \"rstan\",\n  \"rstanarm\",\n  \"remotes\",\n  \"tidybayes\",\n  \"bridgesampling\",\n  \"shinystan\",\n  \"mgcv\"\n)\npckgs_installed <- installed.packages()[,\"Package\"]\npckgs_2_install <- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]\nif(length(pckgs_2_install)) {\n  install.packages(pckgs_2_install)\n} \n\n# install additional packages from GitHub (unless installed)\nif (! \"aida\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/aida-package\")\n}\nif (! \"faintr\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/faintr\")\n}\nif (! \"cspplot\" %in% pckgs_installed) {\n  remotes::install_github(\"CogSciPrag/cspplot\")\n}\n\n# load the required packages\nx <- lapply(pckgs_needed, library, character.only = TRUE)\nlibrary(aida)\nlibrary(faintr)\nlibrary(cspplot)\n\n# these options help Stan run faster\noptions(mc.cores = parallel::detectCores())\n\n# use the CSP-theme for plotting\ntheme_set(theme_csp())\n\n# global color scheme from CSP\nproject_colors = cspplot::list_colors() |> pull(hex)\n# names(project_colors) <- cspplot::list_colors() |> pull(name)\n\n# setting theme colors globally\nscale_colour_discrete <- function(...) {\n  scale_colour_manual(..., values = project_colors)\n}\nscale_fill_discrete <- function(...) {\n   scale_fill_manual(..., values = project_colors)\n}\n\n\n\n\nForgetting: power or exponential\nHere is a very small data set from (Murdoch 1961), as used in this tutorial paper on MLE. We have recall rates y for 100 subjects at six different time points t (in seconds) after after memorization:\n\n\nToggle code\ndata_forget <- tibble(\n  y = c(.94, .77, .40, .26, .24, .16),\n  t = c(  1,   3,   6,   9,  12,  18),\n  N = 100,\n  k = y * N\n)\n\ndata_forget |>\n  ggplot(aes(x = t, y = y)) +\n  geom_point() +\n  xlab(\"time after memorization\") +\n  ylab(\"recall rate\")\n\n\n\n\n\nThere are two competing models on the table. The exponential model assumes that recall rates are predicted by exponential decay:\n\\[\n\\begin{align*}\nk & \\sim \\text{Binomial}( \\theta, 100) \\\\\n\\theta &= a \\exp (-bt) \\\\\na,b &\\sim \\dots \\text{some prior} \\dots\n\\end{align*}\n\\]\n\\[\n\\begin{align*}\nk & \\sim \\text{Binomial}( \\theta, 100) \\\\\n\\theta &= ct^{-d} \\\\\nc,d &\\sim \\dots \\text{some prior} \\dots\n\\end{align*}\n\\]\n\n\nToggle code\nfit_exponential <- brms::brm(\n    formula = brms::bf(k | trials(N) ~ a * exp(-b * t), \n                       a + b ~ 1, \n                       nl=TRUE),\n    data    = data_forget,\n    prior   = prior(lognormal(0,0.5), nlpar = \"a\", lb = 0) + \n              prior(lognormal(0,0.5), nlpar = \"b\", lb = 0),\n    family  = binomial(link = \"identity\"),\n    control = list(adapt_delta = 0.99)\n  )\n\n\n\n\nToggle code\nfit_power <- brms::brm(\n    formula = brms::bf(k | trials(N) ~ c * t^(-d), \n                       c + d ~ 1, \n                       nl=TRUE),\n    data    = data_forget,\n    prior   = prior(lognormal(0,0.5), nlpar = \"c\", lb = 0) + \n              prior(lognormal(0,0.5), nlpar = \"d\", lb = 0),\n    family  = binomial(link = \"identity\"),\n    control = list(adapt_delta = 0.99),\n    iter    = 8000\n  )\n\n\nCompare models with loo.\n\n\nToggle code\nloo_compare <- \n  loo_compare(\n    loo(fit_exponential), \n    loo(fit_power, moment_match = TRUE))\n\n\nUse longer data\n\n\nToggle code\ndata_forget_long <- \n  data_forget |> \n  mutate(l = N-k) |> \n  dplyr::select(t,l,k) |> \n  pivot_longer(c(l,k), names_to = \"y\") |> \n  uncount(value) |> \n  mutate(y = ifelse(y == \"k\", TRUE, FALSE))\n\n\n\n\nToggle code\nfit_exponential <- brms::brm(\n    formula = brms::bf(y ~ a * exp(-b * t), \n                       a + b ~ 1, \n                       nl=TRUE),\n    data    = data_forget_long,\n    prior   = prior(lognormal(0,0.4), nlpar = \"a\", lb = 0) + \n              prior(lognormal(0,0.4), nlpar = \"b\", lb = 0),\n    family  = bernoulli(link = \"identity\"),\n    control = list(adapt_delta = 0.99)\n  )\n\n\n\n\nToggle code\nfit_power <- brms::brm(\n    formula = brms::bf(y ~ c * t^(-d), \n                       c + d ~ 1, \n                       nl=TRUE),\n    data    = data_forget_long,\n    prior   = prior(lognormal(0,0.4), nlpar = \"c\", lb = 0) + \n              prior(lognormal(0,0.4), nlpar = \"d\", lb = 0),\n    family  = bernoulli(link = \"identity\"),\n    control = list(adapt_delta = 0.999)\n  )\n\n\n\n\nToggle code\nloo_compare <- \n  loo_compare(\n    loo(fit_exponential), \n    loo(fit_power))"
  },
  {
    "objectID": "practice-sheets/07b-GAMs.html",
    "href": "practice-sheets/07b-GAMs.html",
    "title": "XX: Generalized additive models in brms",
    "section": "",
    "text": "This tutorial provides both a conceptual and a practical introduction to fitting generalized additive models (GAMs) in brms. GAMs approximate wiggly curves by “smoothed splines”. The central idea to internalize here is that we can think of smoothed splines as a random effect. This is, indeed, how brms deals with GAMs.\n The material here leans heavily on T.J. Mahr’s excellent tutorial. \n\nPreamble\nHere is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\n\n\nToggle code\n# install packages from CRAN (unless installed)\npckgs_needed <- c(\n  \"tidyverse\",\n  \"brms\",\n  \"rstan\",\n  \"rstanarm\",\n  \"remotes\",\n  \"tidybayes\",\n  \"bridgesampling\",\n  \"shinystan\",\n  \"mgcv\"\n)\npckgs_installed <- installed.packages()[,\"Package\"]\npckgs_2_install <- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]\nif(length(pckgs_2_install)) {\n  install.packages(pckgs_2_install)\n} \n\n# install additional packages from GitHub (unless installed)\nif (! \"aida\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/aida-package\")\n}\nif (! \"faintr\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/faintr\")\n}\nif (! \"cspplot\" %in% pckgs_installed) {\n  remotes::install_github(\"CogSciPrag/cspplot\")\n}\n\n# load the required packages\nx <- lapply(pckgs_needed, library, character.only = TRUE)\nlibrary(aida)\nlibrary(faintr)\nlibrary(cspplot)\n\n# these options help Stan run faster\noptions(mc.cores = parallel::detectCores())\n\n# use the CSP-theme for plotting\ntheme_set(theme_csp())\n\n# global color scheme from CSP\nproject_colors = cspplot::list_colors() |> pull(hex)\n# names(project_colors) <- cspplot::list_colors() |> pull(name)\n\n# setting theme colors globally\nscale_colour_discrete <- function(...) {\n  scale_colour_manual(..., values = project_colors)\n}\nscale_fill_discrete <- function(...) {\n   scale_fill_manual(..., values = project_colors)\n}\n\n\n\n\nData set and mission statement\nLet’s start with a common example for introductions to GAMs, the non-linear relationship between time (after impact) and acceleration (of the head of a motorcycle driver). The data is from the MASS package and it looks like this:\n\n\nToggle code\n# motorcycle data\ndata_motor <- MASS::mcycle |> \n  tibble() |> \n  unique()\n\n# plot the data\ndata_motor |> \n  ggplot(aes(x = times, y = accel)) +\n  geom_smooth(color = project_colors[1]) +\n  geom_point()\n\n\n\n\n\nNotice that the plotting function geom_smooth already provides us with a smoothing line to fit the central tendency of this this “wiggly data”. But this one is not very good. We can probably do better.\nThe goals for the remainder of this tutorial are:\n\nTo understand how a wiggly curve can be constructed from a set of “atomic wiggles”.\nTo see the parallel between (i) fitting weights over “atomic wiggles”, with a penalty against excessive wiggliness, and fitting (ii) group-level effects.\nTo be able to run a Bayesian regression model with penalized smoothing splines using brms.\n\n\n\nWiggly curves as mixtures over “atomic wiggles”\nLet’s play a game! It’s called “make a wiggle”. Here’s how it’s played: I give you a bunch of “atomic wiggles”, you adjust some weights, and out pops … TADA! … a smoothing spline.\nOr, for those dull at heart, in technical terms, I give you a set of basis functions. This set is also called basis. Then you build intuitions about how, by adjusting numerical weights, for the basis functions, we can flexibly approximate non-linear data.\nThe following code gives you a function get_basis() which takes as input a sequence of observations \\(x\\) for which we would like to construct a non-linear line. The output is a set of “atomic wiggles” (see plot below). The function also takes an integer value k as input, which is the number of “atomic wiggles” that you get.\nDo not worry about the details of this function, better to directly look at its output. But if you must know: this function uses the gam function from the mgcv package to construct (indirectly, through the model matrix internally built up by mgcv::gam) k cubic regression splines. The brms package uses the function mgcv:s which is called here in the model formula as well (more on this below).\n\n\nToggle code\n# create a set of /k/ splines for vector /x/\nget_basis <- function(x, k = 20){\n  # fit a dummy GAM\n  fit_gam <- mgcv::gam(\n    formula = y ~ s(times, bs = \"cr\", k = 20),\n    data    = tibble(y = 1, times = x))\n  # extract model matrix\n  model_matrix <- model.matrix(fit_gam)\n  # wrangle to long tibble\n  some_curves <- model_matrix |> \n    as_tibble() |> \n    mutate(x = x) |> \n    pivot_longer(cols = -x) |> \n    mutate(\n      name = str_replace(name, \"s\\\\(times\\\\).\", \"curve_\"))\n  return(some_curves)\n}\n\n\nLet’s construct a basis then and plot it:\n\n\nToggle code\nn_x = 1000\nx = seq(0,60, length.out = n_x)\n\nsome_curves <- get_basis(x)\n\n# plot the basis\nsome_curves |> \n  ggplot(aes(x = x, y = value, color = name, group = name)) +\n  geom_line(size=1.5) + \n  scale_colour_manual(values = c(project_colors, project_colors)) +\n  theme(legend.position=\"none\") +\n  ylab(\"\") + ggtitle(\"your basis\") +\n  xlab(\"\")\n\n\n\n\n\nTo play “make a wiggle” you also need a vector of weights, one for each basis function. Here is a function which, given a weight vector, computes the weighted average over all basis functions for a smooth prediction.\n\n\nToggle code\nmake_a_wiggle <- function(some_curves, weights, data = NULL) {\n  \n  your_wiggly_line <- some_curves |> \n    mutate(\n      weight = rep(weights, n_x),\n      weighted_value = value * weight\n      ) |> \n    group_by(x) |> \n    summarize(wiggly_line = sum(weighted_value)) |> \n    ungroup()\n  \n  your_wiggle_plot <- your_wiggly_line |> \n    ggplot(aes(x = x, y = wiggly_line)) + \n    geom_line(color = project_colors[1], size=1.5) + \n    ggtitle(\"your wiggly line\") +\n    xlab(\"x\") + ylab(\"\")\n    \n  your_wiggle_plot\n}\n\n\nAnd here are some examples:\n\n\nToggle code\nweights <- c(0,2,-1,1,1,2,1,1,1,1,1,1,1,10,1,1,1,1,1,1)\nmake_a_wiggle(some_curves, weights)\n\n\n\n\n\n\n\nToggle code\nweights <- c(-1,0.5,-2,-1,0,3,4,3,2,-1,-5,1,1,1,1,2,2,2,2,2)\nmake_a_wiggle(some_curves, weights)\n\n\n\n\n\nAnd a not so wiggly wiggle:\n\n\nToggle code\nweights <- c(1,rep(0, times = 19))\nmake_a_wiggle(some_curves, weights)\n\n\n\n\n\n\n\n\n\n\n\nExercise 1: Try a manual fit\n\n\n\n\n\nPlay around with weights and try to find a constellation that approximates the shape of the motorcycle data in data_motor that we plotted above.\n\n\n\n\n\nPenalizing wiggliness and group-level effects\nWe saw how a vector of weights allows us to approximate non-linear relationship, once we have a set of elementary basis functions. To fit a non-linear regression model, the data should inform us about which vector of weights to use. So, essentially, we can just use the basis functions as linear predictors. There’s a lot of them, so that’s going to be ugly, but let’s do it:\n\n\nToggle code\nmotor_basis <- get_basis(data_motor$times, nrow(data_motor)) |> \n  mutate (accel = rep(data_motor$accel, each = 20)) |> \n  rename(times = x) |> \n  dplyr::filter(name != \"(Intercept)\") |> \n  pivot_wider(id_cols = c(\"times\", \"accel\"), names_from = name, values_from = value, values_fn = mean)\n\nfit_motor_FE <- brms::brm(\n  formula = accel ~ times + curve_1 + curve_2 + curve_3 + curve_4 + curve_5 + curve_6 + curve_7 + curve_8 + curve_9 + curve_10 +\n    curve_11 + curve_12 + curve_13 + curve_14 + curve_15 + curve_16 + curve_17 + curve_18 + curve_19,\n  data    = motor_basis,\n  prior   = prior(normal(0,10))\n)\n\n\nThis really only works if we specify some constraints on the priors for the coefficients! (Try a fit with improper priors!)\nHere is a plot of the posterior linear predictor:\n\n\nToggle code\nplot_post_linPred <- function(fit, data) {\n  postPred <- data |> \n    tidybayes::add_epred_draws(fit) |> \n    group_by(times,accel) |> \n    summarize(\n      lower  = tidybayes::hdi(.epred)[1],\n      mean   = mean(.epred),\n      higher = tidybayes::hdi(.epred)[2]\n    )\n  \n  postPred |> \n    ggplot(aes(x = times,  y = accel)) +\n    geom_line(aes(y = mean), color = project_colors[1], size = 2) +\n    geom_ribbon(aes(ymin = lower, ymax = higher), \n                fill = project_colors[6], alpha = 0.5) +\n    geom_point(size = 2)\n}\n\nplot_post_linPred(fit_motor_FE, motor_basis)\n\n\n\n\n\nThat looks like it’s on the right track, but it’s not wiggly enough.\n\n\n\n\n\n\nExercise 2: Increasing smoothness of the predictor\n\n\n\n\n\nWhat could we do to make the linear predictor more curvy?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe bigger the fixed effect coefficients, the more the basis functions can affect the linear predictor. Currently, there are rather strong priors on the coefficients (A Gaussian with standard deviation 10). If we make these priors wider, we should see a much more curvy fit.\n\n\nToggle code\nfit_motor_FE_wider <- brms::brm(\n  formula = accel ~ times + curve_1 + curve_2 + curve_3 + curve_4 + curve_5 + curve_6 + curve_7 + curve_8 + curve_9 + curve_10 +\n    curve_11 + curve_12 + curve_13 + curve_14 + curve_15 + curve_16 + curve_17 + curve_18 + curve_19,\n  data    = motor_basis,\n  prior   = prior(normal(0,500))\n)\n\nplot_post_linPred(fit_motor_FE_wider, motor_basis)\n\n\n\n\n\n\n\n\n\n\n\nTaking stock so far, all of this shows that:\n\nBy adjusting weights of basis functions, we can get smoothed curves.\nThese weights can be learned from the data (visual results look good).\nBUT: the degree of smoothness is determined by the prior on coefficients.\n\nThe latter problem is worrisome because we do not want to manually adjust an important parameter like that. Ideally, the smoothness of the curve should be dictated by the data itself. Otherwise, we might overfit or remain to linear, so to speak.\nNotice that the parameter to tweak is the standard deviation of the normal distribution of the prior over coefficients. The smaller this is, the more linear a curve we predict. The bigger the more smoothed the fit. So, we want a model that let’s the data decide what the standard deviation is supposed to be for additive offsets to what is otherwise a vanilla regression model. But, hey, that’s exactly what group-level effects do, too!\nConcretely, we want a predictor \\(\\eta\\) which consists of the usual linear regression part \\(X \\beta\\) with an additional part \\(Z b\\), which specifies how much deviate from the vanilla linear prediction. In the case at hand, the matrix \\(Z\\) is, essentially, the basis (rows are for each observation of times, columns correspond to the basis functions). We also want the coefficient’s \\(b\\) to be close to zero but with unknown variance, which is to be inferred from the data. The overal structure of this model is therefore:\n\\[\n\\begin{align*}\n\\eta &= X \\beta + Z b \\\\\nb & \\sim \\mathcal{N}(o, \\sigma_b) \\\\\n\\sigma_b & \\sim \\dots \\text{some prior} \\dots\n\\end{align*}\n\\] And that is exactly the structure of a group-level / random-effects model. Random effects are supposed to be small, where what counts as small enough is to be determined by the data. Likewise, weights of basis functions are suppose to be small and, likewise, what counts as small is not to be fixed by hand but governed by the needs of the data.\nIt is for this reason that brms implements GAMs as, essentially, multi-level models, even though the touch-and-feel maybe different.\n\n\nImplementing GAMs in brms\nTo run a basic GAM in brms, just wrap the predictor to be smoothed in s(), which is brms-wrapper around the mgcv function of the same name.\n\n\nToggle code\nfit_motor <- \n  brms::brm(\n    formula = accel ~ s(times, bs = \"cr\", k = 20),\n    data    = data_motor\n  )\n\n\nHere is the posterior linear predictor for this model, which looks fine:\n\n\nToggle code\nplot_post_linPred(fit_motor, data_motor)\n\n\n\n\n\nIt is interesting to interpret the summary:\n\n\nToggle code\n# interpret this\nsummary(fit_motor)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: accel ~ s(times, bs = \"cr\", k = 20) \n   Data: data_motor (Number of observations: 132) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nSmooth Terms: \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsds(stimes_1)     4.98      1.21     3.16     7.79 1.00      755     1348\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   -25.64      1.95   -29.52   -21.86 1.00     4588     2635\nstimes_1      1.65      0.35     0.96     2.32 1.00     3850     3159\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    22.83      1.47    20.09    25.91 1.00     3865     2890\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe “Smooth Term” sds(stimes_1) is essentially the standard deviation \\(sigma\\) in the equations above. (The “sigma” is the usual standard deviation of the Gaussian likelihood function.) The intercept is the usual intercept, and the term “stimes_1” is the normal linear regressions slope. But these are so distorted by the penalized smoothed in which they are wrapped that is is not prudent (in my understanding; though I am not certain for it) to draw strong conclusions from them.\n\n\nExercises\n\n\n\n\n\n\nExercise 3: Posterior predictives\n\n\n\n\n\nThe previous plot showed ribbons for the 95% HDI for the linear predictor. Make a similar plot for the posterior predictive distribution. Before you code, think about how you expect the plot to differ from the previous one (different curve, more / less uncertainty …).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe expect the central tendency to be similar (albeit a bit more noisy, given sampling inaccuracy), but most of all the ribbons will be broader, given that we quantify uncertaint about where the data points would fall, not about where the central tendency is.\n\n\nToggle code\npostPred <- data_motor |> \n  tidybayes::add_predicted_draws(fit_motor) |> \n  group_by(times,accel) |> \n  summarize(\n    lower  = tidybayes::hdi(.prediction)[1],\n    mean   = mean(.prediction),\n    higher = tidybayes::hdi(.prediction)[2]\n  )\n\npostPred |> \n  ggplot(aes(x = times,  y = accel)) +\n  geom_line(aes(y = mean), color = project_colors[1], size = 2) +\n  geom_ribbon(aes(ymin = lower, ymax = higher), \n              fill = project_colors[7], alpha = 0.3) +\n  geom_point(size = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 4: Posterior predictive check\n\n\n\n\n\nConsult visual posterior predictive checks for the GAM. Does it look alright? Anything systematicaly amiss?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nA simple PPC suggests that the overall distribution of the data is not matched. This could be due to the large uncertainty for early time points, which is not borne out by the data. This explains why the PPCs are flatter, less pronounced.\n\n\nToggle code\npp_check(fit_motor, ndraws = 40)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 5: A distributional GAM\n\n\n\n\n\nGiven the problem above, it makes sense to try a distributional model, regressing the “sigma” parameter of the likelihood function on times. Decide whether it’s better to plot the posterior for the linear predictor or the posterior predictives. Interpret what you see.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nHere is the model fit:\n\n\nToggle code\nfit_motor_distributional <- \n  brms::brm(\n    formula = brms::bf(accel ~ s(times, bs = \"cr\", k = 20),\n                       sigma ~ times),\n    data    = data_motor\n  )\n\n\nWe should consult the posterior predictive because that is where the difference in spread around the linear predictor will show, not in the estimates of the linear predictor.\n\n\nToggle code\npostPred <- data_motor |> \n  tidybayes::add_predicted_draws(fit_motor_distributional) |> \n  group_by(times,accel) |> \n  summarize(\n    lower  = tidybayes::hdi(.prediction)[1],\n    mean   = mean(.prediction),\n    higher = tidybayes::hdi(.prediction)[2]\n  )\n\npostPred |> \n  ggplot(aes(x = times,  y = accel)) +\n  geom_line(aes(y = mean), color = project_colors[1], size = 2) +\n  geom_ribbon(aes(ymin = lower, ymax = higher), \n              fill = project_colors[7], alpha = 0.4) +\n  geom_point(size = 2)\n\n\n\n\n\nWe see a tighter credible interval for early time points, but the change to the non-distributional model is not very large.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 6: A GAM for World Temperature\n\n\n\n\n\nRun a Bayesian GAM for the World Temperature data (aida::data_WorldTemp). Plot the expected posterior predictor of central tendency together with the data. Can you use information from the posterior over model parameters to address the question of whether there is a trend towards higher measurements as time progresses? Compare the conclusions you draw from this linear model to those you would draw from a naive linear model.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFirst run the models.\n\n\nToggle code\nfit_temp_splines <- \n  brms::brm(\n    formula = avg_temp ~ s(year, bs = \"cr\", k=20),\n    data    = aida::data_WorldTemp\n  )\n\nfit_temp_vanilla <- \n  brms::brm(\n    formula = avg_temp ~ year,\n    data    = aida::data_WorldTemp\n  )\n\n\nHere is the posterior predicted central tendency for the GAM (w/ credible intervals):\n\n\nToggle code\npostPred <- aida::data_WorldTemp |> \n  tidybayes::add_predicted_draws(fit_temp_splines) |> \n  group_by(avg_temp, year) |> \n  summarize(\n    lower  = tidybayes::hdi(.prediction)[1],\n    mean   = mean(.prediction),\n    higher = tidybayes::hdi(.prediction)[2]\n  )\n\npostPred |> \n  ggplot(aes(x = year,  y = avg_temp)) +\n  geom_line(aes(y = mean), color = project_colors[1], size = 2) +\n  geom_ribbon(aes(ymin = lower, ymax = higher), \n              fill = project_colors[6], alpha = 0.4) +\n  geom_point(size = 2)\n\n\n\n\n\nWe can compare the summaries of both models:\n\n\nToggle code\ntidybayes::summarise_draws(fit_temp_splines)[1:4,]\n\n\n# A tibble: 4 × 10\n  variable     mean median      sd     mad     q5    q95  rhat ess_bulk ess_tail\n  <chr>       <num>  <num>   <num>   <num>  <num>  <num> <num>    <num>    <num>\n1 b_Interce… 8.31   8.31   0.0199  0.0204  8.28   8.35    1.00    4232.    2703.\n2 bs_syear_1 0.122  0.122  0.00517 0.00513 0.114  0.131   1.00    5295.    3271.\n3 sds_syear… 0.0566 0.0538 0.0183  0.0165  0.0319 0.0907  1.00     699.    1471.\n4 sigma      0.326  0.325  0.0147  0.0143  0.303  0.351   1.00    4744.    3140.\n\n\nToggle code\ntidybayes::summarise_draws(fit_temp_vanilla)[1:4,]\n\n\n# A tibble: 4 × 10\n  variable        mean   median      sd     mad       q5      q95  rhat ess_bulk\n  <chr>          <num>    <num>   <num>   <num>    <num>    <num> <num>    <num>\n1 b_Intercept -3.52    -3.52    6.01e-1 5.84e-1 -4.51    -2.52     1.00    4439.\n2 b_year       0.00628  0.00628 3.19e-4 3.08e-4  0.00575  0.00681  1.00    4443.\n3 sigma        0.405    0.405   1.78e-2 1.75e-2  0.377    0.436    1.00    1490.\n4 lprior      -3.16    -3.16    1.54e-3 1.51e-3 -3.16    -3.16     1.00    1470.\n# ℹ 1 more variable: ess_tail <num>\n\n\nThe population-level slope parameter represents the unsmoothed linear predictor line for the splines model, but it is difficult to interpret (because we have to strip off the smoothing splines, so to speak). There seems to be an indication of a positive linear effect underneath the smooth, but I, personally, would not base firm conclusions on this.\n\n\n\n\n\n\n replace talk of “linear predictor” with central tendency"
  },
  {
    "objectID": "practice-sheets/04b-divergences.html",
    "href": "practice-sheets/04b-divergences.html",
    "title": "Divergent transitions",
    "section": "",
    "text": "Here is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\n\n\nToggle code\n# install packages from CRAN (unless installed)\npckgs_needed <- c(\n  \"tidyverse\",\n  \"brms\",\n  \"rstan\",\n  \"rstanarm\",\n  \"remotes\",\n  \"tidybayes\",\n  \"bridgesampling\",\n  \"shinystan\",\n  \"mgcv\"\n)\npckgs_installed <- installed.packages()[,\"Package\"]\npckgs_2_install <- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]\nif(length(pckgs_2_install)) {\n  install.packages(pckgs_2_install)\n} \n\n# install additional packages from GitHub (unless installed)\nif (! \"aida\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/aida-package\")\n}\nif (! \"faintr\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/faintr\")\n}\nif (! \"cspplot\" %in% pckgs_installed) {\n  remotes::install_github(\"CogSciPrag/cspplot\")\n}\n\n# load the required packages\nx <- lapply(pckgs_needed, library, character.only = TRUE)\nlibrary(aida)\nlibrary(faintr)\nlibrary(cspplot)\n\n# these options help Stan run faster\noptions(mc.cores = parallel::detectCores())\n\n# use the CSP-theme for plotting\ntheme_set(theme_csp())\n\n# global color scheme from CSP\nproject_colors = cspplot::list_colors() |> pull(hex)\n# names(project_colors) <- cspplot::list_colors() |> pull(name)\n\n# setting theme colors globally\nscale_colour_discrete <- function(...) {\n  scale_colour_manual(..., values = project_colors)\n}\nscale_fill_discrete <- function(...) {\n   scale_fill_manual(..., values = project_colors)\n}"
  },
  {
    "objectID": "practice-sheets/04b-divergences.html#a-draw-the-model",
    "href": "practice-sheets/04b-divergences.html#a-draw-the-model",
    "title": "Divergent transitions",
    "section": "3.a Draw the model",
    "text": "3.a Draw the model\nDraw a graphical representation of this mixture model, following the conventions outlined here. Again, any format which we can decipher easily is fine, as long as it is practical (and fun) for you."
  },
  {
    "objectID": "practice-sheets/04b-divergences.html#b-run-the-model-inspect-and-explain-the-divergent-transitions",
    "href": "practice-sheets/04b-divergences.html#b-run-the-model-inspect-and-explain-the-divergent-transitions",
    "title": "Divergent transitions",
    "section": "3.b Run the model, inspect and explain the divergent transitions",
    "text": "3.b Run the model, inspect and explain the divergent transitions\nThe Stan code for this model is shown below and also included in file ADA-W09-Ex3a-8schools-centered.stan.\n\ndata {\n  int<lower=0> N;\n  vector[N] y;\n  vector<lower=0>[N] sigma;\n}\nparameters {\n  real mu;\n  real<lower=0> sigma_prime;\n  vector[N] theta;\n}\nmodel {\n  mu ~ normal(0, 10);\n  sigma_prime ~ cauchy(0, 10);\n  theta ~ normal(mu, sigma_prime);\n  y ~ normal(theta, sigma);\n}\n\n\nToggle code\nstan_fit_3a_8schoolsC <- stan(\n  file = 'stan-files/8schools-centered.stan',\n  data = data_eight_schools,\n  seed = 1969\n)\n\n\nNormally, there are a lot of divergent transitions when you run this code:\n\n\nToggle code\nget_divergent_iterations(stan_fit_3a_8schoolsC) %>% sum()\n\n\n[1] 223\n\n\nLet’s go explore these divergent transitions using shinystan. Execute the command below, go to the tab “Explore” in the Shiny App, select “Bivariate” and explore plots of \\(\\sigma'\\) against \\(\\theta_i\\) for different \\(i\\). Points that experienced divergent transitions are shown in red.\n\n\nToggle code\nshinystan::launch_shinystan(stan_fit_3a_8schoolsC)\n\n\nYou can also produce your own (funnel) plots with the code shown below, which may be even clearer because it uses a log-transform. Again, points with divergencies are shown in red.\n\n\nToggle code\n bayesplot::mcmc_scatter(\n  as.array(stan_fit_3a_8schoolsC),\n  pars = c(\"theta[1]\", \"sigma_prime\"),\n  transform = list(sigma_prime = \"log\"),\n  np = nuts_params(stan_fit_3a_8schoolsC),\n  size = 1\n)\n\n\n\n\n\nExplain in your own intuitive terms why these divergent transitions occur. E.g., you might want to say something like: “Since the step size parameter is …, we see divergencies … because the more … this variable is, the more/less … that variable …”\nSolution:\nSince the step size parameter for approximating the Hamiltonian dynamics is set globally, we run into divergent transitions specifically for cases where \\(\\sigma'\\) is very small (roughly \\(<1\\)), because for such smaller values of \\(\\sigma'\\), the “reasonable” values for \\(\\theta_i\\) are much more constraint / have lower variance than for higher values. That’s why the globally optimal step size leads to divergences inside the narrow part of the “funnel”."
  },
  {
    "objectID": "practice-sheets/04b-divergences.html#c-non-centered-parameterization",
    "href": "practice-sheets/04b-divergences.html#c-non-centered-parameterization",
    "title": "Divergent transitions",
    "section": "3.c Non-centered parameterization",
    "text": "3.c Non-centered parameterization\nAn alternative model, with so-called non-central parameterization does not have this problem with divergent transitions (they can still occur occasionally, though).\nThis non-central model can be written like so:\n\\[\n\\begin{align*}\ny_i & \\sim \\mathcal{N}(\\theta_i, \\sigma_i) \\\\\n\\theta_i & = \\mu + \\sigma' \\eta_i \\\\\n\\eta_i & \\sim \\mathcal{N}(0, 1) \\\\\n\\mu & \\sim \\mathcal{N}(0, 10) \\\\\n\\sigma & \\sim \\text{half-Cauchy}(0, 10) \\\\\n\\end{align*}\n\\]\nImplement and run this model in Stan. Report if you got any divergent transitions, e.g., with command get_divergent_iterations applied to the stanfit object.\nSolution:\n\n\nToggle code\nstan_fit_3c_8schoolsNC <- stan(\n  file = 'stan-files/8schools-non-centered.stan',\n  data = data_eight_schools\n)\n\n\n\n\nToggle code\nget_divergent_iterations(stan_fit_3c_8schoolsNC) %>% sum()\n\n\n[1] 0"
  },
  {
    "objectID": "practice-sheets/04b-divergences.html#explaining-non-central-parameterization",
    "href": "practice-sheets/04b-divergences.html#explaining-non-central-parameterization",
    "title": "Divergent transitions",
    "section": "Explaining non-central parameterization",
    "text": "Explaining non-central parameterization\nLet’s look at a plot similar to the one we looked at for the model with central parameterization in 3.b:\n\n\nToggle code\n bayesplot::mcmc_scatter(\n  as.array(stan_fit_3c_8schoolsNC),\n  pars = c(\"theta[1]\", \"sigma_prime\"),\n  transform = list(sigma_prime = \"log\"),\n  np = nuts_params(stan_fit_3c_8schoolsNC),\n  size = 1\n)\n\n\n\n\n\nWhat is the main striking difference (apart from the presence/absence of divergent transitions)? How is this difference a reason for why divergent transitions can be problematic? Is any estimated posterior mean for any parameter noticeably affected by this?\nSolution:\nThe “funnel” in the non-central model fit is much “deeper”. The samples for \\(\\log \\sigma'\\) stopped at around 1 for the central-parameterization model. But for the non-central one they go to values of -4. While this is in log-scale, it nevertheless shows how the divergencies can cause failure to explore a reasonable chunk of the posterior space. Expectations based on samples with such restrictions can consequently be biased. Indeed, the estimated mean for \\(\\sigma'\\) is discernibly lower for the non-centralized parameterization."
  },
  {
    "objectID": "practice-sheets/00-preamble.html",
    "href": "practice-sheets/00-preamble.html",
    "title": "Bayesian Regression: Theory & Practice",
    "section": "",
    "text": "Here is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\n\n\nToggle code\n# install packages from CRAN (unless installed)\npckgs_needed <- c(\n  \"tidyverse\",\n  \"brms\",\n  \"rstan\",\n  \"rstanarm\",\n  \"remotes\",\n  \"tidybayes\",\n  \"bridgesampling\",\n  \"shinystan\",\n  \"mgcv\"\n)\npckgs_installed <- installed.packages()[,\"Package\"]\npckgs_2_install <- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]\nif(length(pckgs_2_install)) {\n  install.packages(pckgs_2_install)\n} \n\n# install additional packages from GitHub (unless installed)\nif (! \"aida\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/aida-package\")\n}\nif (! \"faintr\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/faintr\")\n}\nif (! \"cspplot\" %in% pckgs_installed) {\n  remotes::install_github(\"CogSciPrag/cspplot\")\n}\n\n# load the required packages\nx <- lapply(pckgs_needed, library, character.only = TRUE)\nlibrary(aida)\nlibrary(faintr)\nlibrary(cspplot)\n\n# these options help Stan run faster\noptions(mc.cores = parallel::detectCores())\n\n# use the CSP-theme for plotting\ntheme_set(theme_csp())\n\n# global color scheme from CSP\nproject_colors = cspplot::list_colors() |> pull(hex)\n# names(project_colors) <- cspplot::list_colors() |> pull(name)\n\n# setting theme colors globally\nscale_colour_discrete <- function(...) {\n  scale_colour_manual(..., values = project_colors)\n}\nscale_fill_discrete <- function(...) {\n   scale_fill_manual(..., values = project_colors)\n}"
  },
  {
    "objectID": "practice-sheets/03b-GLM-exercises.html",
    "href": "practice-sheets/03b-GLM-exercises.html",
    "title": "03b: Generalized linear models (exercises)",
    "section": "",
    "text": "Preamble\nHere is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\n\n\nToggle code\n# install packages from CRAN (unless installed)\npckgs_needed <- c(\n  \"tidyverse\",\n  \"brms\",\n  \"rstan\",\n  \"rstanarm\",\n  \"remotes\",\n  \"tidybayes\",\n  \"bridgesampling\",\n  \"shinystan\",\n  \"mgcv\"\n)\npckgs_installed <- installed.packages()[,\"Package\"]\npckgs_2_install <- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]\nif(length(pckgs_2_install)) {\n  install.packages(pckgs_2_install)\n} \n\n# install additional packages from GitHub (unless installed)\nif (! \"aida\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/aida-package\")\n}\nif (! \"faintr\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/faintr\")\n}\nif (! \"cspplot\" %in% pckgs_installed) {\n  remotes::install_github(\"CogSciPrag/cspplot\")\n}\n\n# load the required packages\nx <- lapply(pckgs_needed, library, character.only = TRUE)\nlibrary(aida)\nlibrary(faintr)\nlibrary(cspplot)\n\n# these options help Stan run faster\noptions(mc.cores = parallel::detectCores())\n\n# use the CSP-theme for plotting\ntheme_set(theme_csp())\n\n# global color scheme from CSP\nproject_colors = cspplot::list_colors() |> pull(hex)\n# names(project_colors) <- cspplot::list_colors() |> pull(name)\n\n# setting theme colors globally\nscale_colour_discrete <- function(...) {\n  scale_colour_manual(..., values = project_colors)\n}\nscale_fill_discrete <- function(...) {\n   scale_fill_manual(..., values = project_colors)\n}\n\n\n\n\nExercise 1: logistic regression\nUse the following data frame:\n\n\nToggle code\n# set up data frame\ndolphin <- aida::data_MT\ndolphin_agg <- dolphin %>% \n  filter(correct == 1) %>% \n  mutate(straight = as.factor(ifelse(prototype_label == \"straight\", 1, 0)),\n         log_RT_s = scale(log(RT)))\n\n\n\n\n\n\n\n\nExercise 1a\n\n\n\n\n\nPlot straight (straight == 1) vs. non-straight (straight == 0) trajectories (y-axis) against log_RT_s and color-code by group.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\ndolphin_agg$straight_numeric <- as.numeric(as.character(dolphin_agg$straight))\n\nggplot(data = dolphin_agg) +\n  geom_point(aes(x = log_RT_s, y = straight_numeric, color = group), \n             # we add a little bit of jitter to make the points better visible\n             position = position_jitter(height = 0.02), alpha = 0.2) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1b\n\n\n\n\n\nRun the appropriate generalized linear model in brms that predicts straight vs. non-straight trajectories based on group, log_RT_s, and their two-way interaction.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\nGlmMdl <- brm(straight ~ log_RT_s * group, \n                 dolphin_agg, cores = 4,\n              family = \"bernoulli\",\n              seed = 123)\nGlmMdl\n\n\n Family: bernoulli \n  Links: mu = logit \nFormula: straight ~ log_RT_s * group \n   Data: dolphin_agg (Number of observations: 1915) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept               0.86      0.07     0.72     1.00 1.00     3479     2887\nlog_RT_s               -0.23      0.07    -0.37    -0.09 1.00     2871     2528\ngrouptouch              0.69      0.11     0.48     0.90 1.00     3636     2716\nlog_RT_s:grouptouch    -0.01      0.11    -0.23     0.20 1.00     2928     3091\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1c\n\n\n\n\n\nDescribe the model predictions based on the posterior means of the population coefficients.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe model predicts that the log odds for the mean log_RT_s in the click group (Intercept = reference level) is 0.86. With every unit of log_RT_s these log odds become smaller by 0.23. The model predicts that the log odds for the mean log_RT_s in the touch group is 1.56 (0.86 + 0.70), i.e. much higher than in the click group. With every unit of log_RT_s these log odds become smaller by 0.22.\nThe baseline difference between click and touch group is compelling with more straight trajectories in the touch group. The effect of log_RT_s is also compelling with less straight trajectories for slower responses. This relationship is not compellingly modulated between the touch and the click group (virtually identical).\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1d\n\n\n\n\n\nExtract the posteriors means and 95% CrIs for the relationships between straight, log_RT_s and group for representative range of log_RT_s values. Plot the logistic regression lines for both groups into one graph. Color code the regression lines according to group.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\n# extract posterior means for model coefficients\npredicted_values <- GlmMdl %>%\n  spread_draws(b_Intercept, b_log_RT_s, b_grouptouch, `b_log_RT_s:grouptouch`) %>%\n  # make a list of relevant value range of logRT\n  mutate(log_RT = list(seq(-3, 7, 0.2))) %>% \n  unnest(log_RT) %>%\n  # transform into proportion space\n  mutate(pred_click = plogis(b_Intercept + b_log_RT_s * log_RT),\n         pred_touch = plogis(b_Intercept + b_log_RT_s * log_RT +\n                               b_grouptouch + `b_log_RT_s:grouptouch` * log_RT)\n         ) %>%\n  group_by(log_RT) %>%\n  summarise(pred_click_m = mean(pred_click, na.rm = TRUE),\n            pred_click_low = quantile(pred_click, prob = 0.025),\n            pred_click_high = quantile(pred_click, prob = 0.975),\n            pred_touch_m = mean(pred_touch, na.rm = TRUE),\n            pred_touch_low = quantile(pred_touch, prob = 0.025),\n            pred_touch_high = quantile(pred_touch, prob = 0.975)\n            ) \n\n# plot predicted values against data\nggplot(data = predicted_values) +\n  geom_hline(yintercept = c(0,1), lty = \"dashed\", color = \"grey\") +\n  geom_point(data = dolphin_agg,\n             aes(x = log_RT_s, y = straight_numeric, color = group), \n             position = position_jitter(height = 0.02), alpha = 0.2) +\n  geom_ribbon(aes(x = log_RT, ymin = pred_click_low, ymax = pred_click_high), alpha = 0.2) +\n  geom_ribbon(aes(x = log_RT, ymin = pred_touch_low, ymax = pred_touch_high), alpha = 0.2) +\n  geom_line(aes(x = log_RT, y = pred_click_m), color = \"#E69F00\", size = 2) +\n  geom_line(aes(x = log_RT, y = pred_touch_m), color = \"#56B4E9\", size = 2) +\n  ylab(\"Predicted prob of straight trajs\") +\n  ylim(-0.3,1.3) +\n  xlim(-3,7)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1e\n\n\n\n\n\nAssume we want to predict correct responses based on condition. We look at the touch group only. Set up a data frame and plot the data as a point plot. (Remember how to jitter the data points)\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\n# set up data frame\ndolphin_agg2 <- dolphin %>% \n filter(group == \"touch\")\n\ndolphin_agg2$correct_numeric <- as.numeric(as.character(dolphin_agg2$correct))\n\nggplot(data = dolphin_agg2) +\n  geom_point(aes(x = condition, y = correct_numeric, color = condition), \n             # we add a little bit of jitter to make the points better visible\n             position = position_jitter(height = 0.02, width = 0.1), alpha = 0.2) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1f\n\n\n\n\n\nRun the appropriate generalized linear model in brms that predicts correct responses based on condition. Extract the posterior means and 95% CrIs for the effect of condition on correct and plot them as points and whiskers into one plot superimposed on the data.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\nGlmMdl2 <- brm(correct ~ condition, \n                 dolphin_agg2, cores = 4,\n              family = \"bernoulli\")\nGlmMdl2\n\n\n Family: bernoulli \n  Links: mu = logit \nFormula: correct ~ condition \n   Data: dolphin_agg2 (Number of observations: 1045) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept            2.31      0.18     1.97     2.69 1.00     3549     2660\nconditionTypical     0.46      0.24    -0.02     0.94 1.00     2862     2605\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nToggle code\n# extract posterior means for model coefficients\npredicted_values <- GlmMdl2 %>%\n  spread_draws(b_Intercept, b_conditionTypical) %>%\n  # transform into proportion space\n  mutate(Atypical = plogis(b_Intercept),\n         Typical = plogis(b_Intercept + b_conditionTypical)\n         ) %>%\n  select(Atypical, Typical) %>% \n  gather(parameter, posterior) %>% \n  group_by(parameter) %>%\n  summarise(mean = mean(posterior, na.rm = TRUE),\n            lower = quantile(posterior, prob = 0.025),\n            upper = quantile(posterior, prob = 0.975)\n            ) \n\n# plot predicted values against data\nggplot(data = predicted_values) +\n  geom_point(data = dolphin_agg2, aes(x = condition, y = correct_numeric, color = condition), \n             # we add a little bit of jitter to make the points better visible\n             position = position_jitter(height = 0.02, width = 0.1), alpha = 0.2) +\n  geom_errorbar(aes(x = parameter, ymin = lower, ymax = upper), \n                width = 0.1, color = \"black\") +\n  geom_point(aes(x = parameter, y = mean, fill = parameter),\n             size = 4, color = \"black\", pch = 21) +\n  ylab(\"Predicted prob of correct responses\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2: Poisson regression\nWe will continue to use dolphin_agg in this exercise.\n\n\n\n\n\n\nExercise 2a\n\n\n\n\n\nPlot the relationship between xpos_flips and log_RT_s in a scatterplot and visually differentiate between conditions as you see fit.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\nggplot(data = dolphin_agg) +\n  geom_point(aes(x = log_RT_s, y = xpos_flips, color = condition), \n             # we add a little bit of jitter to make the points better visible\n             position = position_jitter(height = 0.2), alpha = 0.2) +\n  ylim(-1,8) +\n  xlim(-5,10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2b\n\n\n\n\n\nRun an appropriate generalized regression model for xflips with brms to predict xpos_flips based on log_RT_s, condition, and their two-way interaction.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\nGlmMdl3 <- brm(xpos_flips ~ log_RT_s * condition, \n                 dolphin_agg, cores = 4,\n              family = \"poisson\")\nGlmMdl3\n\n\n Family: poisson \n  Links: mu = log \nFormula: xpos_flips ~ log_RT_s * condition \n   Data: dolphin_agg (Number of observations: 1915) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n                          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                    -0.09      0.05    -0.18    -0.00 1.00     2707\nlog_RT_s                      0.26      0.03     0.20     0.32 1.00     1921\nconditionTypical             -0.18      0.06    -0.29    -0.07 1.00     2538\nlog_RT_s:conditionTypical     0.10      0.04     0.02     0.18 1.00     1879\n                          Tail_ESS\nIntercept                     2628\nlog_RT_s                      2462\nconditionTypical              2673\nlog_RT_s:conditionTypical     2754\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2c\n\n\n\n\n\nExtract the posterior means and 95% CrIs across a range of representative values of log_RT_s (see walkthrough) for both conditions and plot them against the data (as done before in walkthrough and exercise 1).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\npredicted_Poisson_values <- GlmMdl3 %>%\n  spread_draws(b_Intercept, b_log_RT_s, b_conditionTypical, `b_log_RT_s:conditionTypical`) %>%\n  # make a list of relevant value range of logRT\n  mutate(log_RT = list(seq(-5, 10, 0.5))) %>% \n  unnest(log_RT) %>%\n  mutate(pred_atypical = exp(b_Intercept + b_log_RT_s * log_RT),\n         pred_typical = exp(b_Intercept + b_log_RT_s * log_RT +\n                              b_conditionTypical + `b_log_RT_s:conditionTypical` * log_RT)) %>%\n  group_by(log_RT) %>%\n  summarise(pred_atypical_m = mean(pred_atypical, na.rm = TRUE),\n            pred_atypical_low = quantile(pred_atypical, prob = 0.025),\n            pred_atypical_high = quantile(pred_atypical, prob = 0.975),\n            pred_typical_m = mean(pred_typical, na.rm = TRUE),\n            pred_typical_low = quantile(pred_typical, prob = 0.025),\n            pred_typical_high = quantile(pred_typical, prob = 0.975)) \n\n\nggplot(data = predicted_Poisson_values, aes(x = log_RT)) +\n  geom_point(data = dolphin_agg, aes(x = log_RT_s, y = xpos_flips, color = condition), \n             position = position_jitter(height = 0.2), alpha = 0.2) +\n  geom_ribbon(aes(ymin = pred_atypical_low, ymax = pred_atypical_high), alpha = 0.1) +\n  geom_ribbon(aes(ymin = pred_typical_low, ymax = pred_typical_high), alpha = 0.1) +\n  geom_line(aes(y = pred_atypical_m), color = \"#E69F00\", size = 2) +\n  geom_line(aes(y = pred_typical_m),color = \"#56B4E9\", size = 2) +\n  ylab(\"Predicted prob of xflips\") +\n  ylim(-1,10) +\n  xlim(-3,6)\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 3: Logistic regression with binomial likelihood\nBinary logistic regression assumes that the outcome variable comes from a Bernoulli distribution which is a special case of a binomial distribution where the number of trials is \\(n = 1\\) and thus the outcome variable can only be 1 or 0. In contrast, binomial logistic regression assumes that the number of the target events follows a binomial distribution with \\(n\\) trials and probability \\(q\\).\nTake the following subset of the dolphin data frame that only contains correct responses (= 1).\n\n\nToggle code\n# set up data frame\ndolphin_sub <- dolphin %>% \n  filter(correct == 1) %>% \n  mutate(straight = (ifelse(prototype_label == \"straight\", 1, 0)),\n         log_RT_s = scale(log(RT)))\n\n\n\n\n\n\n\n\nExercise 3a\n\n\n\n\n\nFor each subject_id in each group, aggregate the mean log_RT_s, the number of trials that are classified as straight trajectories, and the total number of trials. Plot the proportion of trials that are classified as straight (vs. all trials) trajectories for each subject.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\n# set up data frame\ndolphin_agg3 <- dolphin_sub %>% \n  group_by(subject_id, group) %>% \n  summarize(log_RT_s = mean(log_RT_s),\n            straights = sum(straight),\n            total = n()) \n\n# plot predicted values against data\nggplot(data = dolphin_agg3) +\n  geom_point(aes(x = log_RT_s, y = straights/total, color = group), size = 2, alpha = 0.5) +\n  ylab(\"Proportion of straight trajs\") +\n  ylim(0,1) +\n  xlim(-2.5,2.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 3c\n\n\n\n\n\nFormulate a binomial logistic regression model to predict the proportion of straight trajectories based on log_RT_s, group, and their two-way interaction. Note that these proportional data are not assumed to be generated by a Bernoulli distribution, but a binomial distribution. Take that into account by setting family = binomial(link = \"logit\"). You also need to tell brms about the number of observations by using formula syntax like this: k | trials(N) ~ ... where k is the variable containing the number of “successes” and N the variable containing the number of trials.\nExtract posterior means and 95% CrIs for the effect of log_RT_s for both groups and plot them across a representative range of log_RT_s.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\n# We specify both the number of target events (straights) and the total number of trials (total) wrapped in trials(), which are separated by |. In addition, the family should be “binomial” instead of “bernoulli”.\nGlmMdl4 <- brm(\n  straights | trials(total) ~ log_RT_s * group,  \n  data = dolphin_agg3, \n  family = binomial(link = \"logit\"))\n\nsummary(GlmMdl4)\n\n\n Family: binomial \n  Links: mu = logit \nFormula: straights | trials(total) ~ log_RT_s * group \n   Data: dolphin_agg3 (Number of observations: 108) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept               0.69      0.08     0.54     0.84 1.00     3896     2893\nlog_RT_s                0.47      0.13     0.21     0.73 1.00     3180     2781\ngrouptouch              0.94      0.12     0.70     1.19 1.00     3182     2488\nlog_RT_s:grouptouch    -0.32      0.20    -0.72     0.07 1.00     3207     2587\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nToggle code\n# extract posteriors means and 95% CrIs\npredicted_values <- GlmMdl4 %>%\n  spread_draws(b_Intercept, b_log_RT_s, b_grouptouch, `b_log_RT_s:grouptouch`) %>%\n  # make a list of relevant value range of logRT\n  mutate(log_RT = list(seq(-3, 3, 0.2))) %>% \n  unnest(log_RT) %>%\n  # transform into proportion space\n  mutate(pred_click = plogis(b_Intercept + b_log_RT_s * log_RT),\n         pred_touch = plogis(b_Intercept + b_log_RT_s * log_RT +\n                               b_grouptouch + `b_log_RT_s:grouptouch` * log_RT)\n         ) %>%\n  group_by(log_RT) %>%\n  summarise(pred_click_m = mean(pred_click, na.rm = TRUE),\n            pred_click_low = quantile(pred_click, prob = 0.025),\n            pred_click_high = quantile(pred_click, prob = 0.975),\n            pred_touch_m = mean(pred_touch, na.rm = TRUE),\n            pred_touch_low = quantile(pred_touch, prob = 0.025),\n            pred_touch_high = quantile(pred_touch, prob = 0.975)\n            ) \n\n# plot predicted values against data\nggplot(data = predicted_values) +\n  geom_point(data = dolphin_agg3,\n             aes(x = log_RT_s, y = straights / total, color = group),\n             alpha = 0.2, size = 2) +\n  geom_ribbon(aes(x = log_RT, ymin = pred_click_low, ymax = pred_click_high), alpha = 0.2) +\n  geom_ribbon(aes(x = log_RT, ymin = pred_touch_low, ymax = pred_touch_high), alpha = 0.2) +\n  geom_line(aes(x = log_RT, y = pred_click_m), color = \"#E69F00\", size = 2) +\n  geom_line(aes(x = log_RT, y = pred_touch_m), color = \"#56B4E9\", size = 2) +\n  ylab(\"Predicted prob of straight trajs\") +\n  ylim(0,1) +\n  xlim(-2.5,2.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 3c\n\n\n\n\n\nNow compare the results from this analysis to the results from the model 1b above which you plotted in 1d. How do the model results differ and why could that be? (Feel free to explore the data to understand what is going on.)\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe model in 1b suggested a negative coefficient of reaction time, i.e. slower responses lead to less straight trajectories. The model here suggests a positive coefficient for reaction time, i.e. slower responses lead to more straight trajectories. Given the data, the model, and the priors, this effect is compelling for at least the click group.\nA major difference in the two analyses is that the former analysis looked at all data and disregarded that responses came from clusters of sources. For example, responses that come from one and the same participant are dependent on each other because participants might differ in characteristics relevant to the task, like how fast they move and how many times they move to the target in a straight trajectory. The latter analysis aggregated participants behavior by looking at the proportion of straight trajectories within each subject, thus one data point corresponds to one participant, resulting in data points being independent (at least regarding the participant identity). If all participants showed a negative effect of reaction time on the likelihood of straight trajectories, but participants systematically differ in terms of their baseline correlation between reaction time and likelihood of producing straight trajectories in the opposite direction (positive relationship), we might get discrepancies between these different models. What we ultimately need is to take multiple levels of the data into account simultaneously, which is the topic of next week."
  },
  {
    "objectID": "practice-sheets/02d-MLM-pooling.html",
    "href": "practice-sheets/02d-MLM-pooling.html",
    "title": "Group-level effects, pooling & smoothing",
    "section": "",
    "text": "We can motivate the inclusion of group-level effects in terms of otherwise violated independence assumptions (the reaction times of a single individual are not necessarily independent of each other; some individuals are just slower or faster than others tout court). We can also motivate group-level effects by appeal to their effect of attenuating inference by flexibly weighing information from different groups. A good example of this latter effect arises when the number of observations in each group is not the same. Let’s go and explore!\n\nPreamble\nHere is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\n\n\nToggle code\n# install packages from CRAN (unless installed)\npckgs_needed <- c(\n  \"tidyverse\",\n  \"brms\",\n  \"rstan\",\n  \"rstanarm\",\n  \"remotes\",\n  \"tidybayes\",\n  \"bridgesampling\",\n  \"shinystan\",\n  \"mgcv\"\n)\npckgs_installed <- installed.packages()[,\"Package\"]\npckgs_2_install <- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]\nif(length(pckgs_2_install)) {\n  install.packages(pckgs_2_install)\n} \n\n# install additional packages from GitHub (unless installed)\nif (! \"aida\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/aida-package\")\n}\nif (! \"faintr\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/faintr\")\n}\nif (! \"cspplot\" %in% pckgs_installed) {\n  remotes::install_github(\"CogSciPrag/cspplot\")\n}\n\n# load the required packages\nx <- lapply(pckgs_needed, library, character.only = TRUE)\nlibrary(aida)\nlibrary(faintr)\nlibrary(cspplot)\n\n# these options help Stan run faster\noptions(mc.cores = parallel::detectCores())\n\n# use the CSP-theme for plotting\ntheme_set(theme_csp())\n\n# global color scheme from CSP\nproject_colors = cspplot::list_colors() |> pull(hex)\n# names(project_colors) <- cspplot::list_colors() |> pull(name)\n\n# setting theme colors globally\nscale_colour_discrete <- function(...) {\n  scale_colour_manual(..., values = project_colors)\n}\nscale_fill_discrete <- function(...) {\n   scale_fill_manual(..., values = project_colors)\n}\n\n\n\n\nDifferent ways to a mean\nWe will use the data set of (log) radon measurements that ships with the rstanarm package.\n\n\nToggle code\ndata_radon <- rstanarm::radon |> as_tibble()\ndata_radon\n\n\n# A tibble: 919 × 4\n   floor county log_radon log_uranium\n   <int> <fct>      <dbl>       <dbl>\n 1     1 AITKIN    0.833       -0.689\n 2     0 AITKIN    0.833       -0.689\n 3     0 AITKIN    1.10        -0.689\n 4     0 AITKIN    0.0953      -0.689\n 5     0 ANOKA     1.16        -0.847\n 6     0 ANOKA     0.956       -0.847\n 7     0 ANOKA     0.470       -0.847\n 8     0 ANOKA     0.0953      -0.847\n 9     0 ANOKA    -0.223       -0.847\n10     0 ANOKA     0.262       -0.847\n# ℹ 909 more rows\n\n\nSuppose that we are interested in an estimate of the mean log-radon measured. Easy! We can just take the empirical mean of each measurement:\n\n\nToggle code\n# empirical mean: 1.265\nempirical_mean <- data_radon |> pull(log_radon) |> mean()\nempirical_mean\n\n\n[1] 1.264779\n\n\nBut should we not, somehow, also take into account that these measurements are from different counties? Okay, so let’s just calculate the mean log-random measured for each county, and then take the mean of all those means:\n\n\nToggle code\n# empirical mean of means: 1.38\nemp_mean_of_means <- data_radon |> \n  group_by(county) |> \n  summarize(mean_by_county = mean(log_radon)) |> \n  ungroup() |> \n  pull(mean_by_county) |> \n  mean()\nemp_mean_of_means\n\n\n[1] 1.37991\n\n\nAha! There is a difference between the empirical mean (sample mean; mean of all data points) and the mean-of-means (a.k.a., grand mean). This is because there different numbers of observations for each county:\n\n\nToggle code\ndata_radon |> \n  group_by(county) |> \n  summarize(mean_by_county = mean(log_radon),\n            n_obs_by_county = n())\n\n\n# A tibble: 85 × 3\n   county    mean_by_county n_obs_by_county\n   <fct>              <dbl>           <int>\n 1 AITKIN             0.715               4\n 2 ANOKA              0.891              52\n 3 BECKER             1.09                3\n 4 BELTRAMI           1.19                7\n 5 BENTON             1.28                4\n 6 BIGSTONE           1.54                3\n 7 BLUEEARTH          1.93               14\n 8 BROWN              1.65                4\n 9 CARLTON            0.977              10\n10 CARVER             1.22                6\n# ℹ 75 more rows\n\n\nThe sample mean does not distinguish at all which observation came from which county. The mean-of-means, on the other hand, puts “counties first”, so to speak, and does not acknowledge that the number of observations each county contributed might be different. To understand how this can yield a difference, look at this picture:\n\n\nToggle code\ndata_radon |> \n  group_by(county) |> \n  summarize(mean_by_county = mean(log_radon),\n            n_obs_by_county = n()) |>\n  mutate(county = fct_reorder(county, mean_by_county)) |> \n  ggplot(aes(x = mean_by_county, y = county)) +\n  theme(legend.position=\"none\") +\n  xlab(\"mean log-radon\") + ylab(\"\") +\n  geom_vline(aes(xintercept = empirical_mean), color = project_colors[2], size = 1.5) + \n  geom_vline(aes(xintercept = emp_mean_of_means), color = project_colors[3], size = 1.5) +\n  geom_point(aes(size = n_obs_by_county), alpha = 0.7) +\n  annotate(\"text\", x = empirical_mean - 0.4, y = 70, \n           label = \"sample mean\", color = project_colors[2], size = 7) +\n  annotate(\"text\", x = emp_mean_of_means + 0.52, y = 20, \n           label = \"mean-of-means\", color = project_colors[3], size = 7) +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\n\n\n\n\nThe \\(x\\)-position of the dots represents the mean for each county. The size of the dots represents the number of observations for each county. The sample mean (in red) is lower because some “heavier dots pull it to the left”. Reversely, the mean-of-means (in yellow) is “pulled more towards the right by the lighter dots” (since it doesn’t care about the size of the dots).\nWhich measure is correct? Neither! Or better: both! Or actually: it depends … on what we want. But actually: maybe we should let the data decide.\n\n\nGroup-level effects as smoothing terms\nSuppose we want a Bayesian measure (with quantified uncertainty) of the sample mean and the mean-of-means, how would we do it? (Maybe you want to think about this for a moment, before you uncover the solution below!)\n\n\n\n\n\n\nExercise\n\n\n\n\n\nRetrieve a Bayesian estimate of the sample mean and of the mean-of-means for the log-randon measure.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe sample mean is estimable with an intercept-only model.\n\n\nToggle code\nfit_InterOnly <- brms::brm(\n  formula = log_radon ~ 1,\n  data    = data_radon\n)\n\n\nThe Bayesian estimate of the sample mean is given by the intercept term:\n\n\nToggle code\nsample_mean_estimate <- \n  fit_InterOnly |> tidybayes::tidy_draws() |> \n  pull(b_Intercept) |> \n  aida::summarize_sample_vector(name = \"sample mean\")\nsample_mean_estimate\n\n\n# A tibble: 1 × 4\n  Parameter   `|95%`  mean `95%|`\n  <chr>        <dbl> <dbl>  <dbl>\n1 sample mean   1.22  1.26   1.32\n\n\nThe mean-of-means can be estimated by running a regression model with county as population-level effect.\n\n\nToggle code\n# We need quite high iterations for this to fit properly \n# (likely to do the few observations in many of the groups).\nfit_FE <- brms::brm(\n  formula = log_radon ~ county,\n  prior   = prior(student_t(1,0,10)),\n  iter    = 20000,\n  thin    = 5,\n  data    = data_radon\n)\n\n\nThis model yields estimates for the mean of each county. (There is some data wrangling to do to get at them, given the way categorical factors are treated internally, but that is a different matter). If we average the estimates properly (here using helper functions from the faintr package), we get an estimate of the mean-of-means:\n\n\nToggle code\n# estimated sample mean: 1.381\nmeanOmean_estimate <- faintr::extract_cell_draws(fit_FE) |> \n  pull(draws) |> \n  aida::summarize_sample_vector(name = \"mean-of-means\")\nrbind(sample_mean_estimate, meanOmean_estimate)\n\n\n# A tibble: 2 × 4\n  Parameter     `|95%`  mean `95%|`\n  <chr>          <dbl> <dbl>  <dbl>\n1 sample mean     1.22  1.26   1.32\n2 mean-of-means   1.30  1.38   1.46\n\n\n\n\n\n\n\n\nIn between these two options (no county variable vs. county as a population-level effect), there is a middle path: treating county as a group-level random intercept.\n\n\nToggle code\nfit_RE <- brms::brm(\n  formula = log_radon ~ (1 | county),\n  data    = data_radon\n)\n\n\nIn this model, the intercept term is an estimate of the mean, but it is in between the estimated sample mean and the estimated mean-of-means:\n\n\nToggle code\npooled_mean <- fit_RE |> tidybayes::tidy_draws() |> \n  pull(\"b_Intercept\") |> \n  aida::summarize_sample_vector(name = \"pooled mean\")\nrbind(sample_mean_estimate, meanOmean_estimate, pooled_mean)\n\n\n# A tibble: 3 × 4\n  Parameter     `|95%`  mean `95%|`\n  <chr>          <dbl> <dbl>  <dbl>\n1 sample mean     1.22  1.26   1.32\n2 mean-of-means   1.30  1.38   1.46\n3 pooled mean     1.26  1.35   1.44\n\n\nThis estimate is nuanced. It does take all data points into account (unlike the mean-of-mean). But (unlike the sample mean), it does weigh some data observations more heavily than others. In particular, counties with few but extreme observations receive, so to speak, less attention because “we explain away these observations as flukes for a given county”. In other words, we can think of group-level modeling also as a way of regularizing inference to differentially weigh observations, depending on which group they originated from.\nOkay, this may sound like a plausible rationalization of what one could do, but how do we know that the model actually behaves in this way? – By looking at what the model would predict for different counties. So, here is a plot of the a posteriori expected measurement for each county (ordered by size) together with the empirically observed mean-by-county:\n\n\nToggle code\nplotData_RE <- data_radon |> \n  group_by(county) |> \n  summarize(mean_by_county = mean(log_radon),\n            n_observations = n()) |> \n  tidybayes::add_epred_draws(\n    object = fit_RE\n  ) |> \n  group_by(county, mean_by_county, n_observations) |> \n  summarize(\n    prediction_lower = tidybayes::hdi(.epred)[1],\n    prediction_mean = mean(.epred),\n    prediction_higher = tidybayes::hdi(.epred)[2]\n    ) |> \n  ungroup() |> \n  mutate(county = fct_reorder(county, n_observations))\n\nplotData_RE |> \n  ggplot(aes(x = prediction_mean , y = county), size = 2) +\n  geom_point(aes(x = mean_by_county), color = project_colors[2]) +\n  geom_errorbar(aes(xmin = prediction_lower, xmax = prediction_higher), \n                color = \"gray\", alpha = 0.8) +\n  geom_segment(aes(y = county, yend=county, x = mean_by_county, xend=prediction_mean),\n              color = project_colors[2]) +\n  geom_point() +\n  ylab(\"\") +\n  xlab(\"mean log-radon\")\n\n\n\n\n\nThis graph shows the counties on the \\(y\\)-axis, ordered number of observation from highest on the top to lowest on the bottom. The black dots are the means of the posterior predictive for each county (the gray error bars are 95% credible intervals for these estimates. The red dots are the empirically observed means for each county (the red lines indicating the differences between prediction and observation for each county).\nThis graph shows:\n\nThe higher the number of observations, the less uncertainty about the prediction.\nThe higher the number of observations, the less difference between prediction and observation.\n\nIt is the latter observation that lends credence to the interpretation above: the effect of random effects is differential weighing of observations in a data-driven manner; low certainty cases receive less weight, as they should."
  },
  {
    "objectID": "practice-sheets/04a-MCMC-diagnostics.html",
    "href": "practice-sheets/04a-MCMC-diagnostics.html",
    "title": "MCMC diagnostics (demonstrations)",
    "section": "",
    "text": "Preamble\nHere is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\n\n\nToggle code\n# install packages from CRAN (unless installed)\npckgs_needed <- c(\n  \"tidyverse\",\n  \"brms\",\n  \"rstan\",\n  \"rstanarm\",\n  \"remotes\",\n  \"tidybayes\",\n  \"bridgesampling\",\n  \"shinystan\",\n  \"mgcv\"\n)\npckgs_installed <- installed.packages()[,\"Package\"]\npckgs_2_install <- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]\nif(length(pckgs_2_install)) {\n  install.packages(pckgs_2_install)\n} \n\n# install additional packages from GitHub (unless installed)\nif (! \"aida\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/aida-package\")\n}\nif (! \"faintr\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/faintr\")\n}\nif (! \"cspplot\" %in% pckgs_installed) {\n  remotes::install_github(\"CogSciPrag/cspplot\")\n}\n\n# load the required packages\nx <- lapply(pckgs_needed, library, character.only = TRUE)\nlibrary(aida)\nlibrary(faintr)\nlibrary(cspplot)\n\n# these options help Stan run faster\noptions(mc.cores = parallel::detectCores())\n\n# use the CSP-theme for plotting\ntheme_set(theme_csp())\n\n# global color scheme from CSP\nproject_colors = cspplot::list_colors() |> pull(hex)\n# names(project_colors) <- cspplot::list_colors() |> pull(name)\n\n# setting theme colors globally\nscale_colour_discrete <- function(...) {\n  scale_colour_manual(..., values = project_colors)\n}\nscale_fill_discrete <- function(...) {\n   scale_fill_manual(..., values = project_colors)\n}\n\n\n\n\nToggle code\ndolphin <- aida::data_MT\nmy_scale <- function(x) c(scale(x))\n\n\nThis tutorial provides demonstrations of how to check the quality of MCMC samples obtained from brms model fits.\n\n\nA good model\nTo have something to go on, here are two model fits, one of this is good, the other is … total crap. The first model fits a smooth line to the average world temperature. (We need to set the seed here to have reproducible results.)\n\n\nToggle code\nfit_good <- brm(\n  formula = avg_temp ~ s(year), \n  data = aida::data_WorldTemp,\n  seed = 1969\n) \n\n\nHere is a quick visualization of the model’s posterior prediction:\n\n\nToggle code\nconditional_effects(fit_good)\n\n\n\n\n\nThe good model is rather well behaved. Here is a generic plot of its posterior fits and traceplots:\n\n\nToggle code\nplot(fit_good)\n\n\n\n\n\nTraceplots look like hairy caterpillar madly-in-love with each other. The world is good.\nWe can check \\(\\hat{R}\\) and effective sample sizes also in the summary of the model:\n\n\nToggle code\nsummary(fit_good)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: avg_temp ~ s(year) \n   Data: aida::data_WorldTemp (Number of observations: 269) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nSmooth Terms: \n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsds(syear_1)     3.56      1.08     1.96     6.17 1.00      907     1689\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     8.31      0.02     8.27     8.35 1.00     3732     2652\nsyear_1      14.55      2.25    10.11    19.08 1.00     2182     2371\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.33      0.01     0.30     0.36 1.00     3823     2962\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nImportantly, the summary of the model contains a warning message about one divergent transition. We are recommended to check the pairs() plot, so here goes:\n\n\nToggle code\npairs(fit_good)\n\n\n\n\n\nThis is actually not too bad. (Wait until you see a terrible case below!)\nWe can try to fix this problem with a single divergent transition by doing as recommended by the warning message, namely increasing the adapt_delta parameter in the control structure:\n\n\nToggle code\nfit_good_adapt <- brm(\n  formula = avg_temp ~ s(year), \n  data = aida::data_WorldTemp,\n  seed = 1969,\n  control = list(adapt_delta=0.9),\n) \n\nsummary(fit_good_adapt)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: avg_temp ~ s(year) \n   Data: aida::data_WorldTemp (Number of observations: 269) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nSmooth Terms: \n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsds(syear_1)     3.59      1.09     1.97     6.30 1.00     1003     1613\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     8.31      0.02     8.27     8.35 1.00     3632     2619\nsyear_1      14.59      2.40     9.96    19.34 1.00     2425     2510\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.33      0.01     0.30     0.36 1.00     3588     2868\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThat looks better, but what did we just do? — When the sampler “warms up”, it tries to find good parameter values for the case at hand. The adapt_delta parameter is the minimum amount of accepted proposals (where to jump next) before “warm up” counts as done and successfull. So with a small problem like this, just making the adaptation more ambitious may have have solved the problem. It has also, however, made the sampling slower, less efficient.\nA powerful interactive tool for exploring a fitted model (diagnostics and more) is shinystan:\n\n\nToggle code\nshinystan::launch_shinystan(fit_good_adapt)\n\n\n\n\nA terrible model\nThe main (maybe only) reason for serious problems with the NUTS sampling is this: sampling issues arise for bad models. So, let’s come up with a really stupid model.\nHere’s a model that is like the previous but adds a second predictor., This second predictor is intended to be a normal (non-smoothed) regression coefficient that is almost identical to the original year information. You may already intuit that this cannot possibly be a good idea; the model is notionally deficient. So, we expect nightmares during sampling:\n\n\nToggle code\nfit_bad <-\n  brms::brm(\n  formula = avg_temp ~ s(year) + year_perturbed, \n  data = aida::data_WorldTemp |> mutate(year_perturbed = rnorm(1,year,0.001)),\n  seed = 1969\n) \n\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\n\nThis model is deliberately set up to be stupid (and to mislead you). If you don’t like being held in the dark, try to find the mistake already.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSee below.\n\n\n\n\n\n\n\n\nToggle code\nsummary(fit_bad)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: avg_temp ~ s(year) + year_perturbed \n   Data: mutate(aida::data_WorldTemp, year_perturbed = rnor (Number of observations: 269) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nSmooth Terms: \n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsds(syear_1)     3.61      1.12     1.93     6.31 1.00     1149     1664\n\nPopulation-Level Effects: \n                        Estimate         Est.Error          l-95% CI\nIntercept      12968495823138.16 17682568304044.28 -9475056761973.39\nyear_perturbed    -7410563481.59    10104317163.83   -29387300537.70\nsyear_1                    14.55              2.33             10.00\n                        u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      51427814527605.04 1.97        6       18\nyear_perturbed     5414314087.31 1.97        6       18\nsyear_1                    19.21 1.00     3004     2309\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.33      0.02     0.30     0.36 1.00     4152     2135\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nIndeed, that looks pretty bad. We managed to score badly on all major accounts:\n\nlarge \\(\\hat{R}\\)\nextremely poor efficient sample size\nridiculously far ranging posterior estimates for the main model components\ntons of divergent transitions\nmaximum treedepth reached more often than hipster touches their phone in a week\n\nSome of these caterpillars look like they are in a vicious rose war:\n\n\nToggle code\nplot(fit_bad)\n\n\n\n\n\nWe also see that that the intercept of and the slope for year_perturbed are the main troublemakers (in terms of traceplots).\nInterestingly, a simple posterior check doesn’t look too bad:\n\n\nToggle code\npp_check(fit_bad)\n\n\n\n\n\nThis shows that the warning messages (from Stan) shoult be taken seriously. The samples cannot be trusted, even if a posterior predictive check looks agreeable.\n\n\n\n\n\n\nExercise 1\n\n\n\n\n\nExtract information about \\(\\hat{R}\\) and the ratio of efficient samples with functions brms::rhat and brms::neff_ratio.\nInterpret what you see: why are these numbers not good.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFor R-hat, we do:\n\n\nToggle code\nbrms::rhat(fit_bad)\n\n\n     b_Intercept b_year_perturbed       bs_syear_1      sds_syear_1 \n       1.9735385        1.9735385        1.0011736        1.0028354 \n           sigma     s_syear_1[1]     s_syear_1[2]     s_syear_1[3] \n       1.0022229        1.0015566        1.0002479        1.0008396 \n    s_syear_1[4]     s_syear_1[5]     s_syear_1[6]     s_syear_1[7] \n       1.0005625        1.0001094        1.0005158        1.0000122 \n    s_syear_1[8]           lprior             lp__ \n       0.9998473        1.0028307        1.0020318 \n\n\nThese numbers are bad, since they ought to be close to 1, which is the ideal case when chains are indistinguishable (roughly put).\nFor the efficient-sample ratio:\n\n\nToggle code\nbrms::neff_ratio(fit_bad)\n\n\n     b_Intercept b_year_perturbed       bs_syear_1      sds_syear_1 \n     0.001379397      0.001379397      0.577284933      0.287347238 \n           sigma     s_syear_1[1]     s_syear_1[2]     s_syear_1[3] \n     0.533684315      0.750952286      0.647515129      0.719913789 \n    s_syear_1[4]     s_syear_1[5]     s_syear_1[6]     s_syear_1[7] \n     0.826111497      0.769837302      0.831758911      0.824367629 \n    s_syear_1[8]           lprior             lp__ \n     0.769685186      0.287386677      0.259298988 \n\n\nThese numbers are also poor, because we would like them, ideally, to be 1. However, low efficiency of samples is not necessary a sign that the fit cannot be trusted, just that the sampler has a hard time beating autocorrelation.\n\n\n\n\n\n\nHave a look at the pairs() plot:\n\n\nToggle code\npairs(fit_bad)\n\n\n\n\n\nAha, there we see a clear problem! The joint posterior for the intercept and the slope for year_perturbed looks like a line. This means that these parameters could in principle do the same “job”.\nThis suggests a possible solution strategy. The model is too unconstrained. It can allow these two parameters meander to wherever they want (or so it seems). We could therefore try honing them in by specifying priors, like so:\n\n\nToggle code\nfit_bad_wPrior <- brm(\n  formula = avg_temp ~ s(year) + year_perturbed, \n  data = aida::data_WorldTemp |> mutate(year_perturbed = rnorm(1,year,0.001)),\n  seed = 1969,\n  prior = prior(\"student_t(1,0,5)\", coef = \"year_perturbed\")\n) \n\nsummary(fit_bad_wPrior)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: avg_temp ~ s(year) + year_perturbed \n   Data: mutate(aida::data_WorldTemp, year_perturbed = rnor (Number of observations: 269) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nSmooth Terms: \n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsds(syear_1)     3.56      1.08     1.99     6.24 1.00     1041     1569\n\nPopulation-Level Effects: \n               Estimate Est.Error   l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept       -489.40  68742.97 -104969.75 99623.82 1.01     1649      453\nyear_perturbed     0.28     39.28     -56.92    59.99 1.01     1649      453\nsyear_1           14.60      2.40      10.04    19.45 1.00     1937     2511\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.33      0.01     0.30     0.36 1.00     4167     3295\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWell, alright! That isn’t too bad anymore. But it is still clear from the posterior pairs plot that this model has two parameters that steal each other’s show. The model remains a bad model … for our data.\n\n\nToggle code\npairs(fit_bad)\n\n\n\n\n\nHere’s what’s wrong: year_perturbed is a constant! The model is a crappy model of the data, because the data is not what we thought it would be. Check it out:\n\n\nToggle code\naida::data_WorldTemp |> mutate(year_perturbed = rnorm(1,year,0.001))\n\n\n# A tibble: 269 × 5\n    year anomaly uncertainty avg_temp year_perturbed\n   <dbl>   <dbl>       <dbl>    <dbl>          <dbl>\n 1  1750  -1.41        NA        7.20          1750.\n 2  1751  -1.52        NA        7.09          1750.\n 3  1753  -1.07         1.3      7.54          1750.\n 4  1754  -0.614        1.09     8.00          1750.\n 5  1755  -0.823        1.24     7.79          1750.\n 6  1756  -0.547        1.28     8.06          1750.\n 7  1757  -0.438        1.31     8.17          1750.\n 8  1758  -2.42         1.76     6.19          1750.\n 9  1759  -1.53         2.25     7.08          1750.\n10  1760  -2.46         2.75     6.14          1750.\n# ℹ 259 more rows\n\n\nSo, we basically ran a model with two intercepts!?! 😳\nLet’s try again:\n\n\nToggle code\ndata_WorldTemp_perturbed <- aida::data_WorldTemp |> \n    mutate(year_perturbed = rnorm(nrow(aida::data_WorldTemp),year, 50))\ndata_WorldTemp_perturbed\n\n\n# A tibble: 269 × 5\n    year anomaly uncertainty avg_temp year_perturbed\n   <dbl>   <dbl>       <dbl>    <dbl>          <dbl>\n 1  1750  -1.41        NA        7.20          1826.\n 2  1751  -1.52        NA        7.09          1802.\n 3  1753  -1.07         1.3      7.54          1745.\n 4  1754  -0.614        1.09     8.00          1783.\n 5  1755  -0.823        1.24     7.79          1836.\n 6  1756  -0.547        1.28     8.06          1771.\n 7  1757  -0.438        1.31     8.17          1767.\n 8  1758  -2.42         1.76     6.19          1860.\n 9  1759  -1.53         2.25     7.08          1788.\n10  1760  -2.46         2.75     6.14          1772.\n# ℹ 259 more rows\n\n\nThat’s more like what we thought it was: year_perturbed is supposed to be noisy version of the actual year. So, let’s try again, leaving out the smoothing, just for some more chaos-loving fun:\n\n\nToggle code\nfit_bad_2 <- brm(\n  formula = avg_temp ~ year + year_perturbed, \n  data = data_WorldTemp_perturbed,\n  seed = 1969,\n  prior = prior(\"student_t(1,0,5)\", coef = \"year_perturbed\")\n) \n\nsummary(fit_bad_2)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: avg_temp ~ year + year_perturbed \n   Data: data_WorldTemp_perturbed (Number of observations: 269) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept         -3.48      0.59    -4.67    -2.33 1.00     3939     2827\nyear               0.01      0.00     0.01     0.01 1.00     3326     2083\nyear_perturbed     0.00      0.00    -0.00     0.00 1.00     3397     2269\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.41      0.02     0.37     0.44 1.00     1426     1436\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThere are no warnings, so this model must be good, right? – No!\nIf we check the pairs plot, we see that we now have introduced a fair correlation between the two predictor variables.\n\n\nToggle code\npairs(fit_bad_2)\n\n\n\n\n\nWe should just not have year_perturbed; it’s nonsense, and it shows in the diagnostics.\nYou can diagnose more using shinystan:\n\n\nToggle code\nshinystan::launch_shinystan(fit_bad)"
  },
  {
    "objectID": "practice-sheets/01g-cheat-sheet.html",
    "href": "practice-sheets/01g-cheat-sheet.html",
    "title": "Cheat sheet: common things to do with BRMS",
    "section": "",
    "text": "This document provides a cursory run-down of common operations and manipulations for working with the brms package."
  },
  {
    "objectID": "practice-sheets/01g-cheat-sheet.html#updating-a-model",
    "href": "practice-sheets/01g-cheat-sheet.html#updating-a-model",
    "title": "Cheat sheet: common things to do with BRMS",
    "section": "Updating a model",
    "text": "Updating a model\nUsing stats::update(), refit a model based on an existing model fit, keeping everything as is, except for what is explicitly set:\n\n\nToggle code\n# take existing fit, refit on smaller data set, just take 100 samples (all else equal)\nfit_first_five <- \n  stats::update(\n    object = fit_MC,\n    iter = 100,\n    # use first five participants only\n    newdata = data_MC |> filter(submission_id >= 8550)\n  )"
  },
  {
    "objectID": "practice-sheets/01g-cheat-sheet.html#formula-syntax",
    "href": "practice-sheets/01g-cheat-sheet.html#formula-syntax",
    "title": "Cheat sheet: common things to do with BRMS",
    "section": "Formula syntax",
    "text": "Formula syntax\nThe basic form of a brms formula is: response ~ pterms + (gterms | group)\n\nMulti-level modeling\n\n(gterms || group) : suppress correlation between gterms\n(gterms | g1 + g2) : syntactic sugar for (gterms | g1) + (gterms | g2)\n(gterms | g1 : g2) : all combinations of g1 and g2 (Cartesian product)\n(gterms | g1 / g2) : nesting g2 within g1; equals (gterms | g1) + (gterms | g1 : g2)\n(gterms | IDx | group) : correlation for all group-level categories with IDx\n\nuseful for multi-formula models (e.g., non-linear models)"
  },
  {
    "objectID": "practice-sheets/01g-cheat-sheet.html#summaries",
    "href": "practice-sheets/01g-cheat-sheet.html#summaries",
    "title": "Cheat sheet: common things to do with BRMS",
    "section": "Summaries",
    "text": "Summaries\nStandard summary of the model fit:\n\n\nToggle code\nsummary(fit_MC)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RT ~ condition + (1 + condition + shape | submission_id) \n   Data: data_MC (Number of observations: 2519) \n  Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;\n         total post-warmup draws = 8000\n\nGroup-Level Effects: \n~submission_id (Number of levels: 50) \n                                               Estimate Est.Error l-95% CI\nsd(Intercept)                                     46.92      8.33    31.75\nsd(conditionreaction)                             27.09     12.05     3.76\nsd(conditiondiscrimination)                       89.51     12.24    68.05\nsd(shapesquare)                                   18.98     10.01     1.38\ncor(Intercept,conditionreaction)                  -0.37      0.29    -0.81\ncor(Intercept,conditiondiscrimination)             0.65      0.17     0.27\ncor(conditionreaction,conditiondiscrimination)    -0.28      0.29    -0.80\ncor(Intercept,shapesquare)                        -0.05      0.34    -0.65\ncor(conditionreaction,shapesquare)                -0.09      0.40    -0.81\ncor(conditiondiscrimination,shapesquare)           0.26      0.31    -0.44\n                                               u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)                                     64.77 1.00     4247     5883\nsd(conditionreaction)                             50.76 1.01     1214     1713\nsd(conditiondiscrimination)                      116.00 1.00     3479     4885\nsd(shapesquare)                                   38.64 1.00     1648     3073\ncor(Intercept,conditionreaction)                   0.34 1.00     4375     4644\ncor(Intercept,conditiondiscrimination)             0.93 1.01     1941     3167\ncor(conditionreaction,conditiondiscrimination)     0.31 1.00     1964     3367\ncor(Intercept,shapesquare)                         0.69 1.00     4366     4588\ncor(conditionreaction,shapesquare)                 0.70 1.00     3103     4868\ncor(conditiondiscrimination,shapesquare)           0.80 1.00     6683     4826\n\nPopulation-Level Effects: \n                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                 441.66      9.02   423.90   459.44 1.00     5266\nconditionreaction        -130.74      8.61  -147.84  -113.57 1.00    10357\nconditiondiscrimination    74.14     14.55    45.69   102.73 1.00     4769\n                        Tail_ESS\nIntercept                   5661\nconditionreaction           5885\nconditiondiscrimination     5456\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma   135.47      1.99   131.62   139.36 1.00    11691     6184\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nSame in tidy format:\n\n\nToggle code\ntidybayes::summarise_draws(fit_MC)\n\n\n# A tibble: 216 × 10\n   variable         mean   median     sd    mad       q5      q95  rhat ess_bulk\n   <chr>           <num>    <num>  <num>  <num>    <num>    <num> <num>    <num>\n 1 b_Intercept   442.     442.     9.02   8.91   427.     456.     1.00    5266.\n 2 b_condition… -131.    -131.     8.61   8.35  -145.    -117.     1.00   10357.\n 3 b_condition…   74.1     74.2   14.5   14.6     50.4     98.2    1.00    4769.\n 4 sd_submissi…   46.9     46.5    8.33   8.12    34.3     61.2    1.00    4247.\n 5 sd_submissi…   27.1     27.3   12.1   12.2      6.68    47.0    1.01    1214.\n 6 sd_submissi…   89.5     88.6   12.2   12.1     70.9    111.     1.00    3479.\n 7 sd_submissi…   19.0     19.1   10.0   10.9      2.70    35.4    1.00    1648.\n 8 cor_submiss…   -0.367   -0.411  0.295  0.276   -0.765    0.199  1.00    4375.\n 9 cor_submiss…    0.650    0.666  0.170  0.176    0.343    0.900  1.01    1941.\n10 cor_submiss…   -0.275   -0.287  0.295  0.314   -0.738    0.217  1.00    1964.\n# ℹ 206 more rows\n# ℹ 1 more variable: ess_tail <num>\n\n\nSummary of just the fixed effects:\n\n\nToggle code\nbrms::fixef(fit_MC)\n\n\n                          Estimate Est.Error       Q2.5     Q97.5\nIntercept                441.66488  9.017353  423.89854  459.4419\nconditionreaction       -130.73573  8.614385 -147.83743 -113.5658\nconditiondiscrimination   74.14105 14.546332   45.69221  102.7329\n\n\nSummary of just the random effects (this is huge, so output suppressed):\n\n\nToggle code\nbrms::ranef(fit_MC)"
  },
  {
    "objectID": "practice-sheets/01g-cheat-sheet.html#retrieve-names-of-model-variables",
    "href": "practice-sheets/01g-cheat-sheet.html#retrieve-names-of-model-variables",
    "title": "Cheat sheet: common things to do with BRMS",
    "section": "Retrieve names of model variables",
    "text": "Retrieve names of model variables\n\n\nToggle code\ntidybayes::get_variables(fit_MC)[1:10]\n\n\n [1] \"b_Intercept\"                                                  \n [2] \"b_conditionreaction\"                                          \n [3] \"b_conditiondiscrimination\"                                    \n [4] \"sd_submission_id__Intercept\"                                  \n [5] \"sd_submission_id__conditionreaction\"                          \n [6] \"sd_submission_id__conditiondiscrimination\"                    \n [7] \"sd_submission_id__shapesquare\"                                \n [8] \"cor_submission_id__Intercept__conditionreaction\"              \n [9] \"cor_submission_id__Intercept__conditiondiscrimination\"        \n[10] \"cor_submission_id__conditionreaction__conditiondiscrimination\""
  },
  {
    "objectID": "practice-sheets/01g-cheat-sheet.html#tidy-samples-with-tidybayes",
    "href": "practice-sheets/01g-cheat-sheet.html#tidy-samples-with-tidybayes",
    "title": "Cheat sheet: common things to do with BRMS",
    "section": "Tidy samples with tidybayes",
    "text": "Tidy samples with tidybayes\nRetrieve all samples with tidybayes::tidy_draws():\n\n\nToggle code\ntidybayes::tidy_draws(fit_MC)\n\n\n# A tibble: 8,000 × 225\n   .chain .iteration .draw b_Intercept b_conditionreaction\n    <int>      <int> <int>       <dbl>               <dbl>\n 1      1          1     1        442.               -135.\n 2      1          2     2        435.               -120.\n 3      1          3     3        441.               -137.\n 4      1          4     4        430.               -129.\n 5      1          5     5        439.               -140.\n 6      1          6     6        439.               -140.\n 7      1          7     7        439.               -129.\n 8      1          8     8        448.               -136.\n 9      1          9     9        434.               -124.\n10      1         10    10        438.               -128.\n# ℹ 7,990 more rows\n# ℹ 220 more variables: b_conditiondiscrimination <dbl>,\n#   sd_submission_id__Intercept <dbl>,\n#   sd_submission_id__conditionreaction <dbl>,\n#   sd_submission_id__conditiondiscrimination <dbl>,\n#   sd_submission_id__shapesquare <dbl>,\n#   cor_submission_id__Intercept__conditionreaction <dbl>, …"
  },
  {
    "objectID": "practice-sheets/01g-cheat-sheet.html#getting-summaries-for-samples",
    "href": "practice-sheets/01g-cheat-sheet.html#getting-summaries-for-samples",
    "title": "Cheat sheet: common things to do with BRMS",
    "section": "Getting summaries for samples",
    "text": "Getting summaries for samples\nTo get (Bayesian) summary statistics for a vector of samples from a parameter you can do this:\n\n\nToggle code\nposterior_Intercept <- \n  tidybayes::tidy_draws(fit_MC) |> \n  dplyr::pull(\"b_Intercept\")\n\n\nThe tidybayes::hdi function gives the upper and lower bound of a Bayesian credible interval:\n\n\nToggle code\ntidybayes::hdi(posterior_Intercept, credMass = 0.90)\n\n\n         [,1]     [,2]\n[1,] 424.5264 459.9931\n\n\nThe function aida::summarize_sample_vector does so, too.\n\n\nToggle code\naida::summarize_sample_vector(posterior_Intercept, name = \"Intercept\")\n\n\n# A tibble: 1 × 4\n  Parameter `|95%`  mean `95%|`\n  <chr>      <dbl> <dbl>  <dbl>\n1 Intercept   425.  442.   460.\n\n\nHere is how you can do this for several vectors at once:\n\n\nToggle code\ntidybayes::tidy_draws(fit_MC) |> \n  dplyr::select(starts_with(\"b_\")) |> \n  pivot_longer(cols = everything()) |> \n  group_by(name) |> \n  reframe(aida::summarize_sample_vector(value)[-1])\n\n\n# A tibble: 3 × 4\n  name                      `|95%`   mean `95%|`\n  <chr>                      <dbl>  <dbl>  <dbl>\n1 b_Intercept                425.   442.    460.\n2 b_conditiondiscrimination   45.2   74.1   102.\n3 b_conditionreaction       -147.  -131.   -113."
  },
  {
    "objectID": "practice-sheets/01g-cheat-sheet.html#population-level-parameters",
    "href": "practice-sheets/01g-cheat-sheet.html#population-level-parameters",
    "title": "Cheat sheet: common things to do with BRMS",
    "section": "Population-level parameters",
    "text": "Population-level parameters\nTo plot the posteriors over model paramters, you can use various plots from the bayesplot package (here information here):\n\n\nToggle code\nposterior_draws <- brms::as_draws_matrix(fit_MC)[,c(\"b_conditionreaction\", \"b_conditiondiscrimination\")]\nbayesplot::mcmc_areas(posterior_draws)\n\n\n\n\n\nOr, use the tidybayes package:\n\n\nToggle code\nfit_MC |> \n  tidy_draws() |> \n  select(starts_with(\"b_con\")) |> \n  rename(reaction = b_conditionreaction,\n         discrimination = b_conditiondiscrimination) |> \n  pivot_longer(cols = everything()) |> \n  ggplot(aes(x = value, y = name)) +\n  tidybayes::stat_halfeye(fill = project_colors[1]) +\n  ylab(\"\") +\n  geom_vline(aes(xintercept = 0), color = project_colors[2], size = 2)"
  },
  {
    "objectID": "practice-sheets/01g-cheat-sheet.html#group-level-parameters",
    "href": "practice-sheets/01g-cheat-sheet.html#group-level-parameters",
    "title": "Cheat sheet: common things to do with BRMS",
    "section": "Group-level parameters",
    "text": "Group-level parameters\nHere is an example of plotting the posterior for random effects (here: the by-subject random intercepts):\n\n\nToggle code\nranef(fit_MC)$submission_id[,,\"Intercept\"] |> \n  as.data.frame() |> \n  rownames_to_column(\"submission_id\") |> \n  ggplot(aes(y = submission_id, x = Estimate)) +\n  geom_errorbar(aes(xmin = `Q2.5`, xmax = `Q97.5`), \n                color = project_colors[6], alpha = 0.7)+\n  geom_vline(aes(xintercept = 0), color = project_colors[1], \n             size = 2, alpha = 0.8) +\n  geom_point(color = project_colors[2], size = 2)"
  },
  {
    "objectID": "practice-sheets/01g-cheat-sheet.html#visual-ppcs",
    "href": "practice-sheets/01g-cheat-sheet.html#visual-ppcs",
    "title": "Cheat sheet: common things to do with BRMS",
    "section": "Visual PPCs",
    "text": "Visual PPCs\nUse tools from the bayesplot package. The basic bayesplot::pp_check() plots the distribution of ndraws samples from the posterior (data) predictive against the distribution of the data the model was trained on:\n\n\nToggle code\nbayesplot::pp_check(fit_MC, ndraws = 30)\n\n\n\n\n\nThere are many tweaks to pp_check:\n\n\nToggle code\nbayesplot::pp_check(fit_MC, ndraws = 30, type = \"hist\")\n\n\n\n\n\nYou can also directly use underlying functions like ppc_stat. See help(\"PPC-overview\") and help(\"PPD-overview\") for what is available.\n\n\nToggle code\npredictive_samples <- brms::posterior_predict(fit_MC, ndraws = 1000)\npredictive_samples[1:5, 1:5] \n\n\n         [,1]     [,2]     [,3]     [,4]     [,5]\n[1,] 532.2485 532.2328 132.7101 258.2333 285.9759\n[2,] 185.3769 235.1609 449.1151 349.6656 402.6897\n[3,] 246.7097 295.5252 302.1503 264.4449 293.6660\n[4,] 309.0551 401.6308 320.9148 348.8615 272.9958\n[5,] 255.1437 330.5753 542.3492 398.5456 282.3342\n\n\nToggle code\nbayesplot::ppc_stat(\n  y    = data_MC$RT, \n  yrep = predictive_samples,\n  # specify the test statistic of interest\n  stat = function(x){quantile(x, 0.8)})"
  },
  {
    "objectID": "practice-sheets/01g-cheat-sheet.html#extracting-samples-from-the-posterior-predictive-distribution",
    "href": "practice-sheets/01g-cheat-sheet.html#extracting-samples-from-the-posterior-predictive-distribution",
    "title": "Cheat sheet: common things to do with BRMS",
    "section": "Extracting samples from the posterior predictive distribution",
    "text": "Extracting samples from the posterior predictive distribution\nThere are three kinds of commonly relevant variables a generalized linear model predicts:\n\nthe central tendency of data \\(y\\) for some predictor \\(x\\),\nthe shape of the (hypothetical) data \\(y'\\) for \\(x\\), and\na linear predictor value given values of \\(x\\).\n\nAll of these measures can be obtained from a fitted model with different functions, e.g., from the tidyverse package. Here, it does not matter whether the model was fitted to data or it is a “prior model”, so to speak, fit with the flag sample_prior = \"only\".\nHere is an example for a logistic regression model (where all the three measures clearly show their conceptual difference).\n\n\nToggle code\nfit_MT_logistic <- \n  brms::brm(\n    formula = correct ~ group * condition,\n    data    = aida::data_MT,\n    family  = brms::bernoulli()\n  )\n\n\n\n\nToggle code\n# 2 samples from the predicted central tendency\naida::data_MT |> \n  dplyr::select(group, condition) |> \n  unique() |> \n  tidybayes::add_epred_draws(\n    fit_MT_logistic,\n    ndraws = 2\n    )\n\n\n# A tibble: 8 × 7\n# Groups:   group, condition, .row [4]\n  group condition  .row .chain .iteration .draw .epred\n  <chr> <chr>     <int>  <int>      <int> <int>  <dbl>\n1 touch Atypical      1     NA         NA     1  0.912\n2 touch Atypical      1     NA         NA     2  0.889\n3 touch Typical       2     NA         NA     1  0.956\n4 touch Typical       2     NA         NA     2  0.943\n5 click Atypical      3     NA         NA     1  0.851\n6 click Atypical      3     NA         NA     2  0.889\n7 click Typical       4     NA         NA     1  0.964\n8 click Typical       4     NA         NA     2  0.965\n\n\nToggle code\n# 2 samples from the predictive distribution (data samples)\naida::data_MT |> \n  dplyr::select(group, condition) |> \n  unique() |> \n  tidybayes::add_predicted_draws(\n    fit_MT_logistic,\n    ndraws = 2\n    )\n\n\n# A tibble: 8 × 7\n# Groups:   group, condition, .row [4]\n  group condition  .row .chain .iteration .draw .prediction\n  <chr> <chr>     <int>  <int>      <int> <int>       <int>\n1 touch Atypical      1     NA         NA     1           1\n2 touch Atypical      1     NA         NA     2           1\n3 touch Typical       2     NA         NA     1           1\n4 touch Typical       2     NA         NA     2           1\n5 click Atypical      3     NA         NA     1           1\n6 click Atypical      3     NA         NA     2           1\n7 click Typical       4     NA         NA     1           1\n8 click Typical       4     NA         NA     2           1\n\n\nToggle code\n# 2 samples for the linear predictor\naida::data_MT |> \n  dplyr::select(group, condition) |> \n  unique() |> \n  tidybayes::add_linpred_draws(\n    fit_MT_logistic,\n    ndraws = 2\n    )\n\n\n# A tibble: 8 × 7\n# Groups:   group, condition, .row [4]\n  group condition  .row .chain .iteration .draw .linpred\n  <chr> <chr>     <int>  <int>      <int> <int>    <dbl>\n1 touch Atypical      1     NA         NA     1     2.07\n2 touch Atypical      1     NA         NA     2     2.44\n3 touch Typical       2     NA         NA     1     2.92\n4 touch Typical       2     NA         NA     2     3.04\n5 click Atypical      3     NA         NA     1     1.65\n6 click Atypical      3     NA         NA     2     2.07\n7 click Typical       4     NA         NA     1     3.39\n8 click Typical       4     NA         NA     2     3.17"
  },
  {
    "objectID": "practice-sheets/01g-cheat-sheet.html#inspecting-default-priors-without-running-the-model",
    "href": "practice-sheets/01g-cheat-sheet.html#inspecting-default-priors-without-running-the-model",
    "title": "Cheat sheet: common things to do with BRMS",
    "section": "Inspecting default priors without running the model",
    "text": "Inspecting default priors without running the model\n\n\nToggle code\n# define the model as a \"brmsformula\" object\nmyFormula <- brms::bf(RT ~ 1 + condition + (1 + condition | submission_id))\n\n# get prior information\nbrms::get_prior(\n  formula = myFormula,\n  data    = data_MC,\n  family  = exgaussian()\n  )\n\n\n                    prior     class                    coef         group resp\n student_t(3, 385, 133.4) Intercept                                           \n                   (flat)         b                                           \n                   (flat)         b conditiondiscrimination                   \n                   (flat)         b       conditionreaction                   \n            gamma(1, 0.1)      beta                                           \n                   lkj(1)       cor                                           \n                   lkj(1)       cor                         submission_id     \n   student_t(3, 0, 133.4)        sd                                           \n   student_t(3, 0, 133.4)        sd                         submission_id     \n   student_t(3, 0, 133.4)        sd               Intercept submission_id     \n   student_t(3, 0, 133.4)        sd conditiondiscrimination submission_id     \n   student_t(3, 0, 133.4)        sd       conditionreaction submission_id     \n   student_t(3, 0, 133.4)     sigma                                           \n dpar nlpar lb ub       source\n                       default\n                       default\n                  (vectorized)\n                  (vectorized)\n             0         default\n                       default\n                  (vectorized)\n             0         default\n             0    (vectorized)\n             0    (vectorized)\n             0    (vectorized)\n             0    (vectorized)\n             0         default"
  },
  {
    "objectID": "practice-sheets/01g-cheat-sheet.html#setting-priors",
    "href": "practice-sheets/01g-cheat-sheet.html#setting-priors",
    "title": "Cheat sheet: common things to do with BRMS",
    "section": "Setting priors",
    "text": "Setting priors\n\n\nToggle code\nmyPrior <- \n  brms::prior(\"normal(30,100)\",  class = \"b\", coef = \"conditiondiscrimination\") +\n  brms::prior(\"normal(-30,100)\", class = \"b\", coef = \"conditionreaction\")\n\nfit_with_specified_prior <- \n  brms::brm(\n    formula = myformula,\n    data    = data_MC,\n    prior   = myPrior,\n    family  = exgaussian()\n  )"
  },
  {
    "objectID": "practice-sheets/01g-cheat-sheet.html#sampling-from-the-prior",
    "href": "practice-sheets/01g-cheat-sheet.html#sampling-from-the-prior",
    "title": "Cheat sheet: common things to do with BRMS",
    "section": "Sampling from the prior",
    "text": "Sampling from the prior\n\n\nToggle code\nfit_samples_from_prior_only <- \nbrms::brm(\n  formula = myformula,\n  data    = data_MC,\n  prior   = myPrior,\n  family  = exgaussian(),\n  sample_prior = \"only\"\n)"
  },
  {
    "objectID": "practice-sheets/01g-cheat-sheet.html#extract-the-stan-code",
    "href": "practice-sheets/01g-cheat-sheet.html#extract-the-stan-code",
    "title": "Cheat sheet: common things to do with BRMS",
    "section": "Extract the Stan code",
    "text": "Extract the Stan code\n\n\nToggle code\nbrms::stancode(fit_MC)\n\n\n// generated with brms 2.19.0\nfunctions {\n /* compute correlated group-level effects\n  * Args:\n  *   z: matrix of unscaled group-level effects\n  *   SD: vector of standard deviation parameters\n  *   L: cholesky factor correlation matrix\n  * Returns:\n  *   matrix of scaled group-level effects\n  */\n  matrix scale_r_cor(matrix z, vector SD, matrix L) {\n    // r is stored in another dimension order than z\n    return transpose(diag_pre_multiply(SD, L) * z);\n  }\n}\ndata {\n  int<lower=1> N;  // total number of observations\n  vector[N] Y;  // response variable\n  int<lower=1> K;  // number of population-level effects\n  matrix[N, K] X;  // population-level design matrix\n  // data for group-level effects of ID 1\n  int<lower=1> N_1;  // number of grouping levels\n  int<lower=1> M_1;  // number of coefficients per level\n  int<lower=1> J_1[N];  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_1_1;\n  vector[N] Z_1_2;\n  vector[N] Z_1_3;\n  vector[N] Z_1_4;\n  int<lower=1> NC_1;  // number of group-level correlations\n  int prior_only;  // should the likelihood be ignored?\n}\ntransformed data {\n  int Kc = K - 1;\n  matrix[N, Kc] Xc;  // centered version of X without an intercept\n  vector[Kc] means_X;  // column means of X before centering\n  for (i in 2:K) {\n    means_X[i - 1] = mean(X[, i]);\n    Xc[, i - 1] = X[, i] - means_X[i - 1];\n  }\n}\nparameters {\n  vector[Kc] b;  // population-level effects\n  real Intercept;  // temporary intercept for centered predictors\n  real<lower=0> sigma;  // dispersion parameter\n  vector<lower=0>[M_1] sd_1;  // group-level standard deviations\n  matrix[M_1, N_1] z_1;  // standardized group-level effects\n  cholesky_factor_corr[M_1] L_1;  // cholesky factor of correlation matrix\n}\ntransformed parameters {\n  matrix[N_1, M_1] r_1;  // actual group-level effects\n  // using vectors speeds up indexing in loops\n  vector[N_1] r_1_1;\n  vector[N_1] r_1_2;\n  vector[N_1] r_1_3;\n  vector[N_1] r_1_4;\n  real lprior = 0;  // prior contributions to the log posterior\n  // compute actual group-level effects\n  r_1 = scale_r_cor(z_1, sd_1, L_1);\n  r_1_1 = r_1[, 1];\n  r_1_2 = r_1[, 2];\n  r_1_3 = r_1[, 3];\n  r_1_4 = r_1[, 4];\n  lprior += student_t_lpdf(Intercept | 3, 385, 133.4);\n  lprior += student_t_lpdf(sigma | 3, 0, 133.4)\n    - 1 * student_t_lccdf(0 | 3, 0, 133.4);\n  lprior += student_t_lpdf(sd_1 | 3, 0, 133.4)\n    - 4 * student_t_lccdf(0 | 3, 0, 133.4);\n  lprior += lkj_corr_cholesky_lpdf(L_1 | 1);\n}\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    // initialize linear predictor term\n    vector[N] mu = rep_vector(0.0, N);\n    mu += Intercept;\n    for (n in 1:N) {\n      // add more terms to the linear predictor\n      mu[n] += r_1_1[J_1[n]] * Z_1_1[n] + r_1_2[J_1[n]] * Z_1_2[n] + r_1_3[J_1[n]] * Z_1_3[n] + r_1_4[J_1[n]] * Z_1_4[n];\n    }\n    target += normal_id_glm_lpdf(Y | Xc, mu, b, sigma);\n  }\n  // priors including constants\n  target += lprior;\n  target += std_normal_lpdf(to_vector(z_1));\n}\ngenerated quantities {\n  // actual population-level intercept\n  real b_Intercept = Intercept - dot_product(means_X, b);\n  // compute group-level correlations\n  corr_matrix[M_1] Cor_1 = multiply_lower_tri_self_transpose(L_1);\n  vector<lower=-1,upper=1>[NC_1] cor_1;\n  // extract upper diagonal of correlation matrix\n  for (k in 1:M_1) {\n    for (j in 1:(k - 1)) {\n      cor_1[choose(k - 1, 2) + j] = Cor_1[j, k];\n    }\n  }\n}"
  },
  {
    "objectID": "practice-sheets/01g-cheat-sheet.html#extract-stan-data",
    "href": "practice-sheets/01g-cheat-sheet.html#extract-stan-data",
    "title": "Cheat sheet: common things to do with BRMS",
    "section": "Extract Stan data",
    "text": "Extract Stan data\nThis is the data passed to Stan. Useful for inspecting dimensions etc.\n\n\nToggle code\nbrms::standata(fit_MC) |> names()\n\n\n [1] \"N\"          \"Y\"          \"K\"          \"X\"          \"Z_1_1\"     \n [6] \"Z_1_2\"      \"Z_1_3\"      \"Z_1_4\"      \"J_1\"        \"N_1\"       \n[11] \"M_1\"        \"NC_1\"       \"prior_only\""
  },
  {
    "objectID": "practice-sheets/01g-cheat-sheet.html#inspect-design-matrices",
    "href": "practice-sheets/01g-cheat-sheet.html#inspect-design-matrices",
    "title": "Cheat sheet: common things to do with BRMS",
    "section": "Inspect design matrices",
    "text": "Inspect design matrices\n\nPopulation-level effects\n\n\nToggle code\nX <- brms::standata(fit_MC)$X\nX |> head()\n\n\n  Intercept conditionreaction conditiondiscrimination\n1         1                 1                       0\n2         1                 1                       0\n3         1                 1                       0\n4         1                 1                       0\n5         1                 1                       0\n6         1                 1                       0\n\n\n\n\nGroup-level effects\nThe group-level design matrix is spread out over different variables (all names Z_ followed by some indices), but retrievable like so:\n\n\nToggle code\ndata4Stan <- brms::standata(fit_MC)\nZ <- data4Stan[str_detect(data4Stan |> names(), \"Z_\")] |> as_tibble()\nZ\n\n\n# A tibble: 2,519 × 4\n       Z_1_1     Z_1_2     Z_1_3     Z_1_4\n   <dbl[1d]> <dbl[1d]> <dbl[1d]> <dbl[1d]>\n 1         1         1         0         0\n 2         1         1         0         1\n 3         1         1         0         1\n 4         1         1         0         1\n 5         1         1         0         0\n 6         1         1         0         1\n 7         1         1         0         1\n 8         1         1         0         0\n 9         1         1         0         1\n10         1         1         0         0\n# ℹ 2,509 more rows"
  },
  {
    "objectID": "practice-sheets/01b-simple-regression.html",
    "href": "practice-sheets/01b-simple-regression.html",
    "title": "Simple linear regression",
    "section": "",
    "text": "This is a very basic tutorial for running a simple Bayesian regression with brms. You will learn how to specify and run the model, and to extract, plot, and visualize posterior samples."
  },
  {
    "objectID": "practice-sheets/01b-simple-regression.html#data-wrangling",
    "href": "practice-sheets/01b-simple-regression.html#data-wrangling",
    "title": "Simple linear regression",
    "section": "Data wrangling",
    "text": "Data wrangling\n\n\nToggle code\n# aggregate\ndolphin_agg <- dolphin |> \n  filter(correct == 1) |> \n  group_by(subject_id) |> \n  dplyr::summarize(\n            AUC = median(AUC, na.rm = TRUE),\n            MAD = median(MAD, na.rm = TRUE)) \n  \n# let's have a look\nhead(dolphin_agg)\n\n\n# A tibble: 6 × 3\n  subject_id     AUC    MAD\n       <dbl>   <dbl>  <dbl>\n1       1001  55200. 111.  \n2       1002  59596.  87.9 \n3       1003 -17772  -34.1 \n4       1004  -3600.  -3.83\n5       1005  54054   95.0 \n6       1006  60396. 155."
  },
  {
    "objectID": "practice-sheets/01b-simple-regression.html#visual-assessment",
    "href": "practice-sheets/01b-simple-regression.html#visual-assessment",
    "title": "Simple linear regression",
    "section": "Visual assessment",
    "text": "Visual assessment\nBefore we start thinking about statistical inference, we always want to get a feel for the data visually. You basically always want to plot the data.\n\n\nToggle code\n# plot\nggplot(data = dolphin_agg, \n       aes(x = MAD, \n           y = AUC)) + \n  geom_point(size = 3, alpha = 0.3) \n\n\n\n\n\nThis graph displays the distribution of AUC and MAD values. We can see that there is a strong relationship between AUC and MAD. And that makes a lot of sense. The more the cursor strives toward the competitor, the larger is the overall area under the curve. Heureka! Our hypothesis is confirmed.\nBut wait! As Bayesians, we would like to translate the data into an expression of evidence: do the data provide evidence for our research hypotheses? Also, notice that there is some variability. We want precise estimates of potential effects. We also want a measure of how certain we can be about these estimates."
  },
  {
    "objectID": "practice-sheets/01b-simple-regression.html#bayesian-linear-regression-with-brms",
    "href": "practice-sheets/01b-simple-regression.html#bayesian-linear-regression-with-brms",
    "title": "Simple linear regression",
    "section": "Bayesian linear regression with brms",
    "text": "Bayesian linear regression with brms\nThe brms package allows us to run Bayesian regression models, both simple and rather complex. It uses a sampling method, so its output will be vectors of (correlated) samples from the posterior distribution of the model’s parameters.\nSo, to quantify evidence and uncertainty with posterior samples, let’s run a simple linear regression model using brms. We use the R’s standard notation at first (though brms extends this syntax substantially for more complex models) to specify a formula in which AUC is predicted by MAD.\nAUC ~ MAD\nWhen you run this code, the brms package generates Stan code and runs the Stan program in the background. Stan code is executed in C++, and the model will be ‘compiled’ (you get information about this in the console output). You we will want to learn later what this compilation does (spoiler: it computes gradients for all stochastic nodes in the model). The only thing that is relevant for you at the moment is this: This compilation can take quite a while (especially for complex models) before anything happens.\n\n\nToggle code\n# specify the model \nmodel1 = brm(\n  # model formula\n  AUC ~ MAD, \n  # data\n  data = dolphin_agg\n  )\n\n\n\n\nToggle code\nsummary(model1)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: AUC ~ MAD \n   Data: dolphin_agg (Number of observations: 108) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   523.92   1881.53 -3088.78  4333.60 1.00     3974     2824\nMAD         455.31     16.52   421.98   488.32 1.00     3315     2891\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma 17220.72   1206.85 15045.93 19800.29 1.00     3825     2633\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe output of such a model looks very familiar if you have worked with lm() before. We want to look at what is here called “Population-Level Effects”, which is a small table in this case. The first column contains the names of our coefficients; the Estimate column gives us the posterior mean of these coefficients; the Est.Error give us the standard error; the l-95%and u-95% give us the lower and upper limit of the 95% Credible Interval (henceforth CrI). The column Rhat (R^) which is a diagnostic of chain convergence and should not diverge much from 1 (rule of thumb: should by <1.1). Again, more about that later. The Bulk_ESS and Tail_ESS columns give us numbers of “useful” samples. This number should be sufficiently high. If its not, brms will give you a convenient warning (more about that later, so don’t worry for now). If that happens, you need to increase the chains and / or the number of iterations in order to increase the overall number of samples (again, don’t worry for now).\nIf we need the main summary output in a tidy tibble format, we can use this function from the tidybayes package:\n\n\nToggle code\ntidybayes::summarise_draws(model1)\n\n\n# A tibble: 5 × 10\n  variable       mean  median        sd       mad      q5     q95  rhat ess_bulk\n  <chr>         <num>   <num>     <num>     <num>   <num>   <num> <num>    <num>\n1 b_Intercept   524.    507.  1882.     1868.     -2532.   3563.   1.00    3974.\n2 b_MAD         455.    456.    16.5      16.2      428.    482.   1.00    3315.\n3 sigma       17221.  17153.  1207.     1169.     15342.  19323.   1.00    3825.\n4 lprior        -22.3   -22.3    0.0302    0.0295   -22.4   -22.3  1.00    3828.\n5 lp__        -1218.  -1218.     1.26      1.04   -1221.  -1217.   1.00    2187.\n# ℹ 1 more variable: ess_tail <num>\n\n\nThe model output suggests that the posterior mean of the Intercept is around 500. The coefficient for MAD is estimated to be about 450.\nTo see how good a fit this is, we should manually draw this line into the graph from above.\n\n\nToggle code\n# extract model parameters:\nmodel_intercept <- summary(model1)$fixed[1,1]\nmodel_slope <- summary(model1)$fixed[2,1]\n\n# plot\nggplot(data = dolphin_agg, \n       aes(x = MAD, \n           y = AUC)) + \n  geom_abline(intercept = model_intercept, slope = model_slope, color = project_colors[2], size  = 1) +\n  geom_point(size = 3, alpha = 0.3, color = project_colors[1])\n\n\n\n\n\nLooking at the graph, it does make sense, right? The red line seems to capture the main trend pretty well.\nNow is there a relationship between AUC and MAD? What would it mean if there was no relationship between these two measures? Well no relationship would mean a slope of 0. How would that look like?\n\n\nToggle code\nggplot(data = dolphin_agg, \n       aes(x = MAD, \n           y = AUC)) + \n  geom_abline(intercept = model_intercept, slope = model_slope, color = project_colors[2], size = 1) +\n  geom_abline(intercept = model_intercept, slope = 0, color = project_colors[3], size = 1, lty = \"dashed\") +\n  geom_point(size = 3, alpha = 0.3, color = project_colors[1])\n\n\n\n\n\nThese lines look quite different indeed. But Bayesian data analysis does not give us only one single line. It gives us infinitely many lines, weighted by plausibility. Let’s explore this universe of weighted predictions."
  },
  {
    "objectID": "practice-sheets/01b-simple-regression.html#extracting-posterior-distributions-and-plotting-them",
    "href": "practice-sheets/01b-simple-regression.html#extracting-posterior-distributions-and-plotting-them",
    "title": "Simple linear regression",
    "section": "Extracting posterior distributions and plotting them",
    "text": "Extracting posterior distributions and plotting them\nWe can interpret and visualize our coefficients immediately. We can create a data frame with all posterior samples for each parameter and plot those distributions for all coefficients. Let’s first see what coefficients there are with the get_variables() function from the tidybayes package.\n\n\nToggle code\n# inspect parameters\ntidybayes::get_variables(model1)\n\n\n [1] \"b_Intercept\"   \"b_MAD\"         \"sigma\"         \"lprior\"       \n [5] \"lp__\"          \"accept_stat__\" \"stepsize__\"    \"treedepth__\"  \n [9] \"n_leapfrog__\"  \"divergent__\"   \"energy__\"     \n\n\nEverything that is preceded by a b_ is a population level coefficients, i.e. our predictors. Now let’s wrangle this data frame to get what we need. You don’t have to entirely understand the following code, but make sure you understand it well enough to recycle it later on.\n\n\nToggle code\n# wrangle data frame\nposteriors1 <- model1 |>\n  tidybayes::spread_draws(b_MAD, b_Intercept) |>\n  select(b_MAD, b_Intercept)\n\nposteriors1\n\n\n# A tibble: 4,000 × 2\n   b_MAD b_Intercept\n   <dbl>       <dbl>\n 1  457.       -729.\n 2  473.      -1044.\n 3  441.       1984.\n 4  469.      -4120.\n 5  450.       2933.\n 6  456.      -1841.\n 7  442.       2630.\n 8  455.        523.\n 9  436.       1414.\n10  467.      -1296.\n# ℹ 3,990 more rows\n\n\nNow that we know how to extract posterior samples, let’s actually take a bunch of these samples and plot them as lines into our scatter plot from above. In this code chunk we generate a subsample of 100 parameter pairs. (There are methods to directly sample from the posterior values of the linear predictor, which is what you want to use for complex models, but here we go full hands-on.)\n\n\nToggle code\n# wrangle data frame\nposteriors2 <- model1 |>\n  # parameter 'ndraws' requests 100 random subsamples\n  tidybayes::spread_draws(b_MAD, b_Intercept, ndraws = 100) |>\n  select(b_MAD, b_Intercept)\n  \n# plot\nggplot(data = dolphin_agg, \n       aes(x = MAD, \n           y = AUC)) + \n  geom_abline(data = posteriors2,\n              aes(intercept = b_Intercept, slope = b_MAD), \n              color = project_colors[2], size  = 0.1, alpha = 0.4) +\n  geom_point(size = 3, alpha = 0.3, color = project_colors[1]) +\n  theme_aida()\n\n\n\n\n\n\n\n\n\nGiven our model, assumptions and data, these are 100 plausible regression lines. As you can see they are very similar.\nUsing this pipeline we can also calculate the mean of the posteriors and any kind of Credible Interval (CrI). We first extract the posterior and bring them into a tidy form. Let’s only look at the coefficient for MAD here.\n\n\nToggle code\nposteriors3 <- model1 |>\n   # use the gather_draws() function for \"long data\"\n   tidybayes::gather_draws(b_MAD) |> \n   # change names of columns\n   rename(parameter = .variable,\n          posterior = .value) |> \n   # select only those columns that are relevant\n   select(parameter, posterior)\n\nhead(posteriors3)\n\n\n# A tibble: 6 × 2\n# Groups:   parameter [1]\n  parameter posterior\n  <chr>         <dbl>\n1 b_MAD          457.\n2 b_MAD          473.\n3 b_MAD          441.\n4 b_MAD          469.\n5 b_MAD          450.\n6 b_MAD          456.\n\n\nAnd then calculate the mean, the lower and the upper bound of a 90% CrI, using the function tidybayes::hdi().\n\n\nToggle code\n# get posteriors for the relevant coefficients\nposteriors3_agg <- posteriors3 |> \n  group_by(parameter) |> \n  summarise(\n    `90lowerCrI`   = tidybayes::hdi(posterior, credMass = 0.90)[1],\n    mean_posterior = mean(posterior),\n    `90higherCrI`  = tidybayes::hdi(posterior, credMass = 0.90)[2])\n\nposteriors3_agg \n\n\n# A tibble: 1 × 4\n  parameter `90lowerCrI` mean_posterior `90higherCrI`\n  <chr>            <dbl>          <dbl>         <dbl>\n1 b_MAD             421.           455.          487.\n\n\nNow we use this newly created data frame to plot the posterior distributions of all population-level coefficients. Again, we use our new best friend, the tidybayes package which offers some sweet extensions to ggplot’s geom_ family of functions. We also add a reference point to compare the posteriors against. A common and reasonable reference point is 0. Remember a slope coefficient of zero would correspond to a flat regression line.\n\n\nToggle code\n# plot the regression coefficients\nposteriors1 |> \n  pivot_longer(cols = everything(), names_to = \"parameter\", values_to = \"posterior\") |> \n  ggplot(aes(x = posterior, y = parameter, fill = parameter)) + \n    # plot density w/ 90% credible interval\n    tidybayes::stat_halfeye(.width = 0.9) +\n    # add axes titles\n    xlab(\"\") +\n    ylab(\"\") +\n    # adjust the x-axis \n    scale_x_continuous(limits = c(-100,600)) +\n    # add line for the value zero\n    geom_segment(x = 0, xend = 0, y = Inf, yend = -Inf,\n                 lty = \"dashed\") +\n    theme(legend.position=\"none\")\n\n\n\n\n\nToggle code\nposteriors3_agg[1,2]\n\n\n# A tibble: 1 × 1\n  `90lowerCrI`\n         <dbl>\n1         421.\n\n\nHere you see density plots for our critical coefficients of the model. We care mostly about the slope coefficient (b_MAD) (the posterior of which is shown in red). Values between about 420.74 and about 486.84 are plausible (at the 90% level) and they are indicated by the thick black line in the density plot for this coefficient. The mean of the distribution is indicated by the thick black dot.\nThat’s helpful because we can relate this distribution to relevant values, for example the value 0 (dashed line). If you look at the coefficient, you can see that the posterior distribution does not include the value zero or any small-ish “Region of Practical Equivalence” around it. In fact, the posterior is really far away from zero. Thus, if we believe in the data and the model, we can be very certain that this coefficient is not zero. In other words, we would be very certain that there is a positive relationship between AUC and MAD (and in turn that ‘no relationship’ is not a very plausible scenario).\nThe brms package allows us to quickly evaluate how many posterior samples fall into a certain value range. Just for fun, let’s calculate the amount of posterior samples that are larger than 450. The following code chunk does this for us:\n\n\nToggle code\nhypothesis(model1, 'MAD > 450')\n\n\nHypothesis Tests for class b:\n       Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob\n1 (MAD)-(450) > 0     5.31     16.52   -21.92     32.3       1.72      0.63\n  Star\n1     \n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\n\nThe results tell us that more than 60% of all posterior samples are larger than 450. It also tells us the evidence ratio (more on this later), which is the odds of the hypothesis in question (here ’MAD > 450)."
  },
  {
    "objectID": "practice-sheets/03a-GLM-tutorial.html",
    "href": "practice-sheets/03a-GLM-tutorial.html",
    "title": "03a: Generalized linear models",
    "section": "",
    "text": "Here is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\n\n\nToggle code\n# install packages from CRAN (unless installed)\npckgs_needed <- c(\n  \"tidyverse\",\n  \"brms\",\n  \"rstan\",\n  \"rstanarm\",\n  \"remotes\",\n  \"tidybayes\",\n  \"bridgesampling\",\n  \"shinystan\",\n  \"mgcv\"\n)\npckgs_installed <- installed.packages()[,\"Package\"]\npckgs_2_install <- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]\nif(length(pckgs_2_install)) {\n  install.packages(pckgs_2_install)\n} \n\n# install additional packages from GitHub (unless installed)\nif (! \"aida\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/aida-package\")\n}\nif (! \"faintr\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/faintr\")\n}\nif (! \"cspplot\" %in% pckgs_installed) {\n  remotes::install_github(\"CogSciPrag/cspplot\")\n}\n\n# load the required packages\nx <- lapply(pckgs_needed, library, character.only = TRUE)\nlibrary(aida)\nlibrary(faintr)\nlibrary(cspplot)\n\n# these options help Stan run faster\noptions(mc.cores = parallel::detectCores())\n\n# use the CSP-theme for plotting\ntheme_set(theme_csp())\n\n# global color scheme from CSP\nproject_colors = cspplot::list_colors() |> pull(hex)\n# names(project_colors) <- cspplot::list_colors() |> pull(name)\n\n# setting theme colors globally\nscale_colour_discrete <- function(...) {\n  scale_colour_manual(..., values = project_colors)\n}\nscale_fill_discrete <- function(...) {\n   scale_fill_manual(..., values = project_colors)\n}\n\n\n\n\nToggle code\ndata_MT <- aida::data_MT\n\n\nThis tutorial covers common types of generalized linear regression models (GLMs):\n\nlogistic regression\nmultinomial regression\nordinal regression\nPoisson regression\n\nThe shared form of all of these GLMs is the following “feed-forward computation” (here illustrated for a single datum of the predicted variable \\(y\\) for a vector \\(x\\) of predictor variables and a vector of coefficients \\(\\beta\\):\n\ncompute a linear predictor: \\(\\eta = \\mathbf{x} \\cdot \\beta\\);\ncompute a predictor of central tendency using an appropriate link function \\(\\text{LF}\\): \\(\\xi = \\text{LF}(\\eta ; \\theta_{\\text{LF}})\\);\ndetermine the likelihood function \\(\\text{LH}\\): \\(y \\sim \\text{LH}(\\xi; \\theta_{\\text{LH}})\\).\n\nLink function and likelihood function may have additional free parameters, \\(\\theta_{\\text{LF}}\\) and \\(\\theta_{\\text{LH}}\\), to be fitted alongside the regression coefficients.\nSimple linear regression is the special case of this scheme where the link function is just the identity function and the likelihood is given by \\(y \\sim \\mathcal{N}(\\xi; \\sigma)\\). Different types of regression are used to account for different kinds predicted variable \\(y\\):\n\n\n\n\n\n\n\n\ntype of \\(y\\)\n(inverse) link function\nlikelihood function\n\n\n\n\nmetric\n\\(\\xi = \\eta\\)\n\\(y \\sim \\text{Normal}(\\xi; \\sigma)\\)\n\n\nbinary\n\\(\\xi = \\text{logistic}(\\eta)\\)\n\\(y \\sim \\text{Bernoulli}(\\xi)\\)\n\n\nnominal\n\\(\\xi = \\text{soft-max}(\\eta)\\)\n\\(y \\sim \\text{Categorical}({\\xi})\\)\n\n\nordinal\n\\(\\xi = \\text{cumulative-logit}(\\eta; {\\delta})\\)\n\\(y \\sim \\text{Categorical}({\\xi})\\)\n\n\ncount\n\\(\\xi = \\exp(\\eta)\\)\n\\(y \\sim \\text{Poisson}(\\xi)\\)"
  },
  {
    "objectID": "practice-sheets/03a-GLM-tutorial.html#explanation",
    "href": "practice-sheets/03a-GLM-tutorial.html#explanation",
    "title": "03a: Generalized linear models",
    "section": "Explanation",
    "text": "Explanation\nIn logistic regression, the response variable \\(y\\) is binary, i.e., we want to predict the probability \\(p\\) with which one of two possible outcomes (henceforth: the reference outcome) occurs. The likelihood function for this case is the Bernoulli distribution. This requires a link function \\(LF\\) that maps real-valued linear predictor values \\(\\eta\\) onto the unit interval. A common choice is the logistic function:\n\\[\n\\text{logistic}(\\eta) = \\frac{1}{1+ \\exp(-\\eta)} = \\xi\n\\]\n\n\n\n\n\nThe logistic regression model is then defined as:\n\\[\n\\begin{align*}\n\\eta_i  &= \\mathbf{x}_i \\cdot \\beta       && \\color{gray}{\\text{[linear predictor]}} \\\\\n\\xi_i &= \\text{logistic}(\\eta_i) && \\color{gray}{\\text{[predictor of central tendency]}} \\\\\ny_i & \\sim \\text{Bernoulli}(\\xi_i) && \\color{gray}{\\text{[likelihood]}}\n\\end{align*}\n\\]\nThe linear predictor values \\(\\eta\\) can be interpreted directly, as the log odds-ratio of the predicted probability \\(\\xi\\). This is because the inverse of the logistic function is the logit function, which has the following form:\n\\[\n\\text{logit}(\\xi) = \\log \\frac{\\xi}{1-\\xi} = \\eta\n\\]\n\n\n\n\n\nThis means that differences between linear predictor parameters can be interpreted directly as something like the “evidence ratio” or “Bayes factor”. It is the log of the factor by which to transform log odds-ratios (e.g., changing beliefs from \\(\\xi_1\\) to \\(\\xi_2\\):\n\\[\n\\begin{align*}\n& \\eta_1 - \\eta_2 = \\log \\frac{\\xi_1}{1-\\xi_1} - \\log \\frac{\\xi_2}{1-\\xi_2} = \\log \\left ( \\frac{\\xi_1}{1-\\xi_1} \\frac{1-\\xi_2}{\\xi_2}\\right ) \\\\\n\\Leftrightarrow & \\frac{\\xi_1}{1-\\xi_1} = \\exp (\\eta_1 - \\eta_2) \\ \\frac{\\xi_2}{1-\\xi_2}\n\\end{align*}\n\\]\nFor the purposes of understanding which priors are weakly or strongly informative, a unit difference in the linear predictor can be interpreted as a log Bayes factor (changing prior odds to posterior odds). So a unit difference in the predictor value corresponds to a Bayes factor of around 2.72."
  },
  {
    "objectID": "practice-sheets/03a-GLM-tutorial.html#example",
    "href": "practice-sheets/03a-GLM-tutorial.html#example",
    "title": "03a: Generalized linear models",
    "section": "Example",
    "text": "Example\nConsider the mouse-tracking data again. Our hypothesis is that typical examples are easier to classify, so they should have higher accuracy than atypical ones. We are also interested in additional effects of group on accuracy.\nAs usual, we begin by plotting the relevant data.\n\n\nToggle code\nsum_stats <- data_MT |> \n  group_by(group, condition) |> \n  tidyboot::tidyboot_mean(correct) |> \n  rename(accuracy = empirical_stat)\n  \nsum_stats\n\n\n# A tibble: 4 × 7\n# Groups:   group [2]\n  group condition     n accuracy ci_lower  mean ci_upper\n  <chr> <chr>     <int>    <dbl>    <dbl> <dbl>    <dbl>\n1 click Atypical    318    0.874    0.837 0.873    0.908\n2 click Typical     689    0.964    0.950 0.964    0.977\n3 touch Atypical    330    0.909    0.878 0.909    0.939\n4 touch Typical     715    0.941    0.923 0.941    0.957\n\n\nToggle code\nsum_stats |> \n  ggplot(aes(x = condition, y = accuracy, group = group, color = group)) +\n  geom_line(size = 1, position = position_dodge(0.2)) +\n  geom_point(size = 3, position = position_dodge(0.2)) +\n  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), \n                width = 0.1, size = 0.35, position = position_dodge(0.2))\n\n\n\n\n\nVisually, there might be a hint that typical trials had higher accuracy, but we cannot judge with the naked eye whether this is substantial.\nA logistic regression, regressing correct against group * condition, may tell us more. To run the logistic regression, we must tell the brms that we want to treat 0 and 1 as categories. To be sure, and also to directly dictate which of the two categories is the reference level, we use a factor (of strings) with explicit ordering.\n\n\nToggle code\nfit_logistic <- brm(\n  formula = correct ~ group * condition,\n  data = data_MT |> \n    mutate(correct = factor(ifelse(correct, \"correct\", \"incorrect\"),\n                            levels = c(\"incorrect\", \"correct\"))),\n  family = bernoulli(link=\"logit\")\n)\n\nsummary(fit_logistic)\n\n\nTo test whether typical examples had credibly higher accuracy, the faintr package can be used like so:\n\n\nToggle code\ncompare_groups(\n  fit_logistic,\n  higher = condition == \"Typical\",\n  lower  = condition == \"Atypical\"\n)\n\n\nOutcome of comparing groups: \n * higher:  condition == \"Typical\" \n * lower:   condition == \"Atypical\" \nMean 'higher - lower':  0.9107 \n95% HDI:  [ 0.5508 ; 1.262 ]\nP('higher - lower' > 0):  1 \nPosterior odds:  Inf \n\n\nBased on these results, we may conclude that, given the model and the data, we should believe that typical examples had higher accuracy.\n\n\n\n\n\n\nExercise 1a\n\n\n\n\n\nTest whether there is reason to believe, given model and data, that the touch group was more accurate than the click group. (After all, the click group could change their minds until the very last moment.)\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\ncompare_groups(\n  fit_logistic,\n  higher = group == \"click\",\n  lower  = group == \"touch\"\n)\n\n\nOutcome of comparing groups: \n * higher:  group == \"click\" \n * lower:   group == \"touch\" \nMean 'higher - lower':  0.06632 \n95% HDI:  [ -0.2605 ; 0.4302 ]\nP('higher - lower' > 0):  0.6488 \nPosterior odds:  1.847 \n\n\nThere is no reason to believe (given model and data) that this conjecture is true.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1b\n\n\n\n\n\nIf you look back at the plot of accuracy, it looks as if the change from atypical to typical condition does not have the same effect, at least not at the same level of strength, for the click and the touch group, i.e., it seems that there is an interaction between these two variables (group and condition). Use the function brms::hypothesis() to examine the interaction term of the model fit. What do you conclude from this?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\nbrms::hypothesis(fit_logistic, \"grouptouch:conditionTypical < 0\")\n\n\nHypothesis Tests for class b:\n                Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio\n1 (grouptouch:condi... < 0    -0.88      0.37     -1.5    -0.28     120.21\n  Post.Prob Star\n1      0.99    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\n\nGiven model and data, it is very plausible to believe that there is an interaction between these two variables."
  },
  {
    "objectID": "practice-sheets/03a-GLM-tutorial.html#explanation-1",
    "href": "practice-sheets/03a-GLM-tutorial.html#explanation-1",
    "title": "03a: Generalized linear models",
    "section": "Explanation",
    "text": "Explanation\nIn multinomial regression the predicted variable is categorical with more than two levels: \\(c_1, \\dots, c_k\\), \\(k > 2\\). We want to predict probabilities for each category \\(p_1, \\dots, p_k\\) (with some linear predictors, more on this in a moment). To obtain the probabilities, we estimate a set of weights (so-called logits): \\(s_1, \\dots, s_k\\). By default, we set \\(s_1 = 0\\), because we only need \\(k-1\\) numbers to define a \\(k\\)-place probability vector (given that it must sum to one). For all \\(1 \\le j \\le k\\), we define the probability \\(p_i\\) of category \\(i\\) via the following (so-called soft-max operation):\n\\[\np_j = \\frac{\\exp s_j}{ \\sum_{j'=1}^k \\exp s_j'}\n\\]\nThis entails that for every \\(1 < j \\le k\\), the score \\(s_j\\) can be interpreted as the log-odds of category \\(c_j\\) over the reference category \\(c_1\\):\n\\[\ns_j = \\log \\frac{p_j}{p_1}\n\\]\nFinally, we do not just estimate any-old vector of logits, but we assume that each logit \\(s_j\\) (\\(1 < j \\le k\\)) is estimated as a linear predictor (based on the usual linear regression predictor coefficients, appropriate to the type of the \\(l\\) explanatory variables):\n\\[\ns_j = \\mathbf{x} \\cdot \\beta^j\n\\]\nTwo things are important for interpreting the outcome of a multinomial regression fit:\n\neach category (beyond the reference category) receives its own (independent) set of regression coefficients;\nthe linear predictor predictor \\(s_j\\) for category \\(c_j\\) can be interpreted as the log-odds of the \\(j\\)-th category over the first, reference category."
  },
  {
    "objectID": "practice-sheets/03a-GLM-tutorial.html#example-1",
    "href": "practice-sheets/03a-GLM-tutorial.html#example-1",
    "title": "03a: Generalized linear models",
    "section": "Example",
    "text": "Example\nOur next research question is slightly diffuse: we want to explore whether the distribution of trajectory types is affected by whether the correct target was on the right or the left. We only consider three types of categories (curved, straight and ‘change of mind’) and prepare the data to also give us the information whether the ‘correct’ target was left or right.\n\n\nToggle code\ndata_MT_prepped <-\n  data_MT |>\n  mutate(\n    prototype_label = case_when(\n     prototype_label %in% c('curved', 'straight') ~ prototype_label,\n     TRUE ~ 'CoM'\n    ),\n    prototype_label = factor(prototype_label,\n                             levels = c('straight', 'curved', 'CoM')),\n    target_position = ifelse(category_left == category_correct, \"left\", \"right\")\n    )\n\n\nThe relevant data now looks as follows:\n\n\nToggle code\ndata_MT_prepped |> \n  select(prototype_label, target_position)\n\n\n# A tibble: 2,052 × 2\n   prototype_label target_position\n   <fct>           <chr>          \n 1 straight        left           \n 2 straight        right          \n 3 curved          right          \n 4 curved          left           \n 5 CoM             left           \n 6 CoM             right          \n 7 CoM             right          \n 8 straight        left           \n 9 straight        left           \n10 straight        left           \n# ℹ 2,042 more rows\n\n\nThe counts and proportions we care about are these:\n\n\nToggle code\nsum_stats <- data_MT_prepped |> \n  count(target_position, prototype_label) |>\n  group_by(target_position) |> \n  mutate(proportion = n / sum(n))\n\nsum_stats\n\n\n# A tibble: 6 × 4\n# Groups:   target_position [2]\n  target_position prototype_label     n proportion\n  <chr>           <fct>           <int>      <dbl>\n1 left            straight          751     0.734 \n2 left            curved            136     0.133 \n3 left            CoM               136     0.133 \n4 right           straight          793     0.771 \n5 right           curved             93     0.0904\n6 right           CoM               143     0.139 \n\n\nAnd here is a plot that might be useful to address your current issue:\n\n\nToggle code\nsum_stats |> \n  ggplot(aes(x = prototype_label, y = proportion, fill = prototype_label)) +\n  geom_col() +\n  facet_grid(. ~ target_position)\n\n\n\n\n\nIt is hard to say from visual inspection alone, whether there are any noteworthy differences. We might consider the following:\n\nConjecture: the difference in probability between straight vs curved is higher when the target is on the right than when it is on the left.\n\nThis is not a real “research hypothesis” but a conjecture about the data. Let’s still run a multinomial regression model to test address this conjecture.\n\n\nToggle code\nfit_multinom <- brm(\n  formula = prototype_label ~ target_position,\n  data = data_MT_prepped,\n  family = categorical()\n)\n\n\nThe summary of this model fit is a bit unwieldy:\n\n\nToggle code\nsummary(fit_multinom)\n\n\n Family: categorical \n  Links: mucurved = logit; muCoM = logit \nFormula: prototype_label ~ target_position \n   Data: data_MT_prepped (Number of observations: 2052) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n                              Estimate Est.Error l-95% CI u-95% CI Rhat\nmucurved_Intercept               -1.71      0.09    -1.90    -1.53 1.00\nmuCoM_Intercept                  -1.71      0.09    -1.90    -1.54 1.00\nmucurved_target_positionright    -0.44      0.14    -0.72    -0.16 1.00\nmuCoM_target_positionright       -0.00      0.13    -0.26     0.25 1.00\n                              Bulk_ESS Tail_ESS\nmucurved_Intercept                4922     3008\nmuCoM_Intercept                   4336     2999\nmucurved_target_positionright     4091     3222\nmuCoM_target_positionright        4280     2947\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nFor better visibility here is a plot of the posteriors over relevant model parameters.\n\n\nToggle code\n# there MUST be a nicer way of doing this, but ...\nordered_names <- c(\n  \"b_mucurved_Intercept\", \n  \"b_muCoM_Intercept\",\n  \"b_mucurved_target_positionright\",\n  \"b_muCoM_target_positionright\"\n)\n\nfit_multinom |> \n  tidybayes::tidy_draws() |> \n  pivot_longer(cols = starts_with(\"b_\")) |> \n  select(name, value) |> \n  mutate(name = factor(name, levels = rev(ordered_names))) |> \n  ggplot(aes(x = value, y = name)) +\n  tidybayes::stat_halfeye() +\n  geom_vline(aes(xintercept = 0), color = project_colors[3], alpha= 1, size = 1)\n\n\n\n\n\n\n\n\n\n\n\nExercise 2a\n\n\n\n\n\nLook at the names of the coefficients in the fit summary to find out:What is the reference level for the categorical predictor variable?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIt’s the ‘left’ position, because there is a coefficient for the ‘right’ position.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2b\n\n\n\n\n\nLook at the names of the coefficients in the fit summary to find out: What is the reference level of the categories to be predicted in the multinomial model?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe reference category is ‘straight’ because we have regression coeffiecient for all but the ‘straight’ category.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2c\n\n\n\n\n\nCan you extract information about our conjecture from this plot (or the summary of the model fit)?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYes! Our conjecture is about the difference in probability of the ‘straight’ vs he ‘curved’ category. This difference is directly encoded in regression coefficients. Concretely, the coefficient ‘mucurved_Intercept’ gives us the log odds of the ‘straight’ vs’ the ‘curved’ category for the ‘left’-position cases. The difference of log odds for the ‘right’-position cases is simply the coefficient ‘mucurved_target_positionright’. The is credibly smaller than zero (by a margin), so we may conclude that model and data provide support for our conjecture.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2d\n\n\n\n\n\nUse the posterior means of the regression coefficients to compute the corresponding scores \\(s_i\\) and class probabilities \\(c_i\\). Compare these to the observed frequencies.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\n# extract mean posteriors\nposterior_means <- fit_multinom |> tidybayes::summarise_draws() |> \n  select(variable, mean) |> \n  pivot_wider(names_from = variable, values_from = mean)\n\nas.numeric(posterior_means, names = colnames(posterior_means))  \n\n\n[1] -1.709029e+00 -1.710412e+00 -4.353493e-01 -4.736369e-03 -4.487200e+00\n[6] -1.499819e+03\n\n\nToggle code\nscores_left <- c(\n  0,\n  posterior_means[1,\"b_mucurved_Intercept\"] |> as.numeric(),\n  posterior_means[1,\"b_muCoM_Intercept\"] |> as.numeric()\n)\n\nscores_right <- c(\n  0,\n  posterior_means[1,\"b_mucurved_Intercept\"] |> as.numeric() + posterior_means[1,\"b_mucurved_target_positionright\"] |> as.numeric(),\n  posterior_means[1,\"b_muCoM_Intercept\"] |> as.numeric() + posterior_means[1,\"b_muCoM_target_positionright\"] |> as.numeric()\n)\n\nprobabilities_left <- prop.table(exp(scores_left))\nprobabilities_right <- prop.table(exp(scores_right))\n\nsum_stats |> ungroup() |> \n  mutate(prediction = c(probabilities_left, probabilities_right))\n\n\n# A tibble: 6 × 5\n  target_position prototype_label     n proportion prediction\n  <chr>           <fct>           <int>      <dbl>      <dbl>\n1 left            straight          751     0.734      0.734 \n2 left            curved            136     0.133      0.133 \n3 left            CoM               136     0.133      0.133 \n4 right           straight          793     0.771      0.771 \n5 right           curved             93     0.0904     0.0903\n6 right           CoM               143     0.139      0.139"
  },
  {
    "objectID": "practice-sheets/03a-GLM-tutorial.html#explanation-2",
    "href": "practice-sheets/03a-GLM-tutorial.html#explanation-2",
    "title": "03a: Generalized linear models",
    "section": "Explanation",
    "text": "Explanation\nWhen \\(k>2\\) categories have a natural ordering, the problem of predicting probabilities for each category can be simplified by taking this ordering into account. A common choice of link function for this case is the cumulative logit function which takes the linear predictor and a vector \\(\\delta\\) of \\(k-1\\) thresholds as arguments to return a probability vector, here denoted as \\(\\mathbf{p}\\), whose components are defined like so:\n\\[\np_j = \\text{cumulative-logit}(\\eta_1; \\delta) =\n\\begin{cases}\n\\text{logistic}(\\delta_1 - \\eta_1) & \\text{if } j=1 \\\\\n\\text{logistic}(\\delta_{i} - \\eta_1) - p_{j-1} & \\text{if } j>1 \\\\\n\\end{cases}\n\\]\nTo see what is going on, consider the a case with three categories. Fix the two threshold \\(\\delta_1=-0.75\\) and \\(\\delta_2=1.6\\) just for illustration. Now assume that we have a case there the linear predictor value \\(p\\) is zero. The cumulative logit function above then entails the category probabilities as shown in this plot, as the length of the colored bar segments:\n\n\n\n\n\nIf the linear predictor \\(\\xi\\) is estimated to be bigger than zero, this intuitively means that we shift all of the threshold to the left (by the same amount). For example, the plot below shows the case of \\(\\xi=1\\) where the probability of the first category decreases while that of the third increases.\n\n\n\n\n\nIn sum, the cumulative-logit model for ordinal regression, is defined as follows:\n\\[\n\\begin{align*}\n\\eta_i  &= \\mathbf{x}_i \\cdot \\beta       && \\color{gray}{\\text{[linear predictor]}} \\\\\n\\xi_i &= \\text{cumulative-logit}(\\eta_i; \\delta) && \\color{gray}{\\text{[predictor of central tendency]}} \\\\\ny_i & \\sim \\text{Categorical}(\\xi_i) && \\color{gray}{\\text{[likelihood]}}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "practice-sheets/03a-GLM-tutorial.html#example-2",
    "href": "practice-sheets/03a-GLM-tutorial.html#example-2",
    "title": "03a: Generalized linear models",
    "section": "Example",
    "text": "Example\nThe kind of mouse-trajectories, as categorized in variable prototype_label, are plausibly ordered by the “amount of deviation”. The following therefore tries to predict the ordered category prototype_label from the numerical measure MAD. Here is a plot of how this would look like:\n\n\nToggle code\n# prepare data by making 'prototype_label' an ordered factor\ndata_MT_prepped2 <- data_MT_prepped |> \n    mutate(prototype_label = factor(prototype_label, ordered = T))\n\n# plotting the ordered categories as a function of MAD\ndata_MT_prepped2 |> \n  ggplot(aes(x = MAD, y = prototype_label, \n             color = prototype_label)) +\n  geom_jitter(alpha = 0.3,height = 0.3, width = 0)\n\n\n\n\n\nTo run an ordinal regression model, we specify family = cumulative(). This runs the default cumulative-logit model introduced at the beginning of the session.\n\n\nToggle code\nfit_ordinal <- brm(\n  formula = prototype_label ~ MAD,\n  data = data_MT_prepped2,\n  family = cumulative()\n)\n\n\nThe summary output for this fitted model gives information about the slope of the predictor variable MAD as usual. But it also supplies information about two (!) intercepts: these are the cutoff points for the different categories in the cumulative link function.\n\n\nToggle code\nsummary(fit_ordinal)\n\n\n Family: cumulative \n  Links: mu = logit; disc = identity \nFormula: prototype_label ~ MAD \n   Data: data_MT_prepped2 (Number of observations: 2052) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept[1]     4.04      0.18     3.70     4.40 1.00     2231     2447\nIntercept[2]     9.50      0.53     8.50    10.62 1.00     1584     1841\nMAD              0.02      0.00     0.02     0.03 1.00     1910     2200\n\nFamily Specific Parameters: \n     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ndisc     1.00      0.00     1.00     1.00   NA       NA       NA\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWe can operate with the linear regression coefficients as usual, e.g., asking whether there is any reason to believe, given model and data, that the higher MAD, the higher the probability of seeing a more ‘uncertain’ trajectory type.\n\n\nToggle code\nfit_ordinal |> \n  tidybayes::gather_draws(b_MAD) |> \n  ggplot(aes(x = .value, y = .variable)) +\n  tidybayes::stat_halfeye() +\n  ylab(\"\") + xlab(\"\") + ggplot2::xlim(0,0.03)\n\n\n\n\n\n\n\nToggle code\nbrms::hypothesis(fit_ordinal, \"MAD > 0\")\n\n\nHypothesis Tests for class b:\n  Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1  (MAD) > 0     0.02         0     0.02     0.03        Inf         1    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities."
  },
  {
    "objectID": "practice-sheets/03a-GLM-tutorial.html#explanation-3",
    "href": "practice-sheets/03a-GLM-tutorial.html#explanation-3",
    "title": "03a: Generalized linear models",
    "section": "Explanation",
    "text": "Explanation\nThe Poisson distribution is the common choice for count data. It is defined as:\n\\[\n\\text{Poisson}(k ; \\lambda) = \\frac{\\lambda^k \\ \\exp( -\\lambda)} {k!}\n\\]\nThe link function is the exponential function (so the inverse link function is the logarithmic function). The Poisson regression model is defined as:\n\\[\n\\begin{align*}\n\\eta_i  &= \\mathbf{x}_i \\beta       && \\color{gray}{\\text{[linear predictor]}} \\\\\n\\xi_i &= \\exp(\\eta_i) && \\color{gray}{\\text{[predictor of central tendency]}} \\\\\ny_i & \\sim \\text{Poisson}(\\xi_i) && \\color{gray}{\\text{[likelihood]}}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "practice-sheets/03a-GLM-tutorial.html#example-3",
    "href": "practice-sheets/03a-GLM-tutorial.html#example-3",
    "title": "03a: Generalized linear models",
    "section": "Example",
    "text": "Example\nThere are examples in the next exercise sheet. For a tutorial on Poisson regression specifically geared towards linguists see here."
  },
  {
    "objectID": "practice-sheets/01a-wrangling-plotting.html",
    "href": "practice-sheets/01a-wrangling-plotting.html",
    "title": "Wrangling & plotting",
    "section": "",
    "text": "Here is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\n\n\nToggle code\n# install packages from CRAN (unless installed)\npckgs_needed <- c(\n  \"tidyverse\",\n  \"brms\",\n  \"rstan\",\n  \"rstanarm\",\n  \"remotes\",\n  \"tidybayes\",\n  \"bridgesampling\",\n  \"shinystan\",\n  \"mgcv\"\n)\npckgs_installed <- installed.packages()[,\"Package\"]\npckgs_2_install <- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]\nif(length(pckgs_2_install)) {\n  install.packages(pckgs_2_install)\n} \n\n# install additional packages from GitHub (unless installed)\nif (! \"aida\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/aida-package\")\n}\nif (! \"faintr\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/faintr\")\n}\nif (! \"cspplot\" %in% pckgs_installed) {\n  remotes::install_github(\"CogSciPrag/cspplot\")\n}\n\n# load the required packages\nx <- lapply(pckgs_needed, library, character.only = TRUE)\nlibrary(aida)\nlibrary(faintr)\nlibrary(cspplot)\n\n# these options help Stan run faster\noptions(mc.cores = parallel::detectCores())\n\n# use the CSP-theme for plotting\ntheme_set(theme_csp())\n\n# global color scheme from CSP\nproject_colors = cspplot::list_colors() |> pull(hex)\n# names(project_colors) <- cspplot::list_colors() |> pull(name)\n\n# setting theme colors globally\nscale_colour_discrete <- function(...) {\n  scale_colour_manual(..., values = project_colors)\n}\nscale_fill_discrete <- function(...) {\n   scale_fill_manual(..., values = project_colors)\n}"
  },
  {
    "objectID": "practice-sheets/01a-wrangling-plotting.html#loading-and-inspecting-the-data",
    "href": "practice-sheets/01a-wrangling-plotting.html#loading-and-inspecting-the-data",
    "title": "Wrangling & plotting",
    "section": "Loading and inspecting the data",
    "text": "Loading and inspecting the data\nThe data is part of the aida package, but we can give it a fancy new name:\n\n\nToggle code\ndolphin <- aida::data_MT\n\n\nTo get some information about the data set, we can use the help function:\n\n\nToggle code\nhelp(\"data_MT\")\n\n\nHere is some more information we can get about the data:\n\n\nToggle code\n# number of rows in the data set\nnrow(dolphin)\n\n\n[1] 2052\n\n\nToggle code\n# number of columns in the data set\nncol(dolphin)\n\n\n[1] 16\n\n\nToggle code\n# names of the columns\nnames(dolphin)\n\n\n [1] \"X1\"               \"trial_id\"         \"MAD\"              \"AUC\"             \n [5] \"xpos_flips\"       \"RT\"               \"prototype_label\"  \"subject_id\"      \n [9] \"group\"            \"condition\"        \"exemplar\"         \"category_left\"   \n[13] \"category_right\"   \"category_correct\" \"response\"         \"correct\"         \n\n\nToggle code\n# number of unique `subject_id`s\ndolphin$subject_id |> unique() |> length()\n\n\n[1] 108\n\n\nToggle code\n# number of types each subject saw different `conditions`\ndolphin |> with(table(subject_id, condition)) |> head()\n\n\n          condition\nsubject_id Atypical Typical\n      1001        6      13\n      1002        6      13\n      1003        6      13\n      1004        6      13\n      1005        6      13\n      1006        6      13"
  },
  {
    "objectID": "practice-sheets/01a-wrangling-plotting.html#a-closer-look-at-the-columns",
    "href": "practice-sheets/01a-wrangling-plotting.html#a-closer-look-at-the-columns",
    "title": "Wrangling & plotting",
    "section": "A closer look at the columns",
    "text": "A closer look at the columns\nLet’s take a closer look at the columns and the information inside them.\nWe can get a glimpse of all columns like so:\n\n\nToggle code\nglimpse(dolphin)\n\n\nRows: 2,052\nColumns: 16\n$ X1               <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16…\n$ trial_id         <chr> \"id0001\", \"id0002\", \"id0003\", \"id0004\", \"id0005\", \"id…\n$ MAD              <dbl> 82.53319, 44.73484, 283.48207, 138.94863, 401.93988, …\n$ AUC              <dbl> 40169.5, 13947.0, 84491.5, 74084.0, 223083.0, 308376.…\n$ xpos_flips       <dbl> 3, 1, 2, 0, 2, 2, 1, 0, 2, 0, 2, 2, 0, 0, 3, 1, 0, 1,…\n$ RT               <dbl> 950, 1251, 930, 690, 951, 1079, 1050, 830, 700, 810, …\n$ prototype_label  <chr> \"straight\", \"straight\", \"curved\", \"curved\", \"cCoM\", \"…\n$ subject_id       <dbl> 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001,…\n$ group            <chr> \"touch\", \"touch\", \"touch\", \"touch\", \"touch\", \"touch\",…\n$ condition        <chr> \"Atypical\", \"Typical\", \"Atypical\", \"Atypical\", \"Typic…\n$ exemplar         <chr> \"eel\", \"rattlesnake\", \"bat\", \"butterfly\", \"hawk\", \"pe…\n$ category_left    <chr> \"fish\", \"amphibian\", \"bird\", \"Insekt\", \"bird\", \"fish\"…\n$ category_right   <chr> \"reptile\", \"reptile\", \"mammal\", \"bird\", \"reptile\", \"b…\n$ category_correct <chr> \"fish\", \"reptile\", \"mammal\", \"Insekt\", \"bird\", \"bird\"…\n$ response         <chr> \"fish\", \"reptile\", \"bird\", \"Insekt\", \"bird\", \"bird\", …\n$ correct          <dbl> 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,…\n\n\nHere is a quick explanation of all the different columns:\n\ntrial_id = unique id for individual trials\nMAD = maximal deviation into competitor space\nAUC = area under the curve\nxpos_flips = the amount of horizontal direction changes\nRT = reaction time in ms\nprototype_label = different categories of prototypical movement strategies\nsubject_id = unique id for individual participants\ngroup = groups differ in the response design (click vs. touch)\ncondition = category membership (Typical vs. Atypical)\nexemplar = the concrete animal\ncategory_left = the category displayed on the left\ncategory_right = the category displayed on the right\ncategory_correct= the category that is correct\nresponse = the selected category\ncorrect = whether or not the response matches category_correct"
  },
  {
    "objectID": "practice-sheets/01a-wrangling-plotting.html#selecting-columns",
    "href": "practice-sheets/01a-wrangling-plotting.html#selecting-columns",
    "title": "Wrangling & plotting",
    "section": "Selecting columns",
    "text": "Selecting columns\nFor now, we are only interested in columns RT, group, condition, category_correct, and correct. We can use the select() function of dplyr to get rid of columns we don’t need.\n\n\nToggle code\n# selecting specific columns\ndolphin_selected <-\n  dolphin |>\n  dplyr::select(RT, group, condition, category_correct, correct)\n \n# let's have a look\ndolphin_selected\n\n\n# A tibble: 2,052 × 5\n      RT group condition category_correct correct\n   <dbl> <chr> <chr>     <chr>              <dbl>\n 1   950 touch Atypical  fish                   1\n 2  1251 touch Typical   reptile                1\n 3   930 touch Atypical  mammal                 0\n 4   690 touch Atypical  Insekt                 1\n 5   951 touch Typical   bird                   1\n 6  1079 touch Atypical  bird                   1\n 7  1050 touch Typical   fish                   1\n 8   830 touch Typical   fish                   1\n 9   700 touch Typical   mammal                 1\n10   810 touch Typical   fish                   1\n# ℹ 2,042 more rows"
  },
  {
    "objectID": "practice-sheets/01a-wrangling-plotting.html#filtering-rows",
    "href": "practice-sheets/01a-wrangling-plotting.html#filtering-rows",
    "title": "Wrangling & plotting",
    "section": "Filtering rows",
    "text": "Filtering rows\nIf we care only about a subset of rows, we can use the filter() function. For example, let’s filter all trials in which the correct category was either a fish or a mammal\n\n\nToggle code\ndolphin_filter1 <-\n  dolphin_selected |> \n  filter(category_correct == \"fish\" | category_correct == \"mammal\")\n  # the | is a logical operator that indicates that either the first expression OR \n  # the second one has to be true\n\ndolphin_filter1\n\n\n# A tibble: 1,296 × 5\n      RT group condition category_correct correct\n   <dbl> <chr> <chr>     <chr>              <dbl>\n 1   950 touch Atypical  fish                   1\n 2   930 touch Atypical  mammal                 0\n 3  1050 touch Typical   fish                   1\n 4   830 touch Typical   fish                   1\n 5   700 touch Typical   mammal                 1\n 6   810 touch Typical   fish                   1\n 7  1264 touch Typical   mammal                 1\n 8   890 touch Atypical  mammal                 0\n 9  1040 touch Typical   mammal                 1\n10   730 touch Typical   mammal                 1\n# ℹ 1,286 more rows\n\n\nYou can also filter() against particular conditions. For example, let’s filter all rows that do not have bird as their correct category:\n\n\nToggle code\ndolphin_filter2 <-\n  dolphin_selected |> \n  filter(category_correct != \"bird\")\n\ndolphin_filter2\n\n\n# A tibble: 1,728 × 5\n      RT group condition category_correct correct\n   <dbl> <chr> <chr>     <chr>              <dbl>\n 1   950 touch Atypical  fish                   1\n 2  1251 touch Typical   reptile                1\n 3   930 touch Atypical  mammal                 0\n 4   690 touch Atypical  Insekt                 1\n 5  1050 touch Typical   fish                   1\n 6   830 touch Typical   fish                   1\n 7   700 touch Typical   mammal                 1\n 8   810 touch Typical   fish                   1\n 9  1264 touch Typical   mammal                 1\n10   890 touch Atypical  mammal                 0\n# ℹ 1,718 more rows\n\n\nWe can also filter according to multiple conditions at once, including numeric conditions. Here, we also filter for trials that have correct responses.\n\n\nToggle code\ndolphin_filter3 <-\n  dolphin_selected |> \n  filter(category_correct != \"bird\",\n         correct == 1)\n\ndolphin_filter3\n\n\n# A tibble: 1,602 × 5\n      RT group condition category_correct correct\n   <dbl> <chr> <chr>     <chr>              <dbl>\n 1   950 touch Atypical  fish                   1\n 2  1251 touch Typical   reptile                1\n 3   690 touch Atypical  Insekt                 1\n 4  1050 touch Typical   fish                   1\n 5   830 touch Typical   fish                   1\n 6   700 touch Typical   mammal                 1\n 7   810 touch Typical   fish                   1\n 8  1264 touch Typical   mammal                 1\n 9  1040 touch Typical   mammal                 1\n10   730 touch Typical   mammal                 1\n# ℹ 1,592 more rows"
  },
  {
    "objectID": "practice-sheets/01a-wrangling-plotting.html#grouping-and-summarizing",
    "href": "practice-sheets/01a-wrangling-plotting.html#grouping-and-summarizing",
    "title": "Wrangling & plotting",
    "section": "Grouping and summarizing",
    "text": "Grouping and summarizing\nWe can also generate summary statistics of certain variables with a combination of group_by() & summarise(). Let’s get the means and standard deviations of the reactions times for each level in the variable condition. We also include the minimum and maximum values for each condition.\n\n\nToggle code\ndolphin_aggregate <-\n  dolphin_filter3 |>\n  group_by(condition) |>\n  summarise(\n    min_RT  = min(RT),\n    mean_RT = mean(RT, na.rm = T),\n    sd_RT   = sd(RT, na.rm = T),\n    max_RT  = max(RT)\n    )\n  # the na.rm = T is an argument that is used to tell R that NAs should be ignored \n  # when calculating the summary statistics\n\n# show the aggregated df\ndolphin_aggregate\n\n\n# A tibble: 2 × 5\n  condition min_RT mean_RT sd_RT max_RT\n  <chr>      <dbl>   <dbl> <dbl>  <dbl>\n1 Atypical     630   2149. 1840.  19903\n2 Typical      510   1665. 1283.  20685\n\n\nSo we find that atypical categories are responded to slower than typical categories. Makes sense. Identifying a dolphin as a mammal might be difficult because it shares a lot of features with fish.\nWe can group according to many different factors simultaneously, and we can create multiple summary statistics at the same time. Here, we get summary statistics for each combination of all levels in variables condition and group. We use the tidyboot package to get bootstrapped 95% confidence intervalls. (Notice that these are more informative than standard deviations in the sense that they give an upper and lower deviation, not just one number for both directions, which can be misleading when the data is skewed (like reaction times typically are)):\n\n\nToggle code\ndolphin_aggregate2 <-\n  dolphin_filter3 |>\n  group_by(group, condition) |>\n  summarize(\n    lower_CI = tidyboot::ci_lower(RT),\n    mean_RT  = mean(RT, na.rm = T),\n    upper_CI = tidyboot::ci_upper(RT)\n    )\n\n# show the aggregated df\ndolphin_aggregate2\n\n\n# A tibble: 4 × 5\n# Groups:   group [2]\n  group condition lower_CI mean_RT upper_CI\n  <chr> <chr>        <dbl>   <dbl>    <dbl>\n1 click Atypical     1030.   2417.    7311.\n2 click Typical       950    1847.    4698.\n3 touch Atypical      750    1900.    5368.\n4 touch Typical       686.   1486.    3955.\n\n\nWe can see here that the group that needed to click on a response are overall slower than touch responses, but also much more variable in their behavior."
  },
  {
    "objectID": "practice-sheets/01a-wrangling-plotting.html#changing-and-adding-columns",
    "href": "practice-sheets/01a-wrangling-plotting.html#changing-and-adding-columns",
    "title": "Wrangling & plotting",
    "section": "Changing and adding columns",
    "text": "Changing and adding columns\nOften, we are interested in standardized measures because we do not know what a value of 1 means on any given scale. Is 1 a large difference or a small difference? For example when we want to explore the impact of several predictors on the same measurement, we want to know the relative size of a number. To achieve this, we standardize measures by dividing their mean by their respective standard deviations. We will use the scale() function for this and create a new variable in our data frame via mutate().\n(Note that the scale() function creates an object that is of the matrix class. That is fine for the most part but might create issues later on. To avoid any issues, we wrap the scale() function in as.numeric() to store the results as a numeric vector.)\n\n\nToggle code\ndolphin_standardize <-\n  dolphin_selected |>\n  mutate(RT_scale = as.numeric(scale(RT)))\n  \nhead(dolphin_standardize)\n\n\n# A tibble: 6 × 6\n     RT group condition category_correct correct RT_scale\n  <dbl> <chr> <chr>     <chr>              <dbl>    <dbl>\n1   950 touch Atypical  fish                   1   -0.552\n2  1251 touch Typical   reptile                1   -0.373\n3   930 touch Atypical  mammal                 0   -0.564\n4   690 touch Atypical  Insekt                 1   -0.706\n5   951 touch Typical   bird                   1   -0.551\n6  1079 touch Atypical  bird                   1   -0.475\n\n\nIf we now compare, say atypical and typical categories according to reaction times, we can use the standardized RT ratings. Let’s do all of this in one “pipeline”.\n\n\nToggle code\ndolphin_agg_standardize <- dolphin_selected |>\n  mutate(RT_scale = scale(RT)) |> \n  group_by(condition) |>\n  summarise(mean_RT_scale = mean(RT_scale, na.rm = T))\n  \nhead(dolphin_agg_standardize)\n\n\n# A tibble: 2 × 2\n  condition mean_RT_scale\n  <chr>             <dbl>\n1 Atypical          0.219\n2 Typical          -0.101\n\n\nNow we can see that atypical categories exhibit relatively higher RTs, i.e., more than 0.3 standard deviations higher than for typical categories."
  },
  {
    "objectID": "practice-sheets/01a-wrangling-plotting.html#exercises-for-data-wrangling",
    "href": "practice-sheets/01a-wrangling-plotting.html#exercises-for-data-wrangling",
    "title": "Wrangling & plotting",
    "section": "Exercises for data wrangling",
    "text": "Exercises for data wrangling\n\nExercise 1\n\n\n\n\n\n\nExercise 1a\n\n\n\n\n\nTake the dolphin data set and store a reduced variant of it as dolphin_reduced. The new data frame should contain only the following columns: RT, AUC, group, and exemplar.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\ndolphin_reduced <- dolphin |>\n  select(RT, AUC, group, exemplar)\n\nhead(dolphin_reduced)\n\n\n# A tibble: 6 × 4\n     RT     AUC group exemplar   \n  <dbl>   <dbl> <chr> <chr>      \n1   950  40170. touch eel        \n2  1251  13947  touch rattlesnake\n3   930  84492. touch bat        \n4   690  74084  touch butterfly  \n5   951 223083  touch hawk       \n6  1079 308376  touch penguin    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1b\n\n\n\n\n\nWe are for now only interested in those data that have whales as the exemplar. filter() only those rows and store them in a new dataframe called whales_only.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\nwhales_only <- dolphin_reduced |> \n  filter(exemplar == \"whale\")\n\nhead(whales_only)\n\n\n# A tibble: 6 × 4\n     RT     AUC group exemplar\n  <dbl>   <dbl> <chr> <chr>   \n1   760  13498  touch whale   \n2  1990  42404  click whale   \n3  3613 -10167  click whale   \n4  2030 162678  touch whale   \n5  1490  54054  touch whale   \n6  1305  74222. touch whale   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1c\n\n\n\n\n\nNow filter for only those data that have RTs below 1500ms.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\nwhales_only2 <- whales_only |> \n  filter(RT < 1500)\n\nhead(whales_only2)\n\n\n# A tibble: 6 × 4\n     RT     AUC group exemplar\n  <dbl>   <dbl> <chr> <chr>   \n1   760  13498  touch whale   \n2  1490  54054  touch whale   \n3  1305  74222. touch whale   \n4  1350   -643  touch whale   \n5  1040  19426. touch whale   \n6  1141 -44260. click whale   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1d\n\n\n\n\n\nWe don’t like that AUC is unstandardized. Use mutate() to create a new vector that represents scaled AUC values (scaling is achieved by the function scale()).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\nwhales_only_scaled <- whales_only2 |> \n  mutate(AUC_scaled = scale(AUC))\n\nhead(whales_only_scaled)\n\n\n# A tibble: 6 × 5\n     RT     AUC group exemplar AUC_scaled[,1]\n  <dbl>   <dbl> <chr> <chr>             <dbl>\n1   760  13498  touch whale           -0.456 \n2  1490  54054  touch whale           -0.212 \n3  1305  74222. touch whale           -0.0912\n4  1350   -643  touch whale           -0.541 \n5  1040  19426. touch whale           -0.420 \n6  1141 -44260. click whale           -0.803 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1e\n\n\n\n\n\nCalculate the mean scaled AUC ratings for both both groups.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\nwhales_aggregate <- whales_only_scaled |> \n  group_by(group) |> \n  summarise(mean_AUC_scaled = mean(AUC_scaled, na.rm =TRUE))\n\nhead(whales_aggregate)\n\n\n# A tibble: 2 × 2\n  group mean_AUC_scaled\n  <chr>           <dbl>\n1 click           0.798\n2 touch          -0.372\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1f\n\n\n\n\n\nDo all of the above (a-e) in one pipeline.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\nwhales_aggregate <- dolphin |>\n  select(RT, AUC, group, exemplar) |> \n  filter(exemplar == \"whale\",\n         RT < 1500) |> \n  mutate(AUC_scaled = scale(AUC)) |> \n  group_by(group) |> \n  summarise(mean_AUC_scaled = mean(AUC_scaled, na.rm =TRUE))\n  \nhead(whales_aggregate)\n\n\n# A tibble: 2 × 2\n  group mean_AUC_scaled\n  <chr>           <dbl>\n1 click           0.798\n2 touch          -0.372\n\n\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\n\n\nExercise 2a\n\n\n\n\n\nTake the dolphin data set and store a reduced variant of it. The new data frame should contain only the columns condition, group, and xpos_flips, correct. And within the correct vector, we are only interested in the correct trials (= 1). Filter accordingly.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\ndolphin_sub <- dolphin |> \n  select(condition, group, xpos_flips, correct) |> \n  filter(correct == 1)\n\nhead(dolphin_sub)\n\n\n# A tibble: 6 × 4\n  condition group xpos_flips correct\n  <chr>     <chr>      <dbl>   <dbl>\n1 Atypical  touch          3       1\n2 Typical   touch          1       1\n3 Atypical  touch          0       1\n4 Typical   touch          2       1\n5 Atypical  touch          2       1\n6 Typical   touch          1       1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2b\n\n\n\n\n\nCreate an aggregated data frame that contains the mean xpos_flips value and the standard deviation for group and condition.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\ndolphin_agg <- dolphin_sub |>\n  group_by(group, condition) |> \n  summarise(mean_xpos_flips = mean(xpos_flips, na.rm = TRUE),\n            sd_xpos_flips = sd(xpos_flips, na.rm = TRUE))\n\nhead(dolphin_agg)\n\n\n# A tibble: 4 × 4\n# Groups:   group [2]\n  group condition mean_xpos_flips sd_xpos_flips\n  <chr> <chr>               <dbl>         <dbl>\n1 click Atypical            1.33          1.23 \n2 click Typical             0.956         1.05 \n3 touch Atypical            0.797         1.10 \n4 touch Typical             0.572         0.938\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2c\n\n\n\n\n\nUse the rename() function to rename the new vectors for the mean xflips and their standard deviation to xflips_mean and xflips_sd.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\ndolphin_agg2 <- dolphin_agg |>\n  rename(xflips_mean = mean_xpos_flips,\n         xflips_sd = sd_xpos_flips)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2d\n\n\n\n\n\nDo all of the above (a-c) in one pipeline.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\ndolphin |> \n  select(condition, group, xpos_flips, correct) |> \n  filter(correct == 1) |> \n  group_by(group, condition) |> \n  summarise(mean_xpos_flips = mean(xpos_flips, na.rm = TRUE),\n            sd_xpos_flips = sd(xpos_flips, na.rm = TRUE)) |> \n  rename(xflips_mean = mean_xpos_flips,\n         xflips_sd = sd_xpos_flips)\n\n\n# A tibble: 4 × 4\n# Groups:   group [2]\n  group condition xflips_mean xflips_sd\n  <chr> <chr>           <dbl>     <dbl>\n1 click Atypical        1.33      1.23 \n2 click Typical         0.956     1.05 \n3 touch Atypical        0.797     1.10 \n4 touch Typical         0.572     0.938"
  },
  {
    "objectID": "practice-sheets/01a-wrangling-plotting.html#basic-plots",
    "href": "practice-sheets/01a-wrangling-plotting.html#basic-plots",
    "title": "Wrangling & plotting",
    "section": "Basic plots",
    "text": "Basic plots\nNow that we have pre-processed our data set, we are ready to visually explore it. Let’s start very simple. Let’s plot a bar plot. Let’s also add a title to our plot.\n\n\nToggle code\nggplot(dolphin_agg, aes(x = condition, y = mean_RT)) +\n  geom_bar(stat = \"identity\") +\n  ggtitle(\"a bare bar plot\")\n\n\n\n\n\n\n\n\n\nToggle code\n  # stat = \"identity\" takes the number in the dataset as the bar height (as opposed to a 'count')\n\n\nUgh! What an ugly plot, right? But it’s already telling a story: Atypical categories are responded to slower than typical categories. Let’s add a measure of uncertainty, in our case the bootstrapped 95% confidence intervals, as error bars to the plot:\n\n\nToggle code\nggplot(dolphin_agg, aes(x = condition, y = mean_RT)) +\n  geom_bar(stat = \"identity\") + \n  \n  # this is the added layer\n  geom_errorbar(aes(ymin = lower_CI, \n                    ymax = upper_CI), \n                colour = \"black\",\n                linewidth = 0.5) +\n  \n  ggtitle(\"a bare bar plot with error bars\")\n\n\n\n\n\nWe can observe a couple of things here. First, ggplot automatically adjust the axes based on the elements to be plotted unless we tell it not to. Second, the error bars are plotted in front of the bars, i.e. closer to the viewer. This visual ordering reflects the order of layers. We first plotted the bars and THEN the error bars.\nBeyond bar plots, we can create other useful plots types. For example a point plot. Instead of a bar, we plot the mean RT as points.\n\n\nToggle code\nggplot(dolphin_agg, aes(x = condition, y = mean_RT)) +\n  geom_errorbar(aes(ymin = lower_CI, \n                    ymax = upper_CI), \n                colour = \"black\") +  \n  # this is the new geom \n  geom_point() +\n  ggtitle(\"a point plot\")\n\n\n\n\n\n\n\n\n\nOr a line plot that connects the means with a line. For the line plot to work, we need to indicate a group aesthetic, i.e. the group that we want to connect with a line. If you have for example several interacting categories, you need to indicate which groups are supposed to be connected with lines (see below). Because we have only one group here, condition, we set group to 1.\n\n\nToggle code\nggplot(dolphin_agg, aes(x = condition, y = mean_RT, group = 1)) +\n  geom_line() +\n  ggtitle(\"a line plot\")\n\n\n\n\n\n\n\n\n\nYay, we are on a roll. Let’s plot a box plot. Remember the box shows the median (middle vertical line) and the interquartile range (the middle 50% of the data within the box). Note that for the box plot, we do not plot aggregated values, so we need to refer to the entire data set. We also add the aesthetic fill here and set it to the variable condition to color code our boxes.\n\n\nToggle code\n# we changed the dataset referred to\nggplot(dolphin, aes(x = condition, y = RT, fill = condition)) +\n  # this is the new geom \n  geom_boxplot() +\n  ggtitle(\"a box plot\")\n\n\n\n\n\n\n\n\n\nWhile the above plots illustrate one continuous variable (RT) plotted against a categorical variable (condition), we can also plot two continuous variables against each other. For example, we could plot RT against AUC in a scatter plot.\n\n\nToggle code\n# we changed the y aesthetic to `Hardness`\nggplot(dolphin_subset, aes(x = RT, y = AUC)) +\n  geom_point() +\n  ggtitle(\"a scatter plot\")\n\n\n\n\n\n\n\n\n\nFinally, one central plot type for our class. The density plot. It plots on the x-axis a continous value and on the y-axis the “density”” of these values. So high values on the y-axis means a lot of data at the corresponding x-values. The density curve can be outlined with color and filled with fill. To keep the two categories visually distinct, we add an argument to the geom_density() function: alpha. Alpha controls the transparency of the color! We will see a lot of these density plots in our class.\n\n\nToggle code\nggplot(dolphin, aes(x = RT, color = condition, fill = condition)) +\n  geom_density(alpha = 0.5) +\n  ggtitle(\"a density plot\")"
  },
  {
    "objectID": "practice-sheets/01a-wrangling-plotting.html#adjusting-plot-elements",
    "href": "practice-sheets/01a-wrangling-plotting.html#adjusting-plot-elements",
    "title": "Wrangling & plotting",
    "section": "Adjusting plot elements",
    "text": "Adjusting plot elements\nOkay, so we are now already capable of exploring our data visually with a bunch of plots. These plots are exceptionally ugly and of limited communicative value so far. Note that this is perfectly fine during an exploratory phase of data analysis. If we just eye-ball data and we have no trouble interpreting the plots, thats just fine. However, as soon as we want to communicate patterns to others with these graphs, we need to take a little bit more care of its communicative value. Let’s look at ways we can tailor our plots to the needs of an audience.\nLet’s go back to our bar plot and explore whether condition and group has an impact on RT?\n\n\nToggle code\n# First we aggregate RT for group and condition\ndolphin_agg2 <- dolphin_subset |>\n  group_by(group, condition) |>\n  summarise(mean_RT = mean(RT),\n            sd_RT = sd(RT))\n\n# then we plot and color code for condition (note that, for bar plots, the aesthetic of `color` refers to the border of the bar, and `fill` refers to the actual colour of the bar)\n\nggplot(dolphin_agg2, aes(x = condition, y = mean_RT, fill = group)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\n\n\nHm… that doesn’t work out, the bars are on top of each other, we need to tell ggplot to position the bars next to each other instead. We do that with position_dogde(). Note that ggplot assigns a default color coding scheme to your plots if you don’t specify it by hand.\n\n\nToggle code\nggplot(dolphin_agg2, aes(x = condition, y = mean_RT, fill = group)) +\n  geom_bar(stat = \"identity\", position = position_dodge())\n\n\n\n\n\n\n\n\n\nAwww much better! Alternatively, we can plot the two categories into separate panels. We achieve this by facetting using the facet_grid() function.\n\n\nToggle code\nggplot(dolphin_agg2, aes(x = group, y = mean_RT, fill = condition)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  # this is the facetting function\n  facet_grid(~ condition)\n\n\n\n\n\n\n\n\n\nOkay. We are getting somewhere. We already learned something again. Apparently, the effect of typiciality on RT is pretty similar across tasks. It does not look like we have an interaction here (more on interactions later).\nNow let’s make these plots ready to communicate information. We add appropriate axes titles. Note that we are using a little hack here: The “” inserts an empty line, creating visual distance from axis title to axis, thus making it easier to read it. Our audience will thank us.\n\n\nToggle code\nggplot(dolphin_agg2, aes(x = group, y = mean_RT, fill = condition)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  facet_grid(~ condition) +\n  # add axis titles\n  xlab(\"\\n task\") +\n    ylab(\"mean response latency in ms\\n\") \n\n\n\n\n\n\n\n\n\nThe same graph as a point / line plot indicated to the audience whether there is an interaction pattern or not. Note that here we do not facet because we actually want the points to be plotted within the same horizontal space. We also have to specify the group aesthetic to tell ggplot which points to connect with lines.\n\n\nToggle code\nggplot(dolphin_agg2, aes(x = group, y = mean_RT, color = condition, group = condition)) +\n  # instead of geom_bar we use geom_point and geom_line\n  geom_point(size = 12) +\n  geom_line(size = 2) +\n  xlab(\"\\n task\") +\n  ylab(\"mean response latency in ms\\n\") \n\n\n\n\n\n\n\n\n\nToggle code\n  # # need to change to the color aesthetic instead of fill\n  # scale_y_continuous(expand = c(0, 0), breaks = (c(0, 500, 1000, 1500, 2000, 2500, 3000)), limits = c(0,3000))\n\n\nThese lines look pretty parallel and don’t indicate a strong interaction pattern. But how do different exemplars differ? Let’s aggregate for individual exemplars first and then create the same plot for the means of all exemplars.\n\n\nToggle code\ndolphin_agg3 <- dolphin_subset |> \n  group_by(exemplar, group, condition) |> \n  summarise(mean_RT = mean(RT, na.rm = TRUE))\n\nggplot(dolphin_agg3, aes(x = group, y = mean_RT, color = condition, group = exemplar)) +\n  # instead of geom_bar we use geom_point and geom_line\n  geom_point(size = 6, alpha = 0.3) +\n  geom_line() +\n  geom_label(aes(label = exemplar)) +\n  xlab(\"\\n task\") +\n  ylab(\"mean response latency in ms\\n\")\n\n\n\n\n\n\n\n\n\nIt looks like “shark” and “rattlesnake” behave very different from their buddies in the typical condition. Interesting! We wouldn’t have noticed if we had only looked at the overall means."
  },
  {
    "objectID": "practice-sheets/01a-wrangling-plotting.html#exercises-for-plotting",
    "href": "practice-sheets/01a-wrangling-plotting.html#exercises-for-plotting",
    "title": "Wrangling & plotting",
    "section": "Exercises for plotting",
    "text": "Exercises for plotting\nTake the scatter plot below as a departure point. It plots AUC (area-under-the-curve) against MAD (maximal absolute deviation).\n\n\nToggle code\nggplot(dolphin, aes(x = MAD, y = AUC)) +\n  geom_point() +\n  ggtitle(\"a scatter plot\")\n\n\n\n\n\n\n\n\n\n\n\nExercise 3a\n\n\n\n\n\n\nChange both the x-axis and the y-axis title to sensible and informative titles.\nChange the plot title to something informative.\nChange the scaling of the x-axis to display only MAD values between -500 and 500\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\nggplot(dolphin, aes(x = MAD, y = AUC, \n                    color = group)) +\n  geom_point() +\n  # (1) axes titles\n  xlab(\"\\n maximal absolute deviation\") +\n  ylab(\"area-under-the-curve \\n\") +\n  # (2) change title\n  ggtitle(\"MAD is correlated with AUC\") +\n  # (3) change x-axis (note that certain values are not displayed then. R will spit out a warning)\n  scale_x_continuous(limits = c(-500,500))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 3b\n\n\n\n\n\n\nPlot AUC values as a function of group in a density plot (geom_density).\nMake the density curves semi-transparent with the alpha argument\nAdd the mean values for both groups into the density plot as a line.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\n# (1 - 3)\nggplot(dolphin, aes(x = AUC, color = group, fill = group)) +\n  geom_density(alpha = 0.3) +\n  xlab(\"\\n AUC\")\n\n\n\n\n\n\n\nToggle code\n# (4) aggregate means and add to plot\n\ndolphin_agg <- dolphin |>\n  group_by(group) |>\n  summarise(mean_AUC = mean(AUC, na.rm = TRUE),\n            sd_AUC = sd(AUC, na.rm = TRUE))\n\n# add them to the plot as vertical lines\nggplot(dolphin, aes(x = AUC, color = group, fill = group)) +\n  geom_density(alpha = 0.3) +\n  xlab(\"\\n AUC\") +\n  # since the vertical line refers to dolphin_agg, we need to specify the dataset explicitly \n  geom_vline(data = dolphin_agg, \n             aes(xintercept = mean_AUC, color = group),\n             lty = \"dashed\")"
  },
  {
    "objectID": "practice-sheets/02b-hierarchical-models-exercises.html",
    "href": "practice-sheets/02b-hierarchical-models-exercises.html",
    "title": "05b: Hierarchical regression models (exercises)",
    "section": "",
    "text": "Preamble\nHere is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\n\n\nToggle code\n# install packages from CRAN (unless installed)\npckgs_needed <- c(\n  \"tidyverse\",\n  \"brms\",\n  \"rstan\",\n  \"rstanarm\",\n  \"remotes\",\n  \"tidybayes\",\n  \"bridgesampling\",\n  \"shinystan\",\n  \"mgcv\"\n)\npckgs_installed <- installed.packages()[,\"Package\"]\npckgs_2_install <- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]\nif(length(pckgs_2_install)) {\n  install.packages(pckgs_2_install)\n} \n\n# install additional packages from GitHub (unless installed)\nif (! \"aida\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/aida-package\")\n}\nif (! \"faintr\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/faintr\")\n}\nif (! \"cspplot\" %in% pckgs_installed) {\n  remotes::install_github(\"CogSciPrag/cspplot\")\n}\n\n# load the required packages\nx <- lapply(pckgs_needed, library, character.only = TRUE)\nlibrary(aida)\nlibrary(faintr)\nlibrary(cspplot)\n\n# these options help Stan run faster\noptions(mc.cores = parallel::detectCores())\n\n# use the CSP-theme for plotting\ntheme_set(theme_csp())\n\n# global color scheme from CSP\nproject_colors = cspplot::list_colors() |> pull(hex)\n# names(project_colors) <- cspplot::list_colors() |> pull(name)\n\n# setting theme colors globally\nscale_colour_discrete <- function(...) {\n  scale_colour_manual(..., values = project_colors)\n}\nscale_fill_discrete <- function(...) {\n   scale_fill_manual(..., values = project_colors)\n}\n\n\n\n\nToggle code\ndolphin <- aida::data_MT\nmy_scale <- function(x) c(scale(x))\n\n\n\n\nExercise 1: Logistic regression\nConsider the following model formula for the dolphin data set:\n\n\nToggle code\nbrms::bf(MAD ~ condition + \n     (condition || subject_id) +\n     (condition || exemplar))\n\n\nMAD ~ condition + (condition || subject_id) + (condition || exemplar) \n\n\n\n\n\n\n\n\nExercise 1a\n\n\n\n\n\nWhy is the random effect structure of this model questionable? Can we meaningfully estimate all parameters? (Hint: Think about what group levels vary across predictor levels)\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFactor condition is not crossed with exemplar. An exemplar is either typical or atypical, thus a random slope does not make sense.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1b\n\n\n\n\n\nUse the following data frame:\n\n\nToggle code\n# set up data frame\ndolphin_correct <- dolphin %>% \n  filter(correct == 1) %>% \n  mutate(log_RT_s = my_scale(log(RT)),\n         AUC_s = my_scale(AUC))\n\n\nRun a multilevel model that predicts AUC_s based on condition. Specify maximal random effect structures for exemplars and subject_ids (ignore correlations between intercepts and slopes for now). Specify a seed = 98.\nIf you encounter “divergent transition” warning, make them go away by refitting the model appropriately (Hint: brms gives very useful, actionable advice)\n(This might take a couple of minutes, get used to it ;)\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\n# refit with upped adapt_delta and max_treedepth\nxmdl_AUC2 <- brm(AUC_s ~ condition +\n                  (condition || subject_id) +\n                  (1 | exemplar),\n                data = dolphin_correct,\n                control=list(adapt_delta=0.99, max_treedepth=15), \n                seed = 98\n                )\nxmdl_AUC2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1c\n\n\n\n\n\nYou want to run a multilevel model that predicts log_RT_s based on group. You want to account for group-level variation of both subject_id and exemplar. What kind of groupings can be meaningfully estimated, given the dataset and the experimental design. You can check the crossing of different vectors with xtabs() for example.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\n# check crossing\nxtabs(~ group + subject_id, dolphin_correct)\n\n\n       subject_id\ngroup   1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014\n  click    0   19   17    0    0    0   19    0    0   19    0    0   17   16\n  touch   16    0    0   18   17   19    0   18   19    0   19   16    0    0\n       subject_id\ngroup   1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028\n  click   18    0    0    0   19   17    0   19   14    0    0   19    0   19\n  touch    0   18   19   17    0    0   18    0    0   19   12    0   17    0\n       subject_id\ngroup   1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042\n  click    0   19   15    0   17   19    0    0   18    0   19    0   17    0\n  touch   19    0    0   19    0    0   19   16    0   17    0   18    0   18\n       subject_id\ngroup   1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056\n  click    0   17   17   18    0   18   17   18    0    0    0    0   18    0\n  touch   19    0    0    0   19    0    0    0   18   17   18   19    0   19\n       subject_id\ngroup   1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070\n  click    0    0   18   18   17    0   16   17    0   18    0   17    0   18\n  touch   19   18    0    0    0   19    0    0   18    0   18    0   17    0\n       subject_id\ngroup   1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084\n  click   19   19    0   19    0   19    0   19   19    0    0    0   18   19\n  touch    0    0   15    0   19    0   19    0    0   19   17   15    0    0\n       subject_id\ngroup   1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098\n  click   19    0    0   18    0    0   18    0    0    0   19   15    0    0\n  touch    0   11   17    0   19   18    0   19   18   15    0    0   19   17\n       subject_id\ngroup   1099 1100 1101 1102 1103 1104 1105 1106 1107 1108\n  click   16    0   18    0    0   14   19    0   17    0\n  touch    0   19    0   18   17    0    0   19    0   17\n\n\nToggle code\n# individual subject_ids contributed data only to one group because it is a between-subject design\n# --> we need varying intercepts only, i.e. a different base-rate for subjects\n\nxtabs(~ group + exemplar, dolphin_correct)\n\n\n       exemplar\ngroup   alligator bat butterfly cat chameleon dog eel goldfish hawk horse lion\n  click        51  38        52  53        52  53  50       53   53    53   53\n  touch        53  46        54  54        54  53  53       55   52    53   54\n       exemplar\ngroup   penguin rabbit rattlesnake salmon sealion shark sparrow whale\n  click      48     53          38     53      48    46      53    42\n  touch      52     51          43     52      50    44      55    45\n\n\nToggle code\n# each exemplar contributes data to both groups\n# --> we can integrate varying intercepts and slopes for exemplars\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1d\n\n\n\n\n\nRun a multilevel model that predicts log_RT_s based on group and add maximal random effect structures licensed by the experimental design (ignore possible random intercept-slope interactions for now).\nSpecify weakly informative priors as you see fit.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\npriors <- c(\n  #priors for all fixed effects (group)\n  set_prior(\"student_t(3, 0, 3)\", class = \"b\"),\n  #prior for the Intercept\n  set_prior(\"student_t(3, 0, 3)\", class = \"Intercept\"),\n  #prior for all SDs including the varying intercepts and slopes\n  set_prior(\"student_t(3, 0, 3)\", class = \"sd\")\n)\n\nxmdl <- brm(log_RT_s ~ group + \n              (1 | subject_id) +\n              (group || exemplar),\n            prior = priors,\n            data = dolphin_correct)\nxmdl\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: log_RT_s ~ group + (1 | subject_id) + (group || exemplar) \n   Data: dolphin_correct (Number of observations: 1915) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nGroup-Level Effects: \n~exemplar (Number of levels: 19) \n               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)      0.45      0.09     0.32     0.65 1.00      769     1430\nsd(grouptouch)     0.11      0.05     0.01     0.23 1.00      879      805\n\n~subject_id (Number of levels: 108) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.57      0.04     0.49     0.66 1.01      676     1160\n\nPopulation-Level Effects: \n           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      0.28      0.14     0.02     0.55 1.01      420      855\ngrouptouch    -0.50      0.12    -0.74    -0.27 1.01      448     1096\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.68      0.01     0.66     0.71 1.00     5515     2789\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1e\n\n\n\n\n\nExtract the posterior means and 95% CrIs of touch vs. click log_RT_s and plot them.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\n# Extract the posteriors\nposteriors <- xmdl %>%\n  spread_draws(b_Intercept, \n               b_grouptouch) %>%\n  # calculate posteriors for each individual level\n  mutate(click = b_Intercept,\n         touch = b_Intercept + b_grouptouch) %>% \n  select(click, touch) %>% \n  gather(key = \"parameter\", value = \"posterior\") %>% \n  group_by(parameter) %>% \n  summarise(mean_posterior = mean(posterior),\n            `95lowerCrI` = HDInterval::hdi(posterior, credMass = 0.95)[1],\n            `95higherCrI` = HDInterval::hdi(posterior, credMass = 0.95)[2])\n\n# plot\nggplot(data = posteriors, \n       aes(x = parameter, y = mean_posterior,\n           color = parameter, fill = parameter)) + \n  geom_errorbar(aes(ymin = `95lowerCrI`, ymax = `95higherCrI`),\n                width = 0.2, color = \"grey\") +\n  geom_line(aes(group = 1), color = \"black\") +\n  geom_point(size = 4) +\n  labs(x = \"group\",\n       y = \"posterior log(RT) (scaled)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1f\n\n\n\n\n\nAdd the posterior estimates for different exemplars to the plot. (Hint: Check code from the previous “tutorial” to extract the random effect estimates.)\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\n# extract the random intercepts for exemplars\nrandom_intc_matrix <- ranef(xmdl)$exemplar[, , \"Intercept\"] %>% \n  round(digits = 2) \n\n# extract the by-exemplar random slopes for group\nrandom_slope_matrix <- ranef(xmdl)$exemplar[, , \"grouptouch\"] %>% \n  round(digits = 2)\n\n# random intercepts to dataframe\nrandom_intc_df <- data.frame(exemplar = row.names(random_intc_matrix), random_intc_matrix) %>% \n  select(exemplar, Estimate) %>% \n  rename(rintercept = Estimate)\n\n# combine with random slope matrix\nrandom_slope_df <- data.frame(exemplar = row.names(random_slope_matrix), random_slope_matrix) %>% \n  select(exemplar, Estimate) %>% \n  rename(rslope = Estimate) %>% \n  full_join(random_intc_df) %>% \n  # add population parameters and group-specific parameters\n  mutate(click_population = fixef(xmdl)[1],\n         touch_population = fixef(xmdl)[1] + fixef(xmdl)[2],\n         click = rintercept + click_population,\n         touch = rintercept + rslope + touch_population) %>% \n  select(exemplar, touch, click) %>% \n  gather(parameter, mean_posterior, -exemplar)\n  \n\n# combine with plot\nggplot(data = posteriors, \n       aes(x = parameter, y = mean_posterior,\n           color = parameter, fill = parameter)) + \n   # add random estimates\n  geom_point(data = random_slope_df, \n             alpha = 0.4,\n             size = 2,\n             position = position_jitter(width = 0.01)\n             ) +\n  # add lines between random estimates\n  geom_line(data = random_slope_df, \n            aes(group = exemplar),\n            color = \"grey\", alpha = 0.3) +\n  # add population-level estimates\n  geom_errorbar(aes(ymin = `95lowerCrI`, ymax = `95higherCrI`),\n                width = 0.2, color = \"grey\") +\n  geom_line(aes(group = 1), size = 2, color = \"black\") +\n  geom_point(size = 4, pch = 21, color = \"black\") +\n  labs(x = \"group\",\n       y = \"posterior log(RT) (scaled)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2: Poisson regression\n\n\n\n\n\n\nExercise 2a\n\n\n\n\n\nRun a multilevel poisson regression predicting xpos_flips based on group, log_RT_s, and their two-way interaction. Specify maximal random effect structures for exemplars and subject_ids licensed by the design (ignore correlations between intercepts and slopes for now). (Hint: allow groupings to differ regarding the interaction effect if licensed by the design.) Specify weakly informative priors.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\npriors <- c(\n  #priors for all fixed effects\n  set_prior(\"student_t(3, 0, 3)\", class = \"b\"),\n  #prior for all SDs including the varying intercepts and slopes for both groupings\n  set_prior(\"student_t(3, 0, 3)\", class = \"sd\")\n)\n\npoisson_mdl <- brm(xpos_flips ~ group * log_RT_s +\n                     (log_RT_s || subject_id) +\n                     (group * log_RT_s || exemplar),\n                   data = dolphin_correct,\n                   prior = priors,\n                   family = \"poisson\")\n\npoisson_mdl\n\n\n Family: poisson \n  Links: mu = log \nFormula: xpos_flips ~ group * log_RT_s + (log_RT_s || subject_id) + (group * log_RT_s || exemplar) \n   Data: dolphin_correct (Number of observations: 1915) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nGroup-Level Effects: \n~exemplar (Number of levels: 19) \n                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(Intercept)               0.06      0.04     0.00     0.15 1.00     1591\nsd(grouptouch)              0.10      0.07     0.00     0.26 1.00     1267\nsd(log_RT_s)                0.04      0.03     0.00     0.13 1.00     1777\nsd(grouptouch:log_RT_s)     0.09      0.06     0.00     0.23 1.00     1523\n                        Tail_ESS\nsd(Intercept)               2289\nsd(grouptouch)              1845\nsd(log_RT_s)                2425\nsd(grouptouch:log_RT_s)     2165\n\n~subject_id (Number of levels: 108) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.59      0.06     0.49     0.71 1.00     1480     2496\nsd(log_RT_s)      0.13      0.05     0.02     0.23 1.01      791     1097\n\nPopulation-Level Effects: \n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept              -0.15      0.09    -0.33     0.03 1.01     1386     2284\ngrouptouch             -0.46      0.13    -0.72    -0.20 1.00     1722     2110\nlog_RT_s                0.39      0.05     0.30     0.49 1.00     3398     2901\ngrouptouch:log_RT_s     0.17      0.07     0.03     0.32 1.00     3597     2638\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2b\n\n\n\n\n\nExtract and plot the population level estimates for both click and touch group as a regression line into a scatter plot (x = b_log_RT_s, y = xpos_flips).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\n# extract posterior means for model coefficients\npredicted_Poisson_values <- poisson_mdl %>%\n  spread_draws(b_Intercept, b_log_RT_s, \n               b_grouptouch, `b_grouptouch:log_RT_s`\n               ) %>%\n  # make a list of relevant value range of logRT\n  mutate(log_RT = list(seq(-5, 10, 0.5))) %>% \n  unnest(log_RT) %>%\n  mutate(click = exp(b_Intercept + b_log_RT_s*log_RT),\n         touch = exp(b_Intercept + b_log_RT_s*log_RT +\n                            b_grouptouch + `b_grouptouch:log_RT_s`*log_RT)) %>%\n  select(log_RT, click, touch) %>% \n  gather(group, posterior, -log_RT) %>% \n  group_by(log_RT, group) %>%\n  summarise(pred_m = mean(posterior, na.rm = TRUE),\n            pred_low = quantile(posterior, prob = 0.025),\n            pred_high = quantile(posterior, prob = 0.975)\n            ) \n\n# plot population level\nggplot(data = predicted_Poisson_values, aes(x = log_RT, y = pred_m)) +\n  geom_point(data = dolphin_correct, aes(x = log_RT_s, y = xpos_flips, color = group), \n             position = position_jitter(height = 0.2), alpha = 0.2) +\n  geom_line(aes(y = pred_m, color = group), size = 2) +\n  facet_grid(~group) +\n  ylab(\"Predicted prob of xflips\") +\n  ylim(-1,10) +\n  xlim(-5,10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2c\n\n\n\n\n\nExtract the respective subject-specific estimates from the model and plot them into the same plot (use thinner lines).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\n# extract the random effects for subject_id\n\n# intercepts\nrandom_intc_matrix <- ranef(poisson_mdl)$subject_id[, , \"Intercept\"] %>% \n  round(digits = 3)\n\n# slopes\nrandom_slope_matrix <- ranef(poisson_mdl)$subject_id[, , \"log_RT_s\"] %>% \n  round(digits = 3)\n\n# to df\nrandom_intc_df <- data.frame(subject_id = row.names(random_intc_matrix), random_intc_matrix) %>% \n  select(subject_id, Estimate) %>% \n  rename(rintercept = Estimate)\n\n# wrangle into one df \nrandom_slope_df <- data.frame(subject_id = row.names(random_slope_matrix), random_slope_matrix) %>% \n  select(subject_id, Estimate) %>% \n  rename(rslope = Estimate) %>% \n  full_join(random_intc_df) %>% \n  expand_grid(group = c(\"click\", \"touch\")) %>% \n  # add population parameters and group-specific parameters\n  mutate(adjusted_int = ifelse(group == \"click\",\n           rintercept + fixef(poisson_mdl)[1],\n           rintercept + fixef(poisson_mdl)[1] + fixef(poisson_mdl)[2]),\n         adjusted_slope = ifelse(group == \"click\",\n           rslope + fixef(poisson_mdl)[3],\n           rslope + fixef(poisson_mdl)[3] + fixef(poisson_mdl)[4])) %>% \n  mutate(log_RT = list(seq(-5, 10, 0.5))) %>% \n  unnest(log_RT) %>%\n  select(subject_id, log_RT, group, \n         adjusted_int, adjusted_slope) %>% \n  group_by(subject_id, log_RT, group) %>%\n  mutate(pred_m = exp(adjusted_int + adjusted_slope*log_RT))\n\n# plot the individual regression lines on top of the population estimate\nggplot(data = predicted_Poisson_values, aes(x = log_RT, y = pred_m)) +\n  geom_point(data = dolphin_correct, aes(x = log_RT_s, y = xpos_flips), \n             position = position_jitter(height = 0.2), alpha = 0.01) +\n  geom_line(aes(y = pred_m, color = group), size = 2) +\n  geom_line(data = random_slope_df, \n            aes(x = log_RT, y = pred_m, group = subject_id, color = group),\n            size = 0.5, alpha = 0.2) +\n  facet_grid(~group) +\n  ylab(\"Predicted prob of xflips\") +\n  ylim(-1,10) +\n  xlim(-5,10)"
  },
  {
    "objectID": "practice-sheets/02c-multi-membership.html",
    "href": "practice-sheets/02c-multi-membership.html",
    "title": "Multi-membership models",
    "section": "",
    "text": "Multi-membership models are useful in cases where group-level effects are plausible but elements do not necessarily belong just to a single group. For example, if we want to include group-level effects for the native language (of participants in an experiment), some individuals may have more than one. Individuals with more than one native language would belong to multiple groups, whence the term multi-membership models. Moreover, there are cases where membership in a group is a matter of degree: if a grouping of individuals is by “country of residence”, for example, some people might spend variable amounts of time in different countries, and our model may want to account for that.\n\nPreamble\nHere is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\n\n\nToggle code\n# install packages from CRAN (unless installed)\npckgs_needed <- c(\n  \"tidyverse\",\n  \"brms\",\n  \"rstan\",\n  \"rstanarm\",\n  \"remotes\",\n  \"tidybayes\",\n  \"bridgesampling\",\n  \"shinystan\",\n  \"mgcv\"\n)\npckgs_installed <- installed.packages()[,\"Package\"]\npckgs_2_install <- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]\nif(length(pckgs_2_install)) {\n  install.packages(pckgs_2_install)\n} \n\n# install additional packages from GitHub (unless installed)\nif (! \"aida\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/aida-package\")\n}\nif (! \"faintr\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/faintr\")\n}\nif (! \"cspplot\" %in% pckgs_installed) {\n  remotes::install_github(\"CogSciPrag/cspplot\")\n}\n\n# load the required packages\nx <- lapply(pckgs_needed, library, character.only = TRUE)\nlibrary(aida)\nlibrary(faintr)\nlibrary(cspplot)\n\n# these options help Stan run faster\noptions(mc.cores = parallel::detectCores())\n\n# use the CSP-theme for plotting\ntheme_set(theme_csp())\n\n# global color scheme from CSP\nproject_colors = cspplot::list_colors() |> pull(hex)\n# names(project_colors) <- cspplot::list_colors() |> pull(name)\n\n# setting theme colors globally\nscale_colour_discrete <- function(...) {\n  scale_colour_manual(..., values = project_colors)\n}\nscale_fill_discrete <- function(...) {\n   scale_fill_manual(..., values = project_colors)\n}\n\n\n\n\nMulti-membership group-level effects\nTo understand multi-membership models, consider first a normal (non-multi-membership) group-level modeling approach. Say that each observation \\(i\\) belongs to exactly one of \\(k\\) categories \\(c(i) \\in \\{1, \\dots, k\\}\\). A group-level intercept consists of a vector of additive offsets \\(\\vec{u} = \\langle u_{1}, \\dots, u_{k} \\rangle\\), one adjustment \\(u_{i}\\) for each category \\(i\\). The linear predictor for observation \\(i\\) is computed by adding the appropriate group-level offset to the population-level intercept, like so:\n\\[\n\\eta_i = \\beta_0 + u_{c(i)} + \\dots\n\\]\nInstead of an indexing function \\(c(i)\\), which returns the category index for each observation \\(i\\), this can also be written using a one-hot vector encoding. Let, \\(\\vec{\\delta}_i\\) be a vector of length \\(k\\) which contains only zeros, except in position \\(c(i)\\), where it contains a 1. Then we can equivalently write the equation for the linear predictor term as:\n\\[\n\\eta_i = \\beta_0 + \\vec{u} \\cdot \\vec{\\delta}_i + \\dots\n\\]\nFor example, if there are only four categories, and if observation \\(i\\) belongs to the third category (\\(c(i)=3\\)), we have \\(\\delta_i = \\langle 0,0,1,0 \\rangle\\), so that the dot product \\(\\vec{u} \\cdot \\vec{\\delta}_i\\) will just return the value of \\(\\vec{u}\\) at position 3.\nFrom here, it’s just one step further to a multi-membership model. Instead of a one-hot vector encoding for group-membership, consider a vector of (normalized) weights \\(\\vec{w}_i\\) given the relative degree to which observation \\(i\\) belongs to each category. The linear predictor is then:\n\\[\n\\eta_i = \\beta_0 + \\vec{u} \\cdot \\vec{w}_i + \\dots\n\\]\nand that’s all to it. Similar considerations apply to random slopes.\nImportantly, the weights \\(\\vec{w}_i\\) are given (from observation). (I am not aware that brms allows to estimate the weights as well. If that would be a requirement, directly coding the model in Stan might be necessary.)\n\n\nSimulated data set of multi-membership effects\nTo explore how a multi-membership model can recover the true effects of weighted membership, we use a simulated data set based on a vanilla linear regression (with one predictor term). The true population-level parameters are:\n\n\nToggle code\n# true parameters (population-level)\npop_Intercept = 3\npop_Slope     = 2\npop_SD        = 1\n\n\nFor group-level effects, assume that there are five groups. Each has a true additive offset (random intercept) as follows:\n\n\nToggle code\n# true parameters (group-level):\n# - there are five groups\n# - random intercepts for each group\n#   -> sequence -0.8 -0.4  0.0  0.4  0.8\ngroup_Intercepts = seq(from = -0.8, to = 0.8, length.out = 5)\n\n\nHere are samples from a model, in which each individual observation (each row) can belong to variable degree to one of two groups.\n\n\nToggle code\nn_samples = 2000\ndata_multiMember <- \n  tibble(\n    x     = rnorm(n_samples), \n    g1    = sample(1:5, n_samples, TRUE), \n    g2    = sample(1:5, n_samples, TRUE),\n    y_pop = rnorm(n_samples, mean = pop_Slope * x + pop_Intercept),\n    w1    = rbeta(n_samples, 1, 1),\n    w2    = 1 - w1,\n    RE    = group_Intercepts[g1] * w1 + group_Intercepts[g2] * w2,\n    y     = y_pop + RE\n  )\n\n\nThis plot shows the simulated data, where the points in blue show the actual observations, and the points in yellow show the data before applying the group-level effects.\n\n\nToggle code\n# unperturbed data in yellow, observed data in blue \ndata_multiMember |> \n  ggplot(aes(x,y)) + \n  geom_point(aes(x = x ,y = y_pop), color = project_colors[3], alpha = 0.5) +\n  geom_point(color = project_colors[1], alpha = 0.5)\n\n\n\n\n\nTo run a multi-membership model in brms, there is special syntax for the group-level effects. Writing + (1 | mm(g1, g2)) indicates random intercepts for multi-membership as indicated by the vectors g1 and g2. Without further information, this is interpreted as belonging equally to each group (weights 0.5 for each, if we have two possible group memberships). If membership is weighted, as indicated in our data set in columns w1 and w2, this can be expressed as: + ( 1 | mm(g1, g2, weights = cbind(w1, w2)). (NB: the weights argument expects a matrix, so that we need to include cbind() here.)\n\n\nToggle code\n# multi-membership model with two members per group and equal weights\nfit_mm <- \n  brms::brm(\n    formula = y ~ x + ( 1 | mm(g1, g2, weights = cbind(w1, w2))), \n    data    = data_multiMember,\n    control = list(adapt_delta = 0.99)\n    )\n\n\nDid the model recover the true group-level intercepts? It did, check it:\n\n\nToggle code\ntidybayes::summarise_draws(fit_mm) |> \n  filter(grepl(\"r_\",variable)) |> \n  select(variable, q5, mean, q95)\n\n\n# A tibble: 5 × 4\n  variable                   q5    mean    q95\n  <chr>                   <num>   <num>  <num>\n1 r_mmg1g2[1,Intercept] -1.55   -0.852  -0.200\n2 r_mmg1g2[2,Intercept] -0.997  -0.309   0.346\n3 r_mmg1g2[3,Intercept] -0.635   0.0465  0.701\n4 r_mmg1g2[4,Intercept] -0.317   0.365   1.02 \n5 r_mmg1g2[5,Intercept]  0.0239  0.709   1.37 \n\n\n\n\nExercises\n\n\n\n\n\n\nExercise 1a: Biased group membership\n\n\n\n\n\nConstruct a new data set (using the previous code), in which there is a (mild) bias in group membership. Concretely, add bias to the sampling of variables g1 and g2, making it more likely for g1 to be a lower rather than higher indexed group, and reversely for g2. Also, add a bias in the weights towards g1 by changing the shape parameters of the Beta distribution. This should induce an overall bias towards random intercepts that lower the linear predictor.\nPlot the new data using the previous plotting code.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\nn_samples = 2000\ndata_multiMember <- \n  tibble(\n    x     = rnorm(n_samples), \n    g1    = sample(1:5, n_samples, TRUE, prob = 5:1), \n    g2    = sample(1:5, n_samples, TRUE, prob = 1:5),\n    y_pop = rnorm(n_samples, mean = pop_Slope * x + pop_Intercept),\n    w1    = rbeta(n_samples, 4, 1),\n    w2    = 1 - w1,\n    RE    = group_Intercepts[g1] * w1 + group_Intercepts[g2] * w2,\n    y     = y_pop + RE\n  )\n\n\n\n\nToggle code\n# unperturbed data in yellow, observed data in blue \ndata_multiMember |> \n  ggplot(aes(x,y)) + \n  geom_point(aes(x = x ,y = y_pop), color = project_colors[3], alpha = 0.5) +\n  geom_point(color = project_colors[1], alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1b: Check parameter recovery\n\n\n\n\n\nFit a multi-membership model to the new data and check if group-level parameters are recoverable still. How do you interpret the results: did the model recover the true parameters or not?\n[If you want: Think about what influences recoverability.]\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\n# multi-membership model with two members per group and equal weights\nfit_mm <- \n  brms::brm(\n    formula = y ~ x + ( 1 | mm(g1, g2, weights = cbind(w1, w2)) ), \n    data    = data_multiMember,\n    control = list(adapt_delta = 0.99)\n    )\ntidybayes::summarise_draws(fit_mm) |> \n  filter(grepl(\"r_\",variable)) |> \n  select(variable, q5, mean, q95)\n\n\n# A tibble: 5 × 4\n  variable                    q5     mean    q95\n  <chr>                    <num>    <num>  <num>\n1 r_mmg1g2[1,Intercept] -1.56    -0.809   -0.123\n2 r_mmg1g2[2,Intercept] -1.22    -0.468    0.217\n3 r_mmg1g2[3,Intercept] -0.752    0.00685  0.698\n4 r_mmg1g2[4,Intercept] -0.362    0.385    1.08 \n5 r_mmg1g2[5,Intercept]  0.00466  0.749    1.46 \n\n\nThe recovery is not as immaculate as without the bias, but it is still okay.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1c: Compare multi-membership model to vanilla\n\n\n\n\n\nRun a vanilla regression model for the biased data, i.e., without group-level effects. Compare the models based on a simple posterior predictive check with pp_check(). What do you conclude from this?\nCompare the models based on their posterior predicitive adequacy, e.g., using LOO model comparison (if you can; maybe you have not learned about this yet). Interpret the results.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\nfit_vanilla <- \n  brms::brm(\n    formula = y ~ x, \n    data    = data_multiMember\n  )\n\npp_check(fit_mm)\n\n\n\n\n\nToggle code\npp_check(fit_vanilla)\n\n\n\n\n\nToggle code\nloo_compare(loo(fit_mm), loo(fit_vanilla))\n\n\n            elpd_diff se_diff\nfit_mm         0.0       0.0 \nfit_vanilla -173.1      17.2 \n\n\nThe visual PPC does not distinguish between these models (because it is sensitive only to the overal distribution of the response variable).\nThe LOO-based model comparison is more informative: The vanilla model appears to be substantially worse.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\n\nCan you think of an example from your own line of work where multi-membership group-level effects might be plausible, possibly even mandatory?"
  },
  {
    "objectID": "practice-sheets/01e-contrast-coding-tutorial.html",
    "href": "practice-sheets/01e-contrast-coding-tutorial.html",
    "title": "A tutorial on contrast coding for (Bayesian) regression",
    "section": "",
    "text": "Generalized (non-)linear mixed effect models are a powerful statistical tool that gains increasing popularity for data analysis in cognitive science and many other disciplines. This tutorial will provide an overview of different categorical variable coding schemes commonly used in regression modeling. In particular, it covers:\nWe will look at two example data sets from factorial-design experiments with categorical predictors and a continuous dependent variable which we will analyze using a Bayesian approach. We also show how the faintr package allows extraction of (samples of estimates for) cell means irrespective of encoding scheme.\nEstimated reading time: 1.5h"
  },
  {
    "objectID": "practice-sheets/01e-contrast-coding-tutorial.html#dataset",
    "href": "practice-sheets/01e-contrast-coding-tutorial.html#dataset",
    "title": "A tutorial on contrast coding for (Bayesian) regression",
    "section": "Dataset",
    "text": "Dataset\nThe first part of this tutorial is based on a data set from an experiment by Winter and Grawunder (2012) You can get the dataset by running:\n\n\nToggle code\npoliteness_df <- faintr::politeness\n\n# get a look at the data set\nhead(politeness_df)\n\n\n# A tibble: 6 × 5\n  subject gender sentence context pitch\n  <chr>   <chr>  <chr>    <chr>   <dbl>\n1 F1      F      S1       pol      213.\n2 F1      F      S1       inf      204.\n3 F1      F      S2       pol      285.\n4 F1      F      S2       inf      260.\n5 F1      F      S3       pol      204.\n6 F1      F      S3       inf      287.\n\n\nThe data contains records of the voice pitch of speakers in different social contexts (polite and informal). They investigated whether the mean voice pitch differs across the factor gender of the speakers (F and M) and across the factor contexts - resulting in four different condition combinations (gender X context). Such a design is called factorial design and the single combinations are called design cells."
  },
  {
    "objectID": "practice-sheets/01e-contrast-coding-tutorial.html#explore-data-visually",
    "href": "practice-sheets/01e-contrast-coding-tutorial.html#explore-data-visually",
    "title": "A tutorial on contrast coding for (Bayesian) regression",
    "section": "Explore Data visually",
    "text": "Explore Data visually\nBefore we dive into any statistical analyses of our dataset it is helpful to get a rough idea of what the data looks like. For example, we can start by exploring the dataset visually.\n\n\n\n\n\nFurthermore, we can compute some basic statistics - e.g. the mean of the different design cells, before we turn to more complex linear models. We can also compute the overall mean pitch across all the conditions - the grand mean. These values will be helpful for a sanity check when interpreting the linear models later on.\n\n\nToggle code\ntibble_means <- politeness_df %>%\n  group_by(context, gender) %>%\n  summarize(mean = mean(pitch))\nhead(tibble_means)\n\n\n# A tibble: 4 × 3\n# Groups:   context [2]\n  context gender  mean\n  <chr>   <chr>  <dbl>\n1 inf     F       261.\n2 inf     M       144.\n3 pol     F       233.\n4 pol     M       133.\n\n\nToggle code\nmean(tibble_means$mean)\n\n\n[1] 192.8605"
  },
  {
    "objectID": "practice-sheets/01e-contrast-coding-tutorial.html#dummy-treatment-coding",
    "href": "practice-sheets/01e-contrast-coding-tutorial.html#dummy-treatment-coding",
    "title": "A tutorial on contrast coding for (Bayesian) regression",
    "section": "Dummy (Treatment) Coding",
    "text": "Dummy (Treatment) Coding\nDummy coding, or treatment coding, is the default coding scheme used by R. Understanding the name ‘treatment coding’ helps understanding what this coding scheme does: imagine a medical experiment with a single control group (who obtain a placebo) and different experimental groups each of which gets a different treatment (e.g., different drugs), and where we want to compare each treatment group to the single, pivotal control group. Consequently, dummy coded variables are estimated by comparing all levels of the variable to a reference level. The intercept of a linear model containing dummy-coded variables is the mean of the reference level.\nOur variables only have two levels, so the effect of gender could be estimated by treating female as the reference level and estimating the effect of being male compared to the reference level – so basically estimating the difference in pitch it takes to “get from female to male”. Similarly, we can estimate the effect of context: the informal context can be treated as the reference level and the effect of politeness can be estimated against it. By default, the first level of a factor is treated as the reference level (for unordered factors that is the first string in alphanumeric order) - but principally, there is no difference as to which level should be used as the reference level. It makes sense to choose the level which is in some sense the ‘control’ in your experimental design.\nBecause R uses dummy-coding by default, we can look at the default numerical coding right away. The function contrasts() displays the contrast matrix for the respective variable:\n\n\nToggle code\ncontrasts(politeness_df$gender)\n\n\n  M\nF 0\nM 1\n\n\nToggle code\ncontrasts(politeness_df$context)\n\n\n    pol\ninf   0\npol   1\n\n\nBut if we wish to explicitly assign a dummy (treatment) coding to a variable, we may do so by a built-in R function.\n\n\nToggle code\ncontrasts(politeness_df$gender) <- contr.treatment(n=2) # insert the number of levels here\ncolnames(contrasts(politeness_df$gender)) <- \"M\" # manually declare the contrast level names\n\n\nSo both variables \\(x_1\\) and \\(x_2\\) can take either the value 0 or 1 (because we dummy-code both categorical variables; see below for more). We already defined the referenc levels of the single variables, now we can define the overall reference level of our model (by combining the two individual reference levels) – it is the mean pitch of female speakers in informal contexts.\nHaving set all the basics, we can now turn to computing a linear model of the mean pitch as predicted by the factors gender and context:\n\n\nToggle code\n# here, we only use fixed effects\nfit_dummy_FE <- brm(\n  pitch ~ gender * context,\n  data = politeness_df,\n  cores = 4,\n  iter = 1000\n)\n\n\n\n\nToggle code\nfit_dummy_FE.coefs <- fixef(fit_dummy_FE)[,1] %>% as.numeric() # get the estimated coefficients\nsummary(fit_dummy_FE)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: pitch ~ gender * context \n   Data: politeness_df (Number of observations: 83) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept            260.56      7.87   244.15   275.21 1.00     1205     1470\ngenderM             -116.16     11.01  -137.31   -94.05 1.00     1004     1309\ncontextpol           -27.23     11.10   -48.38    -5.23 1.00     1011     1540\ngenderM:contextpol    15.77     16.05   -16.54    46.24 1.00      910     1156\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    36.19      2.83    31.24    42.18 1.00     1725     1681\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNow how do we interpret the estimated coefficients?\nLet us recall the regression equation that is hidden behind this output: \\[y = \\beta_0 + \\beta_1*x_1 + \\beta_2*x_2 + \\beta_3*x_1x_2\\]\nIn order to help us interpret the output, R assigns string names to the estimated coefficients using the names we used in the generic formula. The (Intercept) corresponds to \\(\\beta_0\\), genderM corresponds to \\(\\beta_1\\), contextpol corresponds to \\(\\beta_2\\) and genderM:contextpol (the interaction term) to \\(\\beta_3\\).\nFurther, let us recall the numerical coding of our variables: for \\(x_1\\) (gender) a 0 means female, a 1 means male; for \\(x_2\\) (context) a 0 means informal, a 1 means polite. So the computed values are the estimates for conditions differing from the respective reference conditions - i.e. when the respective \\(x\\) is a 1.\nTo get an estimate of a certain design cell (\\(y_i\\)) - let’s start with the mean pitch of female speakers (0 for \\(x_1\\)) in informal contexts (0 for \\(x_2\\)) - we just insert the corresponding numeric values for the corresponding \\(x\\) and the estimated value for the corresponding \\(\\beta\\). Thus we get:\n\n\nToggle code\ny1 = fit_dummy_FE.coefs[1] + fit_dummy_FE.coefs[2]*0 +\n  fit_dummy_FE.coefs[3]*0 + fit_dummy_FE.coefs[4]*(0)\ny1\n\n\n[1] 260.5618\n\n\nHence, the mean pitch of female speakers in informal context corresponds to the intercept. As a sanity check, we can recall that for dummy coded variables the model intercept is just the mean of the reference cell (in our case, female speakers in informal contexts!).\nLet’s now calculate the mean pitch of male speakers (1 for \\(x_1\\)) in informal contexts (0 for \\(x_2\\)):\n\n\nToggle code\ny2 = fit_dummy_FE.coefs[1] + fit_dummy_FE.coefs[2]*1 +\n  fit_dummy_FE.coefs[3]*0 + fit_dummy_FE.coefs[4]*(1*0)\ny2\n\n\n[1] 144.4057\n\n\n\nQuantifying uncertainty\nThe previous estimate of the cell mean used the mean estimate for model paramters. This way, we get no information about the uncertainty in this estimate. But we can do the same kind of calculations we did before for the means of the estimates, but for each of the samples from the posterior, thus obtaining samples from a derived variable, for which we can quantify the uncertainty as usual.\n\n\nToggle code\ncell_means_BayesStats <- fit_dummy_FE |> \n  tidybayes::tidy_draws() |> \n  select(starts_with(\"b_\")) |> \n  mutate(\n    female_informal = b_Intercept,\n    female_polite   = b_Intercept + b_contextpol,\n    male_informal   = b_Intercept + b_genderM,\n    male_polite     = b_Intercept + b_contextpol + \n                        b_genderM + `b_genderM:contextpol`\n  ) |> \n  select(5:8) |> \n  pivot_longer(cols = everything()) |>\n  group_by(name) |> \n  reframe(aida::summarize_sample_vector(value)[-1])\ncell_means_BayesStats\n\n\n# A tibble: 4 × 4\n  name            `|95%`  mean `95%|`\n  <chr>            <dbl> <dbl>  <dbl>\n1 female_informal   244.  261.   275.\n2 female_polite     219.  233.   248.\n3 male_informal     129.  144.   159.\n4 male_polite       117.  133.   149.\n\n\nThat’s a lot of manual labor. Thankfully, there is a package to make this simpler.\n\n\nExtracting cell samples with faintr\nThe faintr package allows to extract samples from different levels of categorical predictors, like so:\n\n\nToggle code\n# sample for male speakers in polite contexts\nfaintr::extract_cell_draws(\n  fit = fit_dummy_FE,\n  group = gender == \"M\" & context == \"pol\"\n) |> pull(draws) |> \n  aida::summarize_sample_vector(name = \"male_polite\")\n\n\n# A tibble: 1 × 4\n  Parameter   `|95%`  mean `95%|`\n  <chr>        <dbl> <dbl>  <dbl>\n1 male_polite   117.  133.   149.\n\n\nThe faintr package also allows to compare cells (e.g., “diagonally”):\n\n\nToggle code\nfaintr::compare_groups(\n  fit = fit_dummy_FE,\n  higher  = gender == \"F\" & context == \"inf\",\n  lower   = gender == \"M\" & context == \"pol\"\n)\n\n\nOutcome of comparing groups: \n * higher:  gender == \"F\" & context == \"inf\" \n * lower:   gender == \"M\" & context == \"pol\" \nMean 'higher - lower':  127.6 \n95% HDI:  [ 104.9 ; 149.6 ]\nP('higher - lower' > 0):  1 \nPosterior odds:  Inf \n\n\nThis compares the estimated mean pitch for male speakers in polite contexts against those of female speakers in informal contexts. Clearly, the hypothesis that polite male speakers have lower pitch than informal female speakers is supported by this data and analysis."
  },
  {
    "objectID": "practice-sheets/01e-contrast-coding-tutorial.html#simple-contrast-coding",
    "href": "practice-sheets/01e-contrast-coding-tutorial.html#simple-contrast-coding",
    "title": "A tutorial on contrast coding for (Bayesian) regression",
    "section": "Simple (Contrast) Coding",
    "text": "Simple (Contrast) Coding\nAnother common coding scheme is the simple coding (also called contrast coding). Simple coded variables are also compared to a reference level (just like dummy-coded ones). However, the intercept of a simple coded model is the grand mean – the mean of all cells (i.e. the mean of female-informal & female-polite & male-informal & male-polite cells).\nGenerally, this kind of coding can be created by subtracting \\(1/k\\) from the dummy coding contrast matrix, where \\(k\\) is the number of levels a variable has (in our case, both have two). Hence, the reference level will always only have negative values in the contrast matrix. The general rule is that the contrasts within a column have to add up to 0. R does not provide a built-in function for simple coding, but we can easily create the respective matrix ourselves by subtracting \\(1/k\\) (i.e. 1/2) from the dummy coding matrix:\n\n\nToggle code\n# manual creation of contrasts\ncontr.matrix <- matrix( rep(0.5, 2))\ndummy.matrix <- contr.treatment(2)\ncontr.coding <- dummy.matrix - contr.matrix\n\n# we should duplicate the values to not overwrite previous contrasts\npoliteness_df <- politeness_df %>%\n  mutate(context_contr = context,\n         gender_contr = gender)\ncontrasts(politeness_df$context_contr) <- contr.coding\ncontrasts(politeness_df$gender_contr)  <- contr.coding\n\n\nHence now the gender is coded as -0.5 for female and 0.5 for male; context is coded as -0.5 for informal and 0.5 for polite.\nLet’s again look at our regression model:\n\n\nToggle code\nlm.contr.FE <- brm(\n  pitch ~ gender_contr * context_contr,\n  data = politeness_df,\n  cores = 4,\n  iter =  1000\n)\n\n\n\n\nToggle code\nlm.contr.FE.coefs <- fixef(lm.contr.FE)[,1] %>% as.numeric() # get vector of estimated coefficients\nsummary(lm.contr.FE)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: pitch ~ gender_contr * context_contr \n   Data: politeness_df (Number of observations: 83) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                      192.87      4.08   184.72   201.02 1.00     2418\ngender_contr2                 -108.09      7.83  -123.44   -92.95 1.00     2538\ncontext_contr2                 -19.49      7.68   -33.85    -4.46 1.00     2400\ngender_contr2:context_contr2    16.15     16.63   -17.90    47.32 1.01     2682\n                             Tail_ESS\nIntercept                        1650\ngender_contr2                    1610\ncontext_contr2                   1426\ngender_contr2:context_contr2     1620\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    36.21      2.94    30.81    42.30 1.01     2088     1365\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nIn order to compute the mean pitch of a specific cell, we proceed just as with dummy-coded variables and insert the respective estimates and values for \\(x\\). Let us start with female speakers (\\(x_1\\) is -0.5) in informal contexts (\\(x_2\\) is -0.5):\n\n\nToggle code\ny1 = lm.contr.FE.coefs[1] + lm.contr.FE.coefs[2]*(-0.5) + lm.contr.FE.coefs[3]*(-0.5) + lm.contr.FE.coefs[4]*(-0.5)*(-0.5)\ny1\n\n\n[1] 260.6951\n\n\nWe get the same result as before (as we should - the estimates should not depend on a coding scheme but only on the data). As a sanity check, we can again look at the intercept – it matches the grand mean we computed in the beginning of this tutorial – as it should."
  },
  {
    "objectID": "practice-sheets/01e-contrast-coding-tutorial.html#deviation-sum-coding",
    "href": "practice-sheets/01e-contrast-coding-tutorial.html#deviation-sum-coding",
    "title": "A tutorial on contrast coding for (Bayesian) regression",
    "section": "Deviation (Sum) Coding",
    "text": "Deviation (Sum) Coding\nDeviation coding (also called sum coding) is the most popular coding scheme and is often considered the best choice to get a clear picture of presence (or absence) of an effect and a clear random effects interpretation.\nIt is slightly different from the previous schemes. It compares the mean of the predicted variable for a specific condition to the grand mean. So the estimates do not tell you the difference between the reference level and another level anymore. The intercept of linear models with sum coded variables is the grand mean.\nR has a built-in function for creating sum coded variables:\n\n\nToggle code\n# again create a new variable\npoliteness_df %>%\n  mutate(context_dev = context,\n         gender_dev = gender) -> politeness_df\ncontrasts(politeness_df$context_dev) <- contr.sum(2) # insert number of levels\ncontrasts(politeness_df$gender_dev)  <- contr.sum(2)\n\n\nNow the gender is coded as s 1 for female and -1 for male; context is coded as 1 for informal and -1 for polite.\nBelow we fit a model with the sum-coded variables:\n\n\nToggle code\nlm.dev.FE <- brm(pitch ~ context_dev * gender_dev,\n                data = politeness_df,\n                cores = 4,\n                iter = 1000)\n\n\n\n\nToggle code\nlm.dev.FE.coefs <- fixef(lm.dev.FE)[,1] %>% as.numeric()\nsummary(lm.dev.FE)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: pitch ~ context_dev * gender_dev \n   Data: politeness_df (Number of observations: 83) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                         Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                  192.88      4.04   185.14   200.46 1.00     2797\ncontext_dev1                 9.88      3.96     2.35    17.72 1.00     2482\ngender_dev1                 54.11      4.02    46.50    61.91 1.00     2490\ncontext_dev1:gender_dev1     3.92      3.91    -3.63    11.58 1.00     2721\n                         Tail_ESS\nIntercept                    1493\ncontext_dev1                 1549\ngender_dev1                  1470\ncontext_dev1:gender_dev1     1684\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    36.28      2.92    31.09    42.61 1.00     2053     1589\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe coefficients denote now the difference between the grand mean (i.e. intercept) and the mean of the respective condition.\nWe apply the same idea to estimate the pitch means for specific cases: E.g. for female speakers in informal contexts we do:\n\n\nToggle code\ny1 = lm.dev.FE.coefs[1] + lm.dev.FE.coefs[2]*1 + lm.dev.FE.coefs[3]*1 + lm.dev.FE.coefs[4]*1*1\ny1\n\n\n[1] 260.7897\n\n\nSince the intercept is now the grand mean and not a specific reference level, let us think about the interpretation of the single estimates. The estimate of e.g. the context effect now denotes the value by which the mean pitch in informal (estimate * 1, remember our coding!) or polite contexts (estimate * -1) differs from the grand mean. So if we wish to calculate the mean pitch in polite contexts (across genders), we would do:\n\n\nToggle code\nyPol = lm.dev.FE.coefs[1] + lm.dev.FE.coefs[2] * (-1)\nyPol\n\n\n[1] 183.0052\n\n\nThis means that the single estimates are in some sense ‘independent’ of each other (in contrast to e.g. dummy-coded variables where the estimates are bound to the reference levels of two variables) and give us insight if a specific factor is credibly different from 0. Similarly, if we wish to calculate the mean pitch of male speakers, we would calculate:\n\n\nToggle code\nyM = lm.dev.FE.coefs[1] + lm.dev.FE.coefs[3] * (-1)\nyM\n\n\n[1] 138.7712"
  },
  {
    "objectID": "practice-sheets/01e-contrast-coding-tutorial.html#helmert-coding",
    "href": "practice-sheets/01e-contrast-coding-tutorial.html#helmert-coding",
    "title": "A tutorial on contrast coding for (Bayesian) regression",
    "section": "Helmert Coding",
    "text": "Helmert Coding\nIn this coding scheme, a level of a variable is compared to its subsequent levels. In our dataset, e.g. for gender the level female is compared to the subsequent level male.\nGenerally, to create such a coding, in order to compare the first level to the subsequent levels you would assign \\((k-1)/k\\) to the first level and \\(-1/k\\) to all subsequent levels where \\(k\\) is the total number of levels. To compare the second level to the subsequent levels you would assign 0 to the first level, \\((i-1)/i\\) to the second level and \\(-i/1\\) to all subsequent levels where \\(i = k-1\\) and so on. The difference of this coding scheme to previous ones is more clear for variales with >2 levels (see below). The intercept of a linear model corresponds to the grand mean.\nR does not have a built-in function for standard Helmert coding, so we do it manually:\n\n\nToggle code\n# with politeness data\nhelm.matrix <- matrix(c(0.5, -0.5))\npoliteness_df <-\n politeness_df %>%\n mutate(gender_helm = gender,\n         context_helm = context)\ncontrasts(politeness_df$gender_helm)  <- helm.matrix\ncontrasts(politeness_df$context_helm) <- helm.matrix\n\n\nThe linear model looks like this:\n\n\nToggle code\nlm.helmert.FE <- brm(pitch ~ context_helm * gender_helm,\n                data = politeness_df,\n                cores = 4,\n                iter = 1000)\n\n\n\n\nToggle code\nlm.helmert.FE.coefs <- fixef(lm.helmert.FE)[,1] %>% as.numeric()\nsummary(lm.helmert.FE)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: pitch ~ context_helm * gender_helm \n   Data: politeness_df (Number of observations: 83) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                    192.93      4.08   184.89   200.89 1.00     2631\ncontext_helm1                 19.32      7.73     4.03    34.36 1.00     2245\ngender_helm1                 108.19      7.98    92.32   124.47 1.00     2420\ncontext_helm1:gender_helm1    15.31     15.35   -14.82    43.97 1.00     2508\n                           Tail_ESS\nIntercept                      1480\ncontext_helm1                  1397\ngender_helm1                   1361\ncontext_helm1:gender_helm1     1475\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    36.08      2.93    30.79    42.34 1.00     2044     1423\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe contrast estimate for the first level and the remaining levels is calculated by taking the mean of the dependent variable for the first level and subtracting the mean of the dependent variable for the remaining levels (in our case, just the mean of the second level). In other words, if we look at the context coefficient it denotes the difference between the mean of the polite and informal context means."
  },
  {
    "objectID": "practice-sheets/01e-contrast-coding-tutorial.html#mixing-coding-schemes",
    "href": "practice-sheets/01e-contrast-coding-tutorial.html#mixing-coding-schemes",
    "title": "A tutorial on contrast coding for (Bayesian) regression",
    "section": "Mixing Coding Schemes",
    "text": "Mixing Coding Schemes\nIf you have several categorical predictor variables, it is also possible (and often useful!) to use different coding schemes for the different variables. It might, for example, make sense to use dummy coding for a variable which has a control and a treatment condition, and to use e.g. simple coding for a variable which has two ‘equal’ levels.\nFor example, we could use dummy-coding for context and simple-coding for gender.\nWhen you mix coding schemes or define your own schemes there might be no pre-defined answers to questions as to what the sigle coefficients or the intercept mean. But knowing how the different schemes work, you can easily find this out!\nLet us explore how the interpretation of the model changes if we mix coding schemes:\n\n\nToggle code\nlm.mixedCode.FE <- brm(pitch ~ context * gender_contr,\n                data = politeness_df,\n                cores = 4,\n                iter = 1000)\n\n\n{lm.mixedCode.FE.coefs <- fixef(lm.mixedCode.FE)[,1] %>% as.numeric()} summary(lm.mixedCode.FE)\nGenerally, the interpretation is just the combination of what we have learned about the individual coding schemes. Recall that the intercept of a dummy-coded model is the mean of the reference level – since we dummy-coded context, the refernce level would be informal context. But it is not the intercept yet! We have the second predictor in our model – the simple coded gender. In simple coded models the intercept is the mean across the levels of the variable. Now the intercept of our model with the two different predictors is the mean pitch in informal contexts - across genders.\nFollowing this logic, the context estimate denotes the difference between the informal and polite contexts - still across genders. The gender estimate denoted the difference between the mean pitch and female speakers if multiplied by the value -0.5 (recall our coding above); and the mean pitch and male speakers if multiplied by the value 0.5."
  },
  {
    "objectID": "practice-sheets/01e-contrast-coding-tutorial.html#dummy-treatment-coding-1",
    "href": "practice-sheets/01e-contrast-coding-tutorial.html#dummy-treatment-coding-1",
    "title": "A tutorial on contrast coding for (Bayesian) regression",
    "section": "Dummy (Treatment) Coding",
    "text": "Dummy (Treatment) Coding\nGenerally, the coding schemes work in the very same way independently of the number of levels. The only difference to our old data set is that now we need two numeric variables coding the contrasts between the levels of one variable. If we look at the default (dummy) coding of e.g. the variable List we see a contrast matrix with two columns, each denoting the comparisons between two levels:\n\n\nToggle code\ncontrasts(latinsquare$SOA)\n\n\n       medium short\nlong        0     0\nmedium      1     0\nshort       0     1\n\n\nToggle code\ncontrasts(latinsquare_df$List)\n\n\n   L2 L3\nL1  0  0\nL2  1  0\nL3  0  1\n\n\nSo now the recoding of the categorical variable takes two numeric variables: e.g. \\(x_{1_2}\\) and \\(x_{1_3}\\), where both can take the values 0 or 1; the single levels are denoted by the combination of the two numeric variables. Again there is a reference level - List 1 - described by \\(x_{1_2}\\) and \\(x_{1_3}\\) being 0. \\(x_{1_2}\\) being 1 describes the difference between the reference level and List 2; \\(x_{1_3}\\) being 1 describes the difference between the reference level and List 3. Correspondingly, there is and individual \\(\\beta\\) for each numeric variable estimated in the regression model. The coding of the SOA factor works just the same way. The interactions between specific levels are described by combining the respective numeric variables \\(x\\). So the model we are fitting is described by:\n\\[y = \\beta_0 + \\beta_1 * x_{1_2} + \\beta_2 * x_{1_3} + \\beta_3 * x_{2_2} + \\beta_4 * x{2_3} + \\beta_5 * x_{1_2}x_{2_2} + \\beta_6 * x_{1_3}x_{2_2} + \\beta_7 * x_{1_2}x_{2_3}  + \\beta8 * x_{1_3}x_{2_3}\\]\n\n\nToggle code\nlm3.dummy.FE <- brm(RT ~ List * SOA,\n                   data = latinsquare_df,\n                   cores = 4,\n                   iter = 1000)\n\n\n\n\nToggle code\nlm3.dummy.FE.coefs <- fixef(lm3.dummy.FE)[,1] %>% as.numeric()\nsummary(lm3.dummy.FE)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RT ~ List * SOA \n   Data: latinsquare_df (Number of observations: 144) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept          529.90     11.93   506.69   553.12 1.00      759     1124\nListL2              13.44     17.38   -19.14    47.90 1.01      807     1050\nListL3              -0.63     16.73   -34.01    31.84 1.00      831     1151\nSOAmedium            7.03     17.30   -25.95    41.14 1.00      742     1223\nSOAshort            13.10     17.11   -20.69    46.55 1.00     1012     1158\nListL2:SOAmedium     6.46     24.53   -40.16    54.41 1.00      801     1034\nListL3:SOAmedium   -21.02     23.98   -67.34    27.14 1.00      823      983\nListL2:SOAshort    -22.11     24.48   -68.96    26.27 1.01      975     1018\nListL3:SOAshort    -20.16     23.12   -65.12    25.59 1.00     1090     1289\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    46.65      3.01    41.36    52.92 1.00     1780     1045\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nSince both predictors are dummy-coded the intercept represents the reference level - the mean RT for List 1 and a long SOA. Following the same procedure as for the two-level variables you could calculate the estimated mean RTs for specific conditions."
  },
  {
    "objectID": "practice-sheets/01e-contrast-coding-tutorial.html#simple-contrast-coding-1",
    "href": "practice-sheets/01e-contrast-coding-tutorial.html#simple-contrast-coding-1",
    "title": "A tutorial on contrast coding for (Bayesian) regression",
    "section": "Simple (Contrast) Coding",
    "text": "Simple (Contrast) Coding\nSimple coding only slightly differs from dummy-coding – the intercept of the model is the grand mean, not the mean RT of the reference level. Otherwise, the coefficients still denote the difference between the reference level and other specific levels.\n\n\nToggle code\nlatinsquare_df %>%\n  mutate(List_contr = factor(List),\n         SOA_contr = factor(SOA)) -> latinsquare_df\ndummy.matrix3 <- contr.treatment(3)\ncontr.matrix3 <- matrix(c(1/3, 1/3, 1/3, 1/3, 1/3, 1/3), ncol=2)\ncontrasts(latinsquare_df$List_contr) <- dummy.matrix3 - contr.matrix3\ncontrasts(latinsquare_df$SOA_contr) <-  dummy.matrix3 - contr.matrix3\n\n\n\n\nToggle code\nlm3.simple.FE <- brm(RT ~ List_contr * SOA_contr,\n                   data = latinsquare_df,\n                   cores = 4,\n                   iter = 1000)\n\n\n\n\nToggle code\nlm3.simple.FE.coefs <- fixef(lm3.simple.FE)[,1] %>% as.numeric()\nsummary(lm3.simple.FE)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RT ~ List_contr * SOA_contr \n   Data: latinsquare_df (Number of observations: 144) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                534.38      4.10   526.51   542.48 1.00     2787\nList_contr2                7.70      9.68   -11.66    27.58 1.00     1392\nList_contr3              -14.64      9.66   -34.49     3.60 1.00     1415\nSOA_contr2                 1.88      9.55   -16.65    20.66 1.00     2321\nSOA_contr3                -0.74      9.56   -19.48    17.90 1.00     1943\nList_contr2:SOA_contr2     5.40     23.66   -40.17    51.25 1.00     1609\nList_contr3:SOA_contr2   -21.87     22.28   -64.04    23.51 1.00     1556\nList_contr2:SOA_contr3   -23.49     22.46   -68.63    19.74 1.00     1594\nList_contr3:SOA_contr3   -21.80     22.98   -67.10    26.17 1.00     1924\n                       Tail_ESS\nIntercept                  1369\nList_contr2                1002\nList_contr3                1137\nSOA_contr2                 1811\nSOA_contr3                 1624\nList_contr2:SOA_contr2     1553\nList_contr3:SOA_contr2     1704\nList_contr2:SOA_contr3     1532\nList_contr3:SOA_contr3     1740\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    46.63      2.81    41.43    52.41 1.00     2453     1506\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "practice-sheets/01e-contrast-coding-tutorial.html#deviation-sum-coding-1",
    "href": "practice-sheets/01e-contrast-coding-tutorial.html#deviation-sum-coding-1",
    "title": "A tutorial on contrast coding for (Bayesian) regression",
    "section": "Deviation (Sum) Coding",
    "text": "Deviation (Sum) Coding\nWith increasing number of levels within the factors the complexity and messiness of interpreting the differences between levels against each other increases considerably. Hence it might make a lot of sense to use the deviation coding scheme which provides estimates of effects comapred to the grand mean. We again can use the R built-in function to assign deviation coding to our three-level factors:\n\n\nToggle code\nlatinsquare_df %>%\n  mutate(List_dev = List,\n         SOA_dev = SOA) -> latinsquare_df\ncontrasts(latinsquare_df$List_dev) <- contr.sum(3) # insert number of levels\ncontrasts(latinsquare_df$SOA_dev) <- contr.sum(3)\n\n\nFor e.g. SOA our numeric variables now denote the effect of long SOA compared to the grand mean when \\(x_{2_2}\\) is a 1 and \\(x_{2_3}\\) is a 0; they denote the effect of medium SOA compared to the grand mean when \\(x_{2_2}\\) is a 0 and \\(x_{2_3}\\) is a 1; the effect of short SOA is never compared to the grand mean since it is always assigned a -1.\n\n\nToggle code\nlm3.dev.FE <- brm(RT ~ List_dev * SOA_dev,\n                   data = latinsquare_df,\n                 cores = 4,\n                 iter = 1000)\n\n\n\n\nToggle code\nlm3.dev.FE.coefs <- fixef(lm3.dev.FE)[,1] %>% as.numeric()\nsummary(lm3.dev.FE)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RT ~ List_dev * SOA_dev \n   Data: latinsquare_df (Number of observations: 144) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept            534.57      3.94   526.94   542.32 1.00     3088     1295\nList_dev1              2.30      5.39    -8.65    12.81 1.00     2052     1549\nList_dev2             10.07      5.48    -0.85    21.29 1.00     2046     1647\nSOA_dev1              -0.42      5.47   -11.25    10.27 1.00     1804     1447\nSOA_dev2               1.33      5.72   -10.08    12.43 1.00     2088     1537\nList_dev1:SOA_dev1    -6.91      7.57   -21.55     8.16 1.00     1684     1337\nList_dev2:SOA_dev1    -1.15      7.58   -15.78    13.79 1.00     1385     1628\nList_dev1:SOA_dev2    -1.36      7.70   -16.16    13.87 1.00     1798     1622\nList_dev2:SOA_dev2    10.06      7.63    -4.90    25.36 1.00     1966     1508\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    46.45      2.69    41.69    51.94 1.00     1987     1350\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThink about what the single estimates mean!"
  },
  {
    "objectID": "practice-sheets/01e-contrast-coding-tutorial.html#helmert-coding-1",
    "href": "practice-sheets/01e-contrast-coding-tutorial.html#helmert-coding-1",
    "title": "A tutorial on contrast coding for (Bayesian) regression",
    "section": "Helmert Coding",
    "text": "Helmert Coding\nFor recap: In this coding scheme, a level of a variable is compared to its subsequent levels. What does this mean for the three-level factors?\n\n\nToggle code\nlatinsquare_df %>%\n  mutate(List_helm = List,\n         SOA_helm = SOA) -> latinsquare_df\nhelm.matrix3 <- matrix(c(2/3, -1/3, -1/3, 0, 1/2, -1/2 ), ncol = 2)\ncontrasts(latinsquare_df$List_helm) <- helm.matrix3\ncontrasts(latinsquare_df$SOA_helm) <- helm.matrix3\n\n\n\n\nToggle code\nlm3.helm.FE <- brm(RT ~ List_helm * SOA_helm,\n                   data = latinsquare_df,\n                  cores = 4,\n                  iter = 1000)\n\n\n\n\nToggle code\nlm3.helm.FE.coefs <- fixef(lm3.helm.FE)[,1] %>% as.numeric()\nsummary(lm3.helm.FE)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RT ~ List_helm * SOA_helm \n   Data: latinsquare_df (Number of observations: 144) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept              534.52      4.01   526.81   542.04 1.00     4863\nList_helm1               3.44      8.28   -12.57    19.48 1.00     4113\nList_helm2              22.62      9.75     3.29    42.01 1.00     3900\nSOA_helm1               -1.15      8.32   -17.07    15.73 1.01     4065\nSOA_helm2                2.73      9.29   -14.58    20.77 1.00     3980\nList_helm1:SOA_helm1   -15.27     17.15   -49.55    17.92 1.00     4568\nList_helm2:SOA_helm1   -12.87     19.80   -51.78    24.14 1.00     4569\nList_helm1:SOA_helm2   -14.43     20.83   -55.60    26.25 1.01     4282\nList_helm2:SOA_helm2    28.75     22.28   -14.39    72.08 1.00     4186\n                     Tail_ESS\nIntercept                1675\nList_helm1               1271\nList_helm2               1464\nSOA_helm1                1245\nSOA_helm2                1695\nList_helm1:SOA_helm1     1329\nList_helm2:SOA_helm1     1483\nList_helm1:SOA_helm2     1664\nList_helm2:SOA_helm2     1648\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    46.53      2.94    41.31    52.64 1.00     3058     1708\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe main effect estimates denote the differences between the mean of List1 and the mean of (List2 + List3); and between the mean of List2 and the mean of List3. Respectively, they denote the differences between the mean of SOA long and the mean of (medium + short); and between the mean of SOA medium and the mean of short. The intercept is the grand mean."
  },
  {
    "objectID": "practice-sheets/01e-contrast-coding-tutorial.html#reverse-helmert-coding",
    "href": "practice-sheets/01e-contrast-coding-tutorial.html#reverse-helmert-coding",
    "title": "A tutorial on contrast coding for (Bayesian) regression",
    "section": "Reverse Helmert Coding",
    "text": "Reverse Helmert Coding\nThe reverse Helmert coding scheme (also called difference coding) is quite similar to the Helmert coding, but compares the mean of a level to its previous levels. Since we basically reverse the coding we used in the previous scheme, we also ‘reverse’ the contrast matrix to create such a coding.\n\n\nToggle code\nlatinsquare_df %>%\n  mutate(List_rhelm = List,\n         SOA_rhelm = SOA) -> latinsquare_df\nrhelm.matrix3 <- matrix(c(-1/2, 1/2, 0, -1/3, -1/3, 2/3 ), ncol = 2)\ncontrasts(latinsquare_df$List_rhelm) <- rhelm.matrix3\ncontrasts(latinsquare_df$SOA_rhelm) <- rhelm.matrix3\n\n\n\n\nToggle code\nlm3.rhelm.FE <- brm(RT ~ List_rhelm * SOA_rhelm,\n                   data = latinsquare_df,\n                   iter = 1000,\n                   cores = 4)\n\n\n\n\nToggle code\nlm3.rhelm.FE.coefs <- fixef(lm3.rhelm.FE)[,1] %>% as.numeric()\nsummary(lm3.rhelm.FE)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RT ~ List_rhelm * SOA_rhelm \n   Data: latinsquare_df (Number of observations: 144) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                534.57      3.97   526.52   542.06 1.00     4160\nList_rhelm1                7.56      9.38   -10.92    25.93 1.00     3472\nList_rhelm2              -18.56      8.19   -33.74    -2.88 1.00     3683\nSOA_rhelm1                 2.27      9.82   -16.10    21.71 1.00     3418\nSOA_rhelm2                -1.52      8.41   -18.45    14.59 1.00     3335\nList_rhelm1:SOA_rhelm1     5.94     23.31   -40.30    52.33 1.00     4579\nList_rhelm2:SOA_rhelm1   -24.18     19.78   -63.66    13.77 1.00     4232\nList_rhelm1:SOA_rhelm2   -26.11     20.90   -67.36    15.47 1.00     4641\nList_rhelm2:SOA_rhelm2     2.28     17.63   -32.67    36.13 1.00     4698\n                       Tail_ESS\nIntercept                  1648\nList_rhelm1                1433\nList_rhelm2                1620\nSOA_rhelm1                 1225\nSOA_rhelm2                 1432\nList_rhelm1:SOA_rhelm1     1368\nList_rhelm2:SOA_rhelm1     1482\nList_rhelm1:SOA_rhelm2     1237\nList_rhelm2:SOA_rhelm2     1450\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    46.51      2.77    41.51    52.39 1.01     3222     1669\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nIn our example, the first estimate denotes the difference between the mean of List2 and the mean of List1; the second - the difference between the mean of List3 and the mean of (List1 + List2). The main effects of SOA can be interpreted similarly."
  },
  {
    "objectID": "practice-sheets/01e-contrast-coding-tutorial.html#mixed-schemes-dummy-and-deviation-coding",
    "href": "practice-sheets/01e-contrast-coding-tutorial.html#mixed-schemes-dummy-and-deviation-coding",
    "title": "A tutorial on contrast coding for (Bayesian) regression",
    "section": "Mixed Schemes: Dummy and Deviation Coding",
    "text": "Mixed Schemes: Dummy and Deviation Coding\nJust like with two-level factors, we might wish to use different coding schemes for different predictors. It might make sense to use dummy coding for a variable which has a control and two different treatment conditions, and to use deviation coding for a variable which has ‘equal’ levels.\nFor example, we could use dummy-coding for List and deviation-coding for SOA.\n\n\nToggle code\nlm3.mixedCode.FE <- brm(RT ~ List * SOA_dev,\n                   data = latinsquare_df,\n                   cores = 4,\n                   iter = 1000)\nlm3.mixedCode.FE.coefs <- fixef(lm3.mixedCode.FE)[,1] %>% as.numeric()\nsummary(lm3.mixedCode.FE)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RT ~ List * SOA_dev \n   Data: latinsquare_df (Number of observations: 144) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept         537.01      6.45   524.40   549.50 1.00     1899     1490\nListL2              7.59      9.36   -11.28    26.24 1.00     1799     1584\nListL3            -15.13      9.37   -33.60     3.32 1.00     2058     1622\nSOA_dev1           -7.34      9.44   -24.96    11.32 1.01     1108     1145\nSOA_dev2            0.22      9.79   -18.99    20.09 1.00      985     1447\nListL2:SOA_dev1     5.91     13.83   -21.08    33.31 1.00     1207     1248\nListL3:SOA_dev1    14.46     13.52   -12.39    40.70 1.00     1359     1174\nListL2:SOA_dev2    11.60     13.85   -15.50    38.34 1.00     1189     1325\nListL3:SOA_dev2    -7.16     13.66   -33.54    20.74 1.00     1148     1164\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    46.58      2.83    41.57    52.17 1.00     2310     1728\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nTowards the end of this tutorial, the main take-away is this: when you look at the estimates of (any) model, you could ask yourself a couple of questions like these to make sure you understand what was calculated:\n\nWhat does the intercept represent?\nWhat do the single estimates mean?\nWhat do they tell me about my hypotheses?\n\nOf course, you will also encounter experimental designs which use a two-level and a three-level categorical predictors – but the conceptual basics regarding how to choose the contrasts and how to interpret linear models are the same."
  },
  {
    "objectID": "practice-sheets/01d-predictives.html",
    "href": "practice-sheets/01d-predictives.html",
    "title": "Prior and prior predictives",
    "section": "",
    "text": "The tutorial introduces the concept of “predictive distribution”. There usually three kinds of commonly relevant predictions: (i) linear predictor value, (ii) the central tendency, and (iii) the data. The tutorial shows how to collect samples for all of these. As predictive distributions are important from the prior and posterior point of view, the tutorial also shows how to use the prior predictive distributions as intuition-fuel for the choice of prior."
  },
  {
    "objectID": "practice-sheets/01d-predictives.html#posterior-predictives",
    "href": "practice-sheets/01d-predictives.html#posterior-predictives",
    "title": "Prior and prior predictives",
    "section": "Posterior predictives",
    "text": "Posterior predictives\nHere is an example for a logistic regression model (where all the three measures clearly show their conceptual difference). Fit the model to some data first (here: predicting accuracy for two categorical factors with two levels each):\n\n\nToggle code\nfit_MT_logistic <- \n  brms::brm(\n    formula = correct ~ group * condition,\n    data    = aida::data_MT,\n    family  = brms::bernoulli()\n  )\n\n\nThe posterior predictive (in the most general sense) makes predictions about the to-be-expected data, here a Boolean value of whether a response was correct.\n\n\nToggle code\n# 2 samples from the predictive distribution (data samples)\ndata_MT |> \n  select(group, condition) |> \n  unique() |> \n  tidybayes::add_predicted_draws(\n    fit_MT_logistic,\n    ndraws = 2\n    )\n\n\n# A tibble: 8 × 7\n# Groups:   group, condition, .row [4]\n  group condition  .row .chain .iteration .draw .prediction\n  <chr> <chr>     <int>  <int>      <int> <int>       <int>\n1 touch Atypical      1     NA         NA     1           1\n2 touch Atypical      1     NA         NA     2           1\n3 touch Typical       2     NA         NA     1           1\n4 touch Typical       2     NA         NA     2           1\n5 click Atypical      3     NA         NA     1           1\n6 click Atypical      3     NA         NA     2           1\n7 click Typical       4     NA         NA     1           1\n8 click Typical       4     NA         NA     2           1\n\n\nA predicted central tendency for this logistic model is a probability of giving a correct answer.\n\n\nToggle code\n# 2 samples from the predicted central tendency\ndata_MT |> \n  select(group, condition) |> \n  unique() |> \n  tidybayes::add_epred_draws(\n    fit_MT_logistic,\n    ndraws = 2\n    )\n\n\n# A tibble: 8 × 7\n# Groups:   group, condition, .row [4]\n  group condition  .row .chain .iteration .draw .epred\n  <chr> <chr>     <int>  <int>      <int> <int>  <dbl>\n1 touch Atypical      1     NA         NA     1  0.917\n2 touch Atypical      1     NA         NA     2  0.924\n3 touch Typical       2     NA         NA     1  0.945\n4 touch Typical       2     NA         NA     2  0.945\n5 click Atypical      3     NA         NA     1  0.841\n6 click Atypical      3     NA         NA     2  0.864\n7 click Typical       4     NA         NA     1  0.961\n8 click Typical       4     NA         NA     2  0.956\n\n\nPredictions at the linear predictor level are sometimes not so easy to interpret. The interpretation depends on the kind of link function used (more on this under the topic of “generalized linear models”). For a logistic regression, this number is a log-odds ratio (which determines the predicted correctness-probability).\n\n\nToggle code\n# 2 samples for the linear predictor\ndata_MT |> \n  select(group, condition) |> \n  unique() |> \n  tidybayes::add_linpred_draws(\n    fit_MT_logistic,\n    ndraws = 2\n    )\n\n\n# A tibble: 8 × 7\n# Groups:   group, condition, .row [4]\n  group condition  .row .chain .iteration .draw .linpred\n  <chr> <chr>     <int>  <int>      <int> <int>    <dbl>\n1 touch Atypical      1     NA         NA     1     2.07\n2 touch Atypical      1     NA         NA     2     2.23\n3 touch Typical       2     NA         NA     1     3.22\n4 touch Typical       2     NA         NA     2     2.75\n5 click Atypical      3     NA         NA     1     1.46\n6 click Atypical      3     NA         NA     2     2.00\n7 click Typical       4     NA         NA     1     3.52\n8 click Typical       4     NA         NA     2     3.13"
  },
  {
    "objectID": "practice-sheets/01d-predictives.html#prior-predictives",
    "href": "practice-sheets/01d-predictives.html#prior-predictives",
    "title": "Prior and prior predictives",
    "section": "Prior predictives",
    "text": "Prior predictives\nTo sample from the prior predictive we first need to initialize the model, setting the option sample_prior = \"only\". It is necessary to specify priors for all parameters that would otherwise be improper.\n\n\nToggle code\nfit_MT_logistic_prior <- \n  brms::brm(\n    formula = correct ~ group * condition,\n    data    = aida::data_MT,\n    family  = brms::bernoulli(),\n    # rather unspecific priors\n    prior   = prior(student_t(3,0,2.5)),\n    # tell BRMS to not condition on the data\n    sample_prior = \"only\"\n  )\n\n\nObtaining samples from the prior predictive distributions is then the same as before (from this point on it doesn’t matter whether the model was trained on data or not). For example, here are two samples from the prior predictive (data) distribution:\n\n\nToggle code\n# 2 samples from the predictive distribution (data samples)\ndata_MT |> \n  select(group, condition) |> \n  unique() |> \n  tidybayes::add_predicted_draws(\n    fit_MT_logistic_prior,\n    ndraws = 2\n    )\n\n\n# A tibble: 8 × 7\n# Groups:   group, condition, .row [4]\n  group condition  .row .chain .iteration .draw .prediction\n  <chr> <chr>     <int>  <int>      <int> <int>       <int>\n1 touch Atypical      1     NA         NA     1           0\n2 touch Atypical      1     NA         NA     2           1\n3 touch Typical       2     NA         NA     1           1\n4 touch Typical       2     NA         NA     2           0\n5 click Atypical      3     NA         NA     1           0\n6 click Atypical      3     NA         NA     2           1\n7 click Typical       4     NA         NA     1           1\n8 click Typical       4     NA         NA     2           1\n\n\nNotice that a posteriori correct trials are predicted to be very likely, but a priori they are not more likely than incorrect ones. - Wait, how do we know? Let’s poke a bit.\n\n\n\n\n\n\nExercise 1\n\n\n\n\n\nObtain sufficient samples from the prior predictive distribution for the central tendency (the predicted probability of correctness) to address the question of whether this model, as specified above, initially predicts correct and false answers to be equally likely. Use Bayesian summary statistics to corroborate your claim.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nLet’s obtain 4000 samples for each different quadruple of \\(x\\) values and summarize the resulting samples:\n\n\nToggle code\ndata_MT |> \n  select(group, condition) |> \n  tidybayes::add_epred_draws(\n    fit_MT_logistic_prior,\n    ndraws = 4000\n    ) |> ungroup() |> \n  group_by(group, condition) |> \n  reframe(aida::summarize_sample_vector(.epred)[,-1])\n\n\n# A tibble: 4 × 5\n  group condition   `|95%`  mean `95%|`\n  <chr> <chr>        <dbl> <dbl>  <dbl>\n1 click Atypical  4.22e-13 0.490  1.00 \n2 click Typical   9.91e-13 0.494  0.999\n3 touch Atypical  1.31e-12 0.496  1.00 \n4 touch Typical   2.84e- 4 0.500  1.00 \n\n\nThe expected value of the posterior predictive is indeed around 0.5, but there is total uncertainty (about the accuracy).\n\n\n\n\n\n\n\n\n\n\n\n\nExercise [SPECIAL]\n\n\n\n\n\nI wonder why the same code, but using tidybayes::hdi does not work as expected. If anyone knows or finds out, please share.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\ndata_MT |> \n  select(group, condition) |> \n  tidybayes::add_epred_draws(\n    fit_MT_logistic_prior,\n    ndraws = 4000\n    ) |> ungroup() |>\n  group_by(group, condition) |> \n  summarize(\n    lower  = tidybayes::hdi(.epred, .width = 0.95)[1],\n    mean   = mean(.epred),\n    higher = tidybayes::hdi(.epred, .width = 0.95)[2]\n  )\n\n\n# A tibble: 4 × 5\n# Groups:   group [2]\n  group condition    lower  mean higher\n  <chr> <chr>        <dbl> <dbl>  <dbl>\n1 click Atypical  4.22e-13 0.490  0.432\n2 click Typical   9.91e-13 0.494  0.342\n3 touch Atypical  1.31e-12 0.496  0.456\n4 touch Typical   2.90e-12 0.500  0.376"
  },
  {
    "objectID": "practice-sheets/01d-predictives.html#prior-plausibility",
    "href": "practice-sheets/01d-predictives.html#prior-plausibility",
    "title": "Prior and prior predictives",
    "section": "Prior plausibility",
    "text": "Prior plausibility\nChecking a model’s prior predictive distributions is an integral part of the Bayesian workflow. It is not always apparent what a particular choice of prior entails for the model’s other parameters, or its prior (data) predictive. Let’s explore how we can test implications of prior choice visually.\nWe are using the fit of a linear model to the (scaled) average world temperature data for the year 2025 to 2024. The function plot_predictPriPost below allows you to specify a prior for the model’s paramters for which it will show samples from the model’s prior and posterior prediction of the measure of central tendency, as well as the data. Plotting exercises like these inform you about how strong or biased your priors are, whether they are reasonably in line with your intentions, and whether they seem to inform the posterior strongly (an informal, punctuated sensitivity analysis).\n\n\nToggle code\nplot_predictPriPost <- function(prior_spec, ndraws = 1000) {\n  \n  # get the posterior fit\n  fit <- brm(\n    avg_temp ~ year,\n    prior = prior_spec,\n    data = aida::data_WorldTemp,\n    silent = TRUE,\n    refresh = 0\n  )\n  \n  # retrieve prior samples from the posterior fit\n  fit_prior_only <- update(\n    fit,\n    silent = TRUE,\n    refresh = 0,\n    sample_prior = \"only\"\n  )\n  \n  get_predictions <- function(fit_object, type = \"prior prediction\") {\n    \n    tidybayes::add_epred_draws(\n      fit_object, \n      newdata = tibble(year = aida::data_WorldTemp$year),\n      ndraws = ndraws,\n      value = 'avg_tmp'\n    ) |> \n      ungroup() |> \n      select(year, .draw, avg_tmp) |> \n      mutate(type = type)\n    \n  }\n  \n  get_predictions(fit, \"posterior prediction\") |> \n    rbind(get_predictions(fit_prior_only, \"prior prediction\")) |> \n    mutate(type = factor(type, levels = c(\"prior prediction\", \"posterior prediction\"))) |> \n    ggplot() + \n    facet_grid(type ~ ., scales = \"free\") +\n    geom_line(aes(x = year, y = avg_tmp, group = .draw), \n              color = \"gray\", alpha = 0.3) +\n    geom_point(data = aida::data_WorldTemp, \n               aes(x = year, y = avg_temp), color = project_colors[2], size = 1, alpha = 0.8) +\n    ylab(\"average temperature\")\n}\n\nprior_baseline <- c(prior(\"normal(0, 0.02)\", class = \"b\"),\n                    prior(\"student_t(3, 8, 5)\", class = \"Intercept\"))\nplot_predictPriPost(prior_baseline)\n\n\n\n\n\nToggle code\nprior_opinionated <- c(prior(\"normal(0.2, 0.05)\", class = \"b\"),\n                       prior(\"student_t(3, 8, 5)\", class = \"Intercept\"))\nplot_predictPriPost(prior_opinionated)\n\n\n\n\n\nToggle code\nprior_crazy <- c(prior(\"normal(-1, 0.005)\", class = \"b\"),\n                 prior(\"student_t(3, 8, 5)\", class = \"Intercept\"))\nplot_predictPriPost(prior_crazy)\n\n\n\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\n\nTest different prior specifications, and inspect the resulting prior and posterior predictions. This is just to build your intuitions, and also to help you try out different kinds of prior probability distributions (try a lower- or upper-bounded distribution, if you dare).\n\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\n\n\nSet up a similar plotting pipeline for another model, e.g., the logistic regression model used above, or (ideally) a model you really care about."
  },
  {
    "objectID": "practice-sheets/01d-predictives.html#visual-predictive-checks",
    "href": "practice-sheets/01d-predictives.html#visual-predictive-checks",
    "title": "Prior and prior predictives",
    "section": "Visual predictive checks",
    "text": "Visual predictive checks\nLet’s have a closer look at prior and posterior predictives, and the functions that we can use to explore them. Here, we fit a regression model with the “opinionated priors” from above, obtaining both posterior and prior samples for it.\n\n\nToggle code\nfit_posterior <- brm(\n    avg_temp ~ year,\n    prior = prior_opinionated,\n    data = aida::data_WorldTemp\n  )\n\n\n\n\nToggle code\nfit_prior <- stats::update(\n    fit_posterior,\n    sample_prior = \"only\"\n  )\n\n\nThe bayesplot package has a number of visual predictive check functions nested inside the function pp_check. Here are examples.\nWithout additional argument pp_check compares the overal observed distribution of the repsonse variable to the prior/posterior predictive distribution. Check the observed distribution (marginal of \\(y\\)) first:\n\n\nToggle code\naida::data_WorldTemp |> \n  ggplot(aes(x = avg_temp)) + geom_density()\n\n\n\n\n\nThe prior predictive check shows that this prior is way less “opinionated” or biased than its name may suggest:\n\n\nToggle code\nbrms::pp_check(fit_prior, ndraws = 50)\n\n\n\n\n\nThe posterior predictive check can reveal systematic problems with the model, such as here: an inability to capture the bimodal-ish shape of the data.\n\n\nToggle code\nbrms::pp_check(fit_posterior, ndraws = 50)\n\n\n\n\n\nThere are number of different plots pp_check is able to produce. For fine-grained plotting and exploring, the bayesplot package offers flexible plotting tools. These come in pairs: predicitve distributions only show the predictions, while predictive checks also show the data. See help(\"PPC-overview\") and help(\"PPD-overview\") for more information.\nThe general workflow is that you first extract samples from the relevant predictive distribution (in matrix form), like so:\n\n\nToggle code\npredictive_samples <- brms::posterior_predict(fit_posterior, ndraws = 1000)\npredictive_samples[1:5, 1:5] \n\n\n         [,1]     [,2]     [,3]     [,4]     [,5]\n[1,] 7.133570 7.227160 7.361394 6.814197 7.140860\n[2,] 7.713238 8.014163 7.435610 7.985263 8.429999\n[3,] 7.388428 8.378705 7.053194 7.160673 7.888227\n[4,] 7.004546 6.964185 8.398096 6.753466 6.873376\n[5,] 7.184512 7.006854 7.155113 7.302313 7.250978\n\n\nAnd then you can, for example, compare the distribution of some test statistic (here: the standard deviation), using a function like ppc_stat:\n\n\nToggle code\nbayesplot::ppc_stat(\n  y    = aida::data_WorldTemp$avg_temp, \n  yrep = predictive_samples,\n  stat = sd)\n\n\n\n\n\n\n\n\n\n\n\nExercise 4\n\n\n\n\n\nInterpret this plot.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe light blue histogram indicates the distribution of the values of the test statistics under the predictive distribution (here: posterior). The darker blue line indicates the value of the test statistic for the observed data.\nIn this case, the observed test value is rather central in the posterior predictive distribution, thus suggesting that, as far as the standard deviation is concerned, the model cannot be criticized for its posterior predictions.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 5\n\n\n\n\n\nTry a similar ppc_stat plot for the prior predictive. Can you find a test statistic for which the model looks adequate?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nLooking at the prior predicted mean is not too bad (at least visually).\n\n\nToggle code\npredictive_samples <- brms::posterior_predict(fit_prior, ndraws = 1000)\nbayesplot::ppc_stat(\n  y    = aida::data_WorldTemp$avg_temp, \n  yrep = predictive_samples,\n  stat = mean)\n\n\n\n\n\nThat is because the predictions are very wide. There is nothing wrong about that! But, of course, another criterion that the prior predictive distribution blatantly fails is to predict the deviation in the data adequately (again this is, arguably, how it should be if we want to learn from the data):\n\n\nToggle code\nbayesplot::ppc_stat(\n  y    = aida::data_WorldTemp$avg_temp, \n  yrep = predictive_samples,\n  stat = sd)"
  },
  {
    "objectID": "practice-sheets/01d-predictives.html#bayesian-p-values",
    "href": "practice-sheets/01d-predictives.html#bayesian-p-values",
    "title": "Prior and prior predictives",
    "section": "Bayesian \\(p\\)-values",
    "text": "Bayesian \\(p\\)-values\nUsing model predictions, we can also compute Bayesian \\(p\\)-values as handy summary statistics for visual predictive checks. A single number never replaces the information we obtain from (good) plots, but is easier to communicate and may help interpretation (though should never solely dominate decision making).\nLet’s focus on the posterior model for the temperature data and apply a rigorous (data-informed) test statistic: the standard deviation for the data observation up to 1800.\n\n\nToggle code\npostPred_y <- \n  tidybayes::predicted_draws(\n    object  = fit_posterior,\n    newdata = aida::data_WorldTemp |> select(year) |> filter(year <= 1800),\n    value   = \"avg_temp\",\n    ndraws  = 4000) |> ungroup() |> \n  select(.draw,year, avg_temp)\n\nsd_postPred <- postPred_y |> \n  group_by(.draw) |> \n  summarize(sd_post_pred = sd(avg_temp)) |> \n  pull(sd_post_pred)\n\nsd_data <- aida::data_WorldTemp |> filter(year <= 1800) |> pull(avg_temp) |> sd()\n\nmean(sd_data > sd_postPred)\n\n\n[1] 1\n\n\n\n\n\n\n\n\nExercise 6: Predictive \\(p\\)-values w/ SD as test statistic\n\n\n\n\n\nMake sure you understand how the code in the last code block works. Interpret the numerical result.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe see an estimated \\(p\\)-value of close to one, which is really bad (for the chosen test statistic). It means that the model never predicts data with a value of the test statistic that is that extreme. Notice that “extremeness” here means “very high or very low”. So in this case, we would clearly have ground of accusing the model to fail to predict the aspect captured by this test statistic.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 7: [ambitious] Likelihood as test statistic\n\n\n\n\n\nUse the code above to calculate a Bayesian \\(p\\) value for the same data and model but assuming that the likelihood of the data is the test statistic. Note that brms::log_lik is a handy function for obtaining the likelihood of some \\(y'\\) –be it observed, predicted or made up– given a model (prior or posterior)\nInterpret the result you get (also in relation to the results from the previous exercise).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n[1] 0.81\n\n\ntry yourself; solution will follow"
  },
  {
    "objectID": "practice-sheets/03d-mixture-models.html",
    "href": "practice-sheets/03d-mixture-models.html",
    "title": "XX: Mixture models",
    "section": "",
    "text": "This tutorial discusses a minimal example of a mixture model. After introducing the main idea behind mixture models, a fictitious (minimal) data set is analyzed first with a hand-written Stan program, and then with a mixture regression model using brms."
  },
  {
    "objectID": "practice-sheets/03d-mixture-models.html#the-stan-model",
    "href": "practice-sheets/03d-mixture-models.html#the-stan-model",
    "title": "XX: Mixture models",
    "section": "The Stan model",
    "text": "The Stan model\nWe are going to pack the data together for fitting the Stan model:\n\n\nToggle code\ndata_GMM <- list(\n  y = flower_heights,\n  N = length(flower_heights)\n)\n\n\nBelow is the Stan code for this model. It is also given in file Gaussian-mixture-01-basic.stan. A few comments on this code:\n\nThere is no occurrence of variable \\(z_i\\), as this is marginalized out. We do this by incrementing the log-score manually, using target += log_sum_exp(alpha).\nWe declare vector mu to be of a particular type which we have not seen before. We want the vector to be ordered. We will come back to this later. Don’t worry about it now.\n\ndata {\n  int<lower=1> N; \n  real y[N];      \n}\nparameters {\n  real<lower=0,upper=1> p;         \n  ordered[2] mu;             \n  vector<lower=0>[2] sigma; \n}\nmodel {\n  p ~ beta(1,1);\n  mu ~ normal(12, 10);\n  sigma ~ lognormal(0, 1);\n  for (i in 1:N) {\n    vector[2] alpha;\n    alpha[1] = log(p)   + normal_lpdf(y[i] | mu[1], sigma[1]);\n    alpha[2] = log(1-p) + normal_lpdf(y[i] | mu[2], sigma[2]);\n    target += log_sum_exp(alpha);\n  }\n}\n\n\nToggle code\nstan_fit_2b_GMM <- stan(\n  file = 'stan-files/Gaussian-mixture-01-basic.stan',\n  data = data_GMM\n)\n\n\n\n\nToggle code\nstan_fit_2b_GMM\n\n\nInference for Stan model: Gaussian-mixture-01-basic.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n            mean se_mean   sd    2.5%     25%     50%     75%   97.5% n_eff\np           0.55    0.01 0.16    0.18    0.46    0.54    0.64    0.85   368\nmu[1]      10.32    0.17 1.79    8.84    9.83   10.38   11.02   12.46   115\nmu[2]      15.72    0.05 1.01   13.55   15.40   15.80   16.12   16.73   360\nsigma[1]    2.12    0.02 0.64    1.08    1.69    2.04    2.51    3.42   793\nsigma[2]    1.43    0.03 0.55    0.64    1.07    1.34    1.65    2.98   417\nlp__     -125.77    0.16 2.53 -133.42 -126.71 -125.08 -124.07 -122.94   237\n         Rhat\np        1.01\nmu[1]    1.04\nmu[2]    1.01\nsigma[1] 1.00\nsigma[2] 1.01\nlp__     1.02\n\nSamples were drawn using NUTS(diag_e) at Tue Mar 28 22:06:02 2023.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\n\n\n\n\n\n\nExercise 1b: Interpret this outcome\n\n\n\n\n\nInterpret these results! Focus on parameters \\(p\\), \\(\\mu_1\\) and \\(\\mu_2\\). What does \\(p\\) capture in this implementation? Do the (mean) estimated values make sense?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYes, they do make sense. \\(p\\) is the prevalence of data from the group with the higher mean, which is group \\(B\\) in our case. The model infers that there are roughly equally many data points from each group, which is indeed the case. The model also recovers the descriptive means of each group!\n\n\nToggle code\nffm_data |> \n  group_by(species) |> \n  summarise(\n    mean     = mean(height),\n    std_dev  = sd(height)\n  )\n\n\n# A tibble: 2 × 3\n  species  mean std_dev\n  <chr>   <dbl>   <dbl>\n1 A        10.0    1.76\n2 B        15.7    1.38"
  },
  {
    "objectID": "practice-sheets/03d-mixture-models.html#an-unidentifiable-model",
    "href": "practice-sheets/03d-mixture-models.html#an-unidentifiable-model",
    "title": "XX: Mixture models",
    "section": "An unidentifiable model",
    "text": "An unidentifiable model\nLet’s run the model in file Gaussian-mixture-02-unindentifiable.stan, which is exactly the same as before but with vector mu being an unordered vector of reals.\n\n\nToggle code\nstan_fit_2c_GMM <- stan(\n  file = 'stan-files/Gaussian-mixture-02-unindentifiable.stan',\n  data = data_GMM,\n  # set a seed for reproducible results\n  seed = 1734\n)\n\n\nHere’s a summary of the outcome:\n\n\nToggle code\nstan_fit_2c_GMM\n\n\nInference for Stan model: Gaussian-mixture-02-unindentifiable.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n            mean se_mean   sd    2.5%     25%     50%     75%   97.5% n_eff\np           0.51    0.02 0.14    0.22    0.42    0.51    0.60    0.80    37\nmu[1]      12.50    1.55 2.69    9.15   10.15   11.18   15.59   16.51     3\nmu[2]      13.67    1.55 2.69    9.29   10.63   15.26   15.93   16.63     3\nsigma[1]    1.83    0.23 0.64    0.83    1.34    1.73    2.24    3.22     8\nsigma[2]    1.66    0.21 0.59    0.78    1.22    1.57    2.01    3.07     8\nlp__     -127.03    0.06 1.86 -131.74 -128.01 -126.61 -125.64 -124.66  1065\n         Rhat\np        1.06\nmu[1]    2.93\nmu[2]    3.12\nsigma[1] 1.22\nsigma[2] 1.21\nlp__     1.01\n\nSamples were drawn using NUTS(diag_e) at Tue Mar 28 22:06:12 2023.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\n\n\n\n\n\n\nExercise 1c: Interpret model output\n\n\n\n\n\nWhat is remarkable here? Explain what happened. Explain in what sense this model is “unidentifiable”.\nHint: Explore the parameters with high \\(\\hat{R}\\) values. When a model fit seems problematic, a nice tool to explore what might be amiss is the package shinystan. You could do this:\n\n\nToggle code\nshinystan::launch_shinystan(stan_fit_2c_GMM)\n\n\nThen head over to the tab “Explore” and have a look at some of the parameters.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe \\(\\hat{R}\\) values of the mean parameters are substantially above 1, suggesting that the model did not converge. But if we look at trace plots for these parameters, for example, we see that \\(\\mu_1\\) has “locked into” group A for some chains, and into group B for some other chains.\nThe model is therefore unidentifiable in the sense that, without requiring that mu is ordered, \\(\\mu_1\\) could be for group A or group B, and which one it will take on depends on random initialization. Requiring that mu be ordered, breaks this symmetry."
  },
  {
    "objectID": "practice-sheets/03d-mixture-models.html#posterior-predictive-check",
    "href": "practice-sheets/03d-mixture-models.html#posterior-predictive-check",
    "title": "XX: Mixture models",
    "section": "Posterior predictive check",
    "text": "Posterior predictive check\nWe can extend the (identifiable) model from above to also output samples from the posterior predictive distribution. This is given in file Gaussian-mixture-03-withPostPred.stan. Let’s run this model, collect the posterior predictive samples in a variable called yrep and draw a density plot.\n\n\nToggle code\nstan_fit_2d_GMM <- stan(\n  file = 'stan-files/Gaussian-mixture-03-withPostPred.stan',\n  data = data_GMM,\n  # only return the posterior predictive samples\n  pars = c('yrep')\n)\n\n\n\n\nToggle code\ntibble(\n  source  = c(rep(\"data\", length(flower_heights)), rep(\"PostPred\", length(extract(stan_fit_2d_GMM)$yrep))),\n  height = c(flower_heights, extract(stan_fit_2d_GMM)$yrep)\n) |>  \n  ggplot(aes(x = height, fill=source, color = source)) +\n  geom_density(size = 2, alpha = 0.3)\n\n\n\n\n\n\n\n\n\n\n\nExercise 1d: Scrutinize posterior predictive check\n\n\n\n\n\nDoes this look like a distribution that could have generated the data?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe distribution looks plausible enough. The visual fit in these density plots is not perfect also because we use quite a different number of samples to estimate the density."
  },
  {
    "objectID": "practice-sheets/01c-priors.html",
    "href": "practice-sheets/01c-priors.html",
    "title": "Priors in brms: inspecting, setting & sampling",
    "section": "",
    "text": "This tutorial covers how to inspect, set and sample priors in Bayesian regression models with brms. The main conceptual take-home message is: The choice of prior should be informed by their effect on the prior predictive distribution. How to obtain samples from the prior predictive distribution is covered in a separate chapter.\n\nPreamble\nHere is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\n\n\nToggle code\n# install packages from CRAN (unless installed)\npckgs_needed <- c(\n  \"tidyverse\",\n  \"brms\",\n  \"rstan\",\n  \"rstanarm\",\n  \"remotes\",\n  \"tidybayes\",\n  \"bridgesampling\",\n  \"shinystan\",\n  \"mgcv\"\n)\npckgs_installed <- installed.packages()[,\"Package\"]\npckgs_2_install <- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]\nif(length(pckgs_2_install)) {\n  install.packages(pckgs_2_install)\n} \n\n# install additional packages from GitHub (unless installed)\nif (! \"aida\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/aida-package\")\n}\nif (! \"faintr\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/faintr\")\n}\nif (! \"cspplot\" %in% pckgs_installed) {\n  remotes::install_github(\"CogSciPrag/cspplot\")\n}\n\n# load the required packages\nx <- lapply(pckgs_needed, library, character.only = TRUE)\nlibrary(aida)\nlibrary(faintr)\nlibrary(cspplot)\n\n# these options help Stan run faster\noptions(mc.cores = parallel::detectCores())\n\n# use the CSP-theme for plotting\ntheme_set(theme_csp())\n\n# global color scheme from CSP\nproject_colors = cspplot::list_colors() |> pull(hex)\n# names(project_colors) <- cspplot::list_colors() |> pull(name)\n\n# setting theme colors globally\nscale_colour_discrete <- function(...) {\n  scale_colour_manual(..., values = project_colors)\n}\nscale_fill_discrete <- function(...) {\n   scale_fill_manual(..., values = project_colors)\n}\n\n\n\n\nSet up example\nWe work with the mouse-tracking data from the aida package. As a running example, we look at the linear relation between (aggregates of) area-under-the-curve AUC and MAD. Here is the relevant data plot:\n\n\nToggle code\n# catchy name for the data\ndolphin <- aida::data_MT\n\n# create aggregate data\ndolphin_agg <- dolphin |> \n  filter(correct == 1) |> \n  group_by(subject_id) |> \n  dplyr::summarize(\n    AUC = median(AUC, na.rm = TRUE),\n    MAD = median(MAD, na.rm = TRUE))\n\ndolphin_agg |> \nggplot(aes(x = MAD, y = AUC)) + \n  geom_point(size = 3, alpha = 0.3) \n\n\n\n\n\n\n\nToggle code\n# run the model\nmodel1 = brm(\n  AUC ~ MAD, \n  data = dolphin_agg)\n\n\n\n\nInspect & change priors\nWe can inspect the priors used in in a model fit like so:\n\n\nToggle code\nbrms::prior_summary(model1)\n\n\n                          prior     class coef group resp dpar nlpar lb ub\n student_t(3, 14864.2, 32772.3) Intercept                                 \n                         (flat)         b                                 \n                         (flat)         b  MAD                            \n       student_t(3, 0, 32772.3)     sigma                             0   \n       source\n      default\n      default\n (vectorized)\n      default\n\n\nThe table gives information about the kind of prior used for different parameters. Parameters are classified into different classes (column “class”). The “b” class contains the slope coeffiecients. Here, we only have one slope (for MAD), which is identified in the “coef” column. For more complex models, the other colums may be important (e.g., identifying groups in multi-level models, for parameters in distributional and non-linear models, as well as lower and upper bounded paramters).\nThis particular output tells us that the priors for the slope coefficient for the variable MAD was “flat”. Per default, brms uses so-called improper priors for slope coefficients, i.e., not specifying any prior at all, so that every parameter value is equally weighted (even if this is not a proper probability distribution since the support is infinite).\nIn contrast, brms /does/ use more specific, in fact rather smart, priors for the intercept and for the standard deviation. These priors are informed by the data. Look:\n\n\nToggle code\ndolphin_agg |> pull(AUC) |> median()\n\n\n[1] 14864.25\n\n\nToggle code\ndolphin_agg |> pull(AUC) |> sd()\n\n\n[1] 49258.31\n\n\nWe can change the priors used to fit the model with the prior attribute and the brms::prior() function. Here, we set it to a normal (with ridiculously small standard deviation).\n\n\nToggle code\nmodel2 <- brm(\n  AUC ~ MAD, \n  data = dolphin_agg,\n  prior = brms::prior(normal(0,10), class = \"b\")\n)\n\n\nThe brms::prior() function expects the prior to be specified as a Stan expression. Full documentation of the available functions for priors is in the Stan Functions Reference.\n\n\n\n\n\n\nExercise 1a\n\n\n\n\n\nFit a third model model3 as the previous ones, but set the prior for the slope coefficient to a Student’s \\(t\\) distribution with mean 0, standard deviation 100 and one degree of freedom.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\nmodel3 <- brm(\n  AUC ~ MAD, \n  data = dolphin_agg,\n  prior = brms::prior(student_t(1,0,100), class = \"b\")\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1b\n\n\n\n\n\nCompare the mean posteriors for all three main parameters (intercept, slope for MAD and sigma) in all three models. What effect did changing the prior on the slope parameter have for models 2 and 3? Remember that the priors for these models are quite “unreasonable” in the sense that they are far away from the posterior obtained for model 1.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\nextract_info <- function(model_fit, label) {\n  tidybayes::summarise_draws(model_fit) |> \n    mutate(model = label) |> \n    select(model, variable, q5, mean, q95) |> \n    filter(variable %in% c('b_Intercept', 'b_MAD', 'sigma'))\n}\n\nrbind(\n  extract_info(model1, \"model 1\"),\n  extract_info(model2, \"model 2\"),\n  extract_info(model3, \"model 3\")\n) |> arrange(variable, model)\n\n\n# A tibble: 9 × 5\n  model   variable          q5    mean     q95\n  <chr>   <chr>          <num>   <num>   <num>\n1 model 1 b_Intercept -2599.     493.   3451. \n2 model 2 b_Intercept 14770.   22260.  29709. \n3 model 3 b_Intercept -2513.     522.   3619. \n4 model 1 b_MAD         428.     455.    483. \n5 model 2 b_MAD           4.93    21.7    38.0\n6 model 3 b_MAD         428.     454.    481. \n7 model 1 sigma       15326.   17184.  19265. \n8 model 2 sigma       42012.   47331.  53062. \n9 model 3 sigma       15318.   17176.  19258. \n\n\nWe see that the Student-t prior in model 3 gives a very similar fit as for model 1. This is likely due to the heavier tails of the Student-t distribution.\nWe also see that the more restricted model 2 has a much lower mean posterior for the slope coefficient (because this parameter is “leashed close to zero” by the prior). Instead, model 2 compensates with a much higher intercept estimate.\n\n\n\n\n\n\nThe important upshot of this exercise is that since all parameters jointly condition the likelihood function, it can happen that changing the priors for just one parameter will also affect the posterior inferences for other parameters (who have to “go out of their way” to compensate for what the other parameter can or cannot do, so to speak).\nThis raises the question of how to determine “good priors”. This is a chapter of its own, and a controversial one, and definitely a matter that depends on what you want to do with your model (explore or monkey-around, make serious predictions about the future (e.g., disease spread, market development), or draw theoretical conclusions from data (e.g., which theory of reading-times in garden-path sentences is supported better by some data)). In almost all cases, however, it is good advice to remember this: priors should be evaluated in the context of the (prior) predictions they entail. That’s the topic we attend to in the next section.\n\n\nSample from prior\nBefore going there, here is how we can obtain samples from the prior distribution over parameters of a model. Sampling from the prior only works if priors are not the improper (flat) default priors. Firstly, we can use the option sample_prior = \"only\" to obtain only samples from the prior. (NB: we still need to supply the data because it is used for the setting up the model; e.g., specifying the prior for the intercept.)\n\n\nToggle code\nmodel2_priorOnly <- brm(\n  AUC ~ MAD, \n  data = dolphin_agg,\n  prior = brms::prior(normal(0,10), class = \"b\"),\n  sample_prior = 'only'\n)\n\n\n\n\nToggle code\nmodel2_priorOnly |> tidybayes::summarise_draws() |> select(1:6)\n\n\n# A tibble: 5 × 6\n  variable          mean      median       sd      mad       q5\n  <chr>            <num>       <num>    <num>    <num>    <num>\n1 b_Intercept 13769.     14745.      56681.   37782.   -65903. \n2 b_MAD           0.0566    -0.00592     9.86     9.56    -15.9\n3 sigma       35463.     24400.      43290.   23222.     2298. \n4 lprior        -27.4      -26.9         1.72     1.38    -30.8\n5 lp__          -17.4      -17.1         1.57     1.28    -20.6\n\n\nIt is also possible to obtain a posterior fit /and/ prior samples at the same time, but that is a bit more fickle, as the prior samples will have other names, and (AFAICS) other functions are required than for posterior samples, entailing other formatting of the returned samples.\n\n\nToggle code\nmodel2_priorAdded <- brm(\n  AUC ~ MAD, \n  data = dolphin_agg,\n  prior = brms::prior(normal(0,10), class = \"b\"),\n  sample_prior = TRUE\n)\n\n\n\n\nToggle code\n# posterior samples\nmodel2_priorAdded |> tidybayes::summarise_draws() |> select(1:6)\n\n\n# A tibble: 8 × 6\n  variable               mean     median       sd       mad        q5\n  <chr>                 <num>      <num>    <num>     <num>     <num>\n1 b_Intercept     22187.      22145.      4514.    4463.     14564.  \n2 b_MAD              21.4        21.3        9.99    10.2        4.73\n3 sigma           47399.      47261.      3424.    3422.     42109.  \n4 prior_Intercept 14913.      15307.     57234.   37144.    -61565.  \n5 prior_b             0.00183     0.0210    10.1     10.1      -16.5 \n6 prior_sigma     34265.      23967.     39655.   23701.      1757.  \n7 lprior            -29.2       -28.7        2.19     2.04     -33.4 \n8 lp__            -1335.      -1334.         1.18     0.991  -1337.  \n\n\nToggle code\n# prior samples\nbrms::prior_samples(model2_priorAdded) |> summary()\n\n\n   Intercept             b                 sigma         \n Min.   :-943286   Min.   :-35.84579   Min.   :     4.3  \n 1st Qu.:  -9377   1st Qu.: -6.97042   1st Qu.: 10299.6  \n Median :  15307   Median :  0.02105   Median : 23967.2  \n Mean   :  14913   Mean   :  0.00183   Mean   : 34265.0  \n 3rd Qu.:  40688   3rd Qu.:  6.67421   3rd Qu.: 46057.8  \n Max.   : 570808   Max.   : 40.87189   Max.   :646076.3  \n\n\nA third possibility is to use stats::update() to draw additional prior samples from an already fitted object, like so:\n\n\nToggle code\n# this fit only contains priors but keeps them with the same names and structure\n# as the posterior samples in `model2`\nmodel2_priorUpdate <- stats::update(model2, sample_prior = \"only\")\n\n\n\n\nToggle code\nmodel2_priorUpdate |> tidybayes::summarise_draws() |> select(1:6)\n\n\n# A tibble: 5 × 6\n  variable         mean    median       sd      mad       q5\n  <chr>           <num>     <num>    <num>    <num>    <num>\n1 b_Intercept 13851.    15239.    57361.   37158.   -64731. \n2 b_MAD          -0.129    -0.138    10.0     10.1     -16.7\n3 sigma       36680.    25774.    40606.   23253.     2477. \n4 lprior        -27.4     -27.0       1.70     1.36    -30.7\n5 lp__          -17.4     -17.0       1.58     1.26    -20.4"
  },
  {
    "objectID": "practice-sheets/03c-distributional-models.html",
    "href": "practice-sheets/03c-distributional-models.html",
    "title": "XX: Distributional models",
    "section": "",
    "text": "The term “distributional model” is not sharply defined and not altogether common. Alternatively, one may read “models for location, scale and shape” or similar verbiage. The general idea, however, is simple: when our model has a likelihood function with additional parameters, e.g., the standard deviation \\(\\sigma\\) in the Gaussian likelihood function of a vanilla linear regression, we can not only infer these parameters, but also make them dependent, e.g., on other predictors.\nFor example, when a standard linear regression model would look like this:\n\\[\n\\begin{align*}\ny & \\sim \\mathcal{N}(\\mu, \\sigma) \\\\\n\\mu & = X \\ \\beta \\\\\n\\beta, \\sigma & \\sim \\dots \\text{some priors} \\dots\n\\end{align*}\n\\]\na simple distributional model could look like this:\n\\[\n\\begin{align*}\ny & \\sim \\mathcal{N}(\\mu, \\sigma) \\\\\n\\mu & = X \\ \\beta^{\\mu} \\\\\n\\sigma & = \\exp \\left ( X \\ \\beta^{\\sigma} \\right) \\\\\n\\beta^{\\mu}, \\beta^{\\sigma} & \\sim \\dots \\text{some priors} \\dots\n\\end{align*}\n\\]\nthereby assuming that \\(\\sigma\\) itself depends on the predictors \\(X\\) in a linear way.\n\nPreamble\nHere is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\n\n\nToggle code\n# install packages from CRAN (unless installed)\npckgs_needed <- c(\n  \"tidyverse\",\n  \"brms\",\n  \"rstan\",\n  \"rstanarm\",\n  \"remotes\",\n  \"tidybayes\",\n  \"bridgesampling\",\n  \"shinystan\",\n  \"mgcv\"\n)\npckgs_installed <- installed.packages()[,\"Package\"]\npckgs_2_install <- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]\nif(length(pckgs_2_install)) {\n  install.packages(pckgs_2_install)\n} \n\n# install additional packages from GitHub (unless installed)\nif (! \"aida\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/aida-package\")\n}\nif (! \"faintr\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/faintr\")\n}\nif (! \"cspplot\" %in% pckgs_installed) {\n  remotes::install_github(\"CogSciPrag/cspplot\")\n}\n\n# load the required packages\nx <- lapply(pckgs_needed, library, character.only = TRUE)\nlibrary(aida)\nlibrary(faintr)\nlibrary(cspplot)\n\n# these options help Stan run faster\noptions(mc.cores = parallel::detectCores())\n\n# use the CSP-theme for plotting\ntheme_set(theme_csp())\n\n# global color scheme from CSP\nproject_colors = cspplot::list_colors() |> pull(hex)\n# names(project_colors) <- cspplot::list_colors() |> pull(name)\n\n# setting theme colors globally\nscale_colour_discrete <- function(...) {\n  scale_colour_manual(..., values = project_colors)\n}\nscale_fill_discrete <- function(...) {\n   scale_fill_manual(..., values = project_colors)\n}\n\n\n\n\nExample: World temperature data\nThe World Temperature data, included in the aida package provides a useful minimal example. We want to regress avg_temp on year, but we see that early measurements appear to be much more noisy, so that a linear fit will be better for data between 1900 and 1980, and worse for data between 1750 to 1900, simply because of differences related to differently noise measurements at different times.\n\n\nToggle code\naida::data_WorldTemp |> \n  ggplot(aes(x = year, y = avg_temp)) +\n  geom_point() +\n  ylab(\"average temperature\") + geom_smooth(method = \"lm\")\n\n\n\n\n\nA simple linear regression model is easy to fit:\n\n\nToggle code\n# vanilla regression\nfit_temp_vanilla <- \n  brms::brm(\n    avg_temp ~ year,\n    data = aida::data_WorldTemp)\n\n\nBut there are clear indicators that this is not a very good model, e.g., using a posterior predictive \\(p\\)-value with the standard deviation for predictions of all data points between 1750-1800 as a test statistic:\n\n\nToggle code\npostPred_y <- \n  tidybayes::predicted_draws(\n    object  = fit_temp_vanilla,\n    newdata = aida::data_WorldTemp |> dplyr::select(year) |> filter(year <= 1800),\n    value   = \"avg_temp\",\n    ndraws  = 4000) |> ungroup() |> \n  dplyr::select(.draw, year, avg_temp)\n\nsd_postPred <- postPred_y |> \n  group_by(.draw) |> \n  summarize(sd_post_pred = sd(avg_temp)) |> \n  pull(sd_post_pred)\n\nsd_data <- aida::data_WorldTemp |> filter(year <= 1800) |> pull(avg_temp) |> sd()\n\nmean(sd_data > sd_postPred)\n\n\n[1] 1\n\n\nIf we care about faithful prediction in this early period, including accuracy about the noisiness of the data, the vanilla linear model is clearly bad.\n\n\nA simple distributional model\nWe want to fit the distributional model sketched in the beginning:\n\\[\n\\begin{align*}\ny & \\sim \\mathcal{N}(\\mu, \\sigma) \\\\\n\\mu & = X \\ \\beta^{\\mu} \\\\\n\\sigma & = \\exp \\left ( X \\ \\beta^{\\sigma} \\right) \\\\\n\\beta^{\\mu}, \\beta^{\\sigma} & \\sim \\dots \\text{some priors} \\dots\n\\end{align*}\n\\]\nTo fit this model with brms, we need to specify the formula for the regression as follows:\n\n\nToggle code\nformula_temp_distributional = brms::bf(avg_temp ~ year, sigma ~ year)\n\n\nThis formula first declares that avg_temp is to be regressed on year, as usual, and also declares that sigma is supposed to be regressed (in quite the same way) on year as well. The variable sigma does not occur in the data, but is recognized as an internal variable, namely the standard deviation of the Gaussian likelihood function.\nSampling with Stan can get troublesome if parameters are on quite different scales, so we should make sure that the two estimands are roughly on the same scale.\n\n\nToggle code\n# to run a distributional model, we need to rescale 'year'\n#   division by a factor of 1000 is sufficient\ndata_WorldTemp <- aida::data_WorldTemp |> \n  mutate(year = (year)/1000)\n\n\n\n\nToggle code\nfit_temp_distributional <- \n  brms::brm( \n    formula = brms::bf(avg_temp ~ year, sigma ~ year),\n    data    = data_WorldTemp\n  )\n\n\nThis model provides us with information about intercepts and slopes for both components, the regression of avg_temp and sigma.\n\n\nToggle code\nsummary(fit_temp_distributional)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = log \nFormula: avg_temp ~ year \n         sigma ~ year\n   Data: data_WorldTemp (Number of observations: 269) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept          -5.07      0.67    -6.36    -3.78 1.00     3099     3305\nsigma_Intercept     4.08      1.00     2.14     6.03 1.00     3104     3189\nyear                7.09      0.35     6.42     7.77 1.00     3136     3194\nsigma_year         -2.67      0.53    -3.71    -1.64 1.00     3099     3244\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nHere is a plot that shows the posterior (means and credible interval) of \\(\\sigma\\) as a function of year.\n\n\nToggle code\ntidybayes::tidy_draws(fit_temp_distributional) |> \n  dplyr::select(.draw, b_sigma_Intercept, b_sigma_year) |> \n  cross_join(tibble(year = (1750:2020) / 1000)) |>\n  mutate(sigma_predicted = exp(b_sigma_Intercept + b_sigma_year * year)) |> \n  group_by(year) |> \n  summarize(\n    lower  = tidybayes::hdi(sigma_predicted)[1],\n    mean   = mean(sigma_predicted),\n    higher = tidybayes::hdi(sigma_predicted)[2]) |> \n  ggplot(aes(x = year * 1000, y = mean)) +\n  geom_ribbon(aes(ymin=lower, ymax=higher), fill = project_colors[2], alpha = 0.2) +\n  geom_line(color = project_colors[2], linewidth = 2) +\n  xlab(\"year\") +\n  ylab(\"estimated sigma\")\n\n\n\n\n\n\n\n\n\n\n\nExercise 1a\n\n\n\n\n\nLet’s inspect how brms sets up the priors for this model:\n\n\nToggle code\nbrms::get_prior(\n  formula = brms::bf(avg_temp ~ year, sigma ~ year),\n  data    = data_WorldTemp)\n\n\n                  prior     class coef group resp  dpar nlpar lb ub\n student_t(3, 8.3, 2.5) Intercept                                  \n                 (flat)         b                                  \n                 (flat)         b year                             \n   student_t(3, 0, 2.5) Intercept                 sigma            \n                 (flat)         b                 sigma            \n                 (flat)         b year            sigma            \n       source\n      default\n      default\n (vectorized)\n      default\n      default\n (vectorized)\n\n\nUsing this information set a prior on the slope coefficient for both components of the model. Use a Student-t distribution with one degree of freedom, mean 0 and standard deviation 10.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\nprior_temp_distributional <- \n  c(prior(student_t(1,0,10), class = \"b\", coef = \"year_c\"),\n    prior(student_t(1,0,10), class = \"b\", dpar = \"sigma\")\n    )"
  },
  {
    "objectID": "practice-sheets/06-causal-inference.html",
    "href": "practice-sheets/06-causal-inference.html",
    "title": "07: Causal inference",
    "section": "",
    "text": "Here is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\n\n\nToggle code\n# install packages from CRAN (unless installed)\npckgs_needed <- c(\n  \"tidyverse\",\n  \"brms\",\n  \"rstan\",\n  \"rstanarm\",\n  \"remotes\",\n  \"tidybayes\",\n  \"bridgesampling\",\n  \"shinystan\",\n  \"mgcv\"\n)\npckgs_installed <- installed.packages()[,\"Package\"]\npckgs_2_install <- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]\nif(length(pckgs_2_install)) {\n  install.packages(pckgs_2_install)\n} \n\n# install additional packages from GitHub (unless installed)\nif (! \"aida\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/aida-package\")\n}\nif (! \"faintr\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/faintr\")\n}\nif (! \"cspplot\" %in% pckgs_installed) {\n  remotes::install_github(\"CogSciPrag/cspplot\")\n}\n\n# load the required packages\nx <- lapply(pckgs_needed, library, character.only = TRUE)\nlibrary(aida)\nlibrary(faintr)\nlibrary(cspplot)\n\n# these options help Stan run faster\noptions(mc.cores = parallel::detectCores())\n\n# use the CSP-theme for plotting\ntheme_set(theme_csp())\n\n# global color scheme from CSP\nproject_colors = cspplot::list_colors() |> pull(hex)\n# names(project_colors) <- cspplot::list_colors() |> pull(name)\n\n# setting theme colors globally\nscale_colour_discrete <- function(...) {\n  scale_colour_manual(..., values = project_colors)\n}\nscale_fill_discrete <- function(...) {\n   scale_fill_manual(..., values = project_colors)\n}"
  },
  {
    "objectID": "practice-sheets/06-causal-inference.html#case-1-gender",
    "href": "practice-sheets/06-causal-inference.html#case-1-gender",
    "title": "07: Causal inference",
    "section": "Case 1: Gender",
    "text": "Case 1: Gender\nIn the first scenario (referred to as “Case 1: Gender as a confound”), the data was collected by the following procedure:\n\n700 participants were recruited, out of which 357 identified as male and 343 identified as 350 female\neach participant decides whether or not to take a new drug\nwe observe whether the patient recovered or not\n\nHere is the data for the first scenario:\n\n\nToggle code\n##################################################\n# set up the data for SP\n##################################################\n\ndata_simpsons_paradox <- tibble(\n  gender = c(\"Male\", \"Male\", \"Female\", \"Female\"),\n  bloodP = c(\"Low\", \"Low\", \"High\", \"High\"),\n  drug   = c(\"Take\", \"Refuse\", \"Take\", \"Refuse\"),\n  k      = c(81, 234, 192, 55),\n  N      = c(87, 270, 263, 80),\n  proportion = k/N\n)\n\ndata_simpsons_paradox |> select(-bloodP)\n\n\n# A tibble: 4 × 5\n  gender drug       k     N proportion\n  <chr>  <chr>  <dbl> <dbl>      <dbl>\n1 Male   Take      81    87      0.931\n2 Male   Refuse   234   270      0.867\n3 Female Take     192   263      0.730\n4 Female Refuse    55    80      0.688\n\n\nResearch Team 1 bends over this data set and notices that the drug increases recovery for both males and females, as shown in the plot below. Based on this, Research Team 1 concludes that the drug is effective and they recommend its usage.\n\n\nToggle code\ndata_simpsons_paradox |> \n  ggplot(aes(x = drug, y = proportion, group = gender)) +\n  geom_line(size = 1.2, color = project_colors[12]) + \n  geom_point(size = 3, aes(color = gender))"
  },
  {
    "objectID": "practice-sheets/06-causal-inference.html#case-2-blood-pressure",
    "href": "practice-sheets/06-causal-inference.html#case-2-blood-pressure",
    "title": "07: Causal inference",
    "section": "Case 2: Blood pressure",
    "text": "Case 2: Blood pressure\nBut now consider a second scenario. Data was collected by the following process:\n\n700 participants were recruited\neach participant decides whether or not to take a new drug\neach patient is assigned to a high and low blood pressure group, after a blood pressure measurement (if a patient took the drug, this measurement happens after having taken the drug)\nwe observe whether each patient recovered or not\n\nThe data for this scenario look as follows:\n\n\nToggle code\ndata_simpsons_paradox |> select(-gender)\n\n\n# A tibble: 4 × 5\n  bloodP drug       k     N proportion\n  <chr>  <chr>  <dbl> <dbl>      <dbl>\n1 Low    Take      81    87      0.931\n2 Low    Refuse   234   270      0.867\n3 High   Take     192   263      0.730\n4 High   Refuse    55    80      0.688\n\n\n(Yes, you are right! The numbers are exactly the same as before!)\nResearch Team 2 bends over this data set and notices that the drug decreases recovery rate in the whole population, as shown in the plot below. Based on this, Research Team 2 concludes that the drug is not effective. They do not recommend it for usage.\n\n\nToggle code\ndata_simpsons_paradox |>\n  group_by(drug) |> \n  summarise(proportion = sum(k) / sum(N)) |> \n  ggplot(aes(x = drug, y = proportion)) +\n  geom_line(size = 1.2, aes(group = 1), color = project_colors[12]) +\n  geom_point(size = 3, aes(color = drug))"
  },
  {
    "objectID": "practice-sheets/06-causal-inference.html#whats-the-paradox",
    "href": "practice-sheets/06-causal-inference.html#whats-the-paradox",
    "title": "07: Causal inference",
    "section": "What’s the paradox?",
    "text": "What’s the paradox?\nThe puzzle here is that two research teams have reached opposite conclusions based on data which is at least numerically the exact same. Each team seems, at first glance, to have drawn reasonable conclusions. How is it possible to reach opposite conclusions about whether or not to use a drug, based on the same set of numbers?"
  },
  {
    "objectID": "practice-sheets/06-causal-inference.html#resolving-the-puzzle-by-causal-analysis",
    "href": "practice-sheets/06-causal-inference.html#resolving-the-puzzle-by-causal-analysis",
    "title": "07: Causal inference",
    "section": "Resolving the puzzle by causal analysis",
    "text": "Resolving the puzzle by causal analysis\nYou say: “The data sets are not the same! The numbers are, but in one case we observed gender and in the other we observed blood pressure. That makes a difference, doesn’t it?”\nI say: “Well, okay, but not necessarily. Just having different labels for levels of a categorical variable doesn’t make for a different data set, does it?”\nBut you immediately shoot back: “Maybe, but you also told us about a difference in the data-generating process. There is at least a temporal difference: blood pressure is measured after the treatment, but the level of gender was fixed already before the treatment.”\n“Okay” I say. “So, do you suggest a temporal analysis?”\nYou roll your eyes and after a few more (ridiculous) turns of this conversation, we both agree that, at least conceptually speaking, there is a difference in the plausible causal structure of the involved variables.\nConsider the first case. There are three binary variables involved. Given the temporal sequence of events in the data-generating process, the likely causal relation between the variables is that gender may influence both drug (the decision to take the drug or not) and recovery. Moreover, the drug may have influenced recovery directly.\n\n\n\nCausal structure of scenario 1: gender is a confound\n\n\nNow consider the second scenario. Again, we have three binary variables. But since blood pressure is measures after the treatment, it is not plausible that it could have influenced the decision of whether to take the drug or not. Reversely, it is plausible to assume, i.e., to at least allow for the possibility, that blood pressure was affected by gender and that it may have affected recovery. As before, we also make room for the possibility that drug may affect recovery also directly.\n\n\n\nCausal structure of scenario 2: blood pressure is a mediator"
  },
  {
    "objectID": "practice-sheets/06-causal-inference.html#case-1-gender-as-a-confound",
    "href": "practice-sheets/06-causal-inference.html#case-1-gender-as-a-confound",
    "title": "07: Causal inference",
    "section": "Case 1: Gender as a confound",
    "text": "Case 1: Gender as a confound\nSince the set \\(\\{G\\}\\) statisfies the backdoor criterion for the assumed causal DAG, we know that we can calculate the total causal effect (TCE) by eliminating the do-operator in the conditioning using the adjustment formula:\n\\[\nP(R=r \\mid \\mathit{do}(D=d)) = \\sum_{g \\in \\{0,1\\}} P(R=r \\mid D=d, G=g) \\ P(G=g)\n\\]\nThis means that we need to estimate two probability distributions: the conditional probability of recovery given drug and gender and the (marginal) probability of gender. We can use maximum-likelihood estimates for these by just using the observed frequencies as estimators. For \\(\\mathit{do}(D=1)\\), this yields:\n\\[\n\\begin{align*}\n& P(R=1 \\mid \\mathit{do}(D=1))\n\\\\\n= &   P(R=1 \\mid D=1, G=f) \\ P(G=f) + P(R=1 \\mid D=1, G=m) \\ P(G=m)\n\\\\\n= &   \\frac{192}{263} \\ \\frac{343}{700}\n    + \\frac{81}{87}  \\ \\frac{357}{700}\n\\\\\n= & 0.8325462\n\\end{align*}\n\\]\nFor \\(\\mathit{do}(D=0)\\), we get:\n\\[\n\\begin{align*}\n& P(R=1 \\mid \\mathit{do}(D=0))\n\\\\\n= &   P(R=1 \\mid D=0, G=f) \\ P(G=f) + P(R=1 \\mid D=0, G=m) \\ P(G=m)\n\\\\\n= &   \\frac{55}{80} \\ \\frac{343}{700}\n    + \\frac{234}{270}  \\ \\frac{357}{700}\n\\\\\n= & 0.778875\n\\end{align*}\n\\] So, an ML-estimate of the TCE would be:\n\n\nToggle code\nMLE_case1 <- 192/263 * 343/700 + 81/87 * 357/700 -\n  (55/80 * 343/700 + 234/270 * 357/700)\nMLE_case1\n\n\n[1] 0.05367122\n\n\nThis suggest that the drug is effectively increasing expected recovery, but we do not have a measure of uncertainty of this estimate. We don’t know if we should consider this convincing evidence to recommend wide-spread adoption. That’s why we need (something like) Bayesian estimation eventually. But let’s first look at the second scenario."
  },
  {
    "objectID": "practice-sheets/06-causal-inference.html#case-2-blood-pressure-as-a-mediator",
    "href": "practice-sheets/06-causal-inference.html#case-2-blood-pressure-as-a-mediator",
    "title": "07: Causal inference",
    "section": "Case 2: Blood pressure as a mediator",
    "text": "Case 2: Blood pressure as a mediator\nFor the causal graph assumed for the second scenario, the do-intervention reduces to the conditional probability:\n\\[\nP\\left(R=1 \\mid \\mathit{do}(D=d)\\right)\n= P\\left(R = 1 \\mid D=d \\right)\n\\]\nFor \\(\\mathit{do}(D=1)\\), this yields:\n\\[\nP(R=1 \\mid \\mathit{do}(D=1)) = P(R=1 \\mid D=1) = \\frac{273}{350} = 0.78\n\\]\nFor \\(\\mathit{do}(D=0)\\), we get:\n\\[\nP(R=1 \\mid \\mathit{do}(D=0)) = P(R=1 \\mid D=0) = \\frac{289}{350} = 0.8257143\n\\]\nThe ML-esimtate of the TCE is therefore:\n\n\nToggle code\nMLE_case2 <- 273/350 - 289/350\nMLE_case2\n\n\n[1] -0.04571429\n\n\nThis differs in sign from the previous estimate, and we might conclude that administering the drug is, overall, not beneficial. Yet, again, we have no uncertainty quantification regarding this estimate."
  },
  {
    "objectID": "practice-sheets/06-causal-inference.html#some-data-wrangling",
    "href": "practice-sheets/06-causal-inference.html#some-data-wrangling",
    "title": "07: Causal inference",
    "section": "Some data wrangling",
    "text": "Some data wrangling\nFor subsequent analysis (especially when generating predictive samples), it helps to have the data in long format. The uncount() function is a great tool for this.\n\n\nToggle code\n# cast into long format\ndata_SP_long <- rbind(\n  data_simpsons_paradox |> uncount(k) |> \n    mutate(recover = TRUE)  |> select(-N, -proportion),\n  data_simpsons_paradox |> uncount(N-k) |> \n    mutate(recover = FALSE) |> select(-N, -proportion, -k)\n)\ndata_SP_long\n\n\n# A tibble: 700 × 4\n   gender bloodP drug  recover\n   <chr>  <chr>  <chr> <lgl>  \n 1 Male   Low    Take  TRUE   \n 2 Male   Low    Take  TRUE   \n 3 Male   Low    Take  TRUE   \n 4 Male   Low    Take  TRUE   \n 5 Male   Low    Take  TRUE   \n 6 Male   Low    Take  TRUE   \n 7 Male   Low    Take  TRUE   \n 8 Male   Low    Take  TRUE   \n 9 Male   Low    Take  TRUE   \n10 Male   Low    Take  TRUE   \n# ℹ 690 more rows"
  },
  {
    "objectID": "practice-sheets/06-causal-inference.html#case-1-gender-as-a-confound-1",
    "href": "practice-sheets/06-causal-inference.html#case-1-gender-as-a-confound-1",
    "title": "07: Causal inference",
    "section": "Case 1: Gender as a confound",
    "text": "Case 1: Gender as a confound\nGiven the causal structure assumed for scenario 1, we can calculate the effects of the relevant do-intervention as:\n\\[\nP(R=1 \\mid \\mathit{do}(D=d)) = \\sum_{g \\in \\{0,1\\}} P(R=1 \\mid D=d, G=g) \\ P(G=g)\n\\]\nTherefore, we need to do three things:\n\nWe estimate \\(P(R=1 \\mid D=d, G=g)\\). This can be done with a logistic regression model, regressing \\(R\\) on \\(D\\) and \\(G\\).\nWe estimate \\(P(G)\\), which we can do with an intercept-only logistic regression model.\nWe calculate the TCE based on samples from the posterior predictive distributions of these models.\n\n\nStep 1: Intercept-only model for gender\nHere is an intercept-only logistic regression model for gender:\n\n\nToggle code\nniter = 2000\n\nfit_SP_GonIntercept <- brm(\n  formula = gender ~ 1,\n  data    = data_SP_long,\n  family  = bernoulli(link = \"logit\"),\n  iter    = niter\n)\n\n\nEach sample from the posterior of the Intercept parameter represents (a guess of) the log-odds of the Male category. The posterior over the proportion of male participants can therefore be retrieved and plotted as follows (the yellow line shows the observed frequency):\n\n\nToggle code\nlogistic <- function(x) {\n  1 / (1 + exp(-x))\n}\n\nposterior_SP_GonIntercept <- tidybayes::tidy_draws(fit_SP_GonIntercept) |> \n  mutate(prop_male = logistic(b_Intercept)) |> \n  select(prop_male)\n\nposterior_SP_GonIntercept |> \n  ggplot(aes(x = prop_male)) + \n  tidybayes::stat_halfeye() +\n  geom_vline(aes(xintercept = 357/700), color = project_colors[3]) +\n  xlab(\"proportion males\") + \n  ylab(\"posterior density\")\n\n\n\n\n\n\n\nStep 2: Regressing \\(R\\) against \\(G\\) and \\(D\\)\nNext, we regress recover on drug and gender.\n\n\nToggle code\nfit_SP_RonGD <- brm(\n  formula = recover ~ gender * drug,\n  data    = data_SP_long,\n  family  = bernoulli(link = \"logit\"),\n  iter    = niter\n)\n\n\n\n\nStep 3: Compute the posterior expectations and the TCE\nIn a third step, we draw posterior predictive samples for the model from step 2, based on posterior predictive samples for the model from step 1, while manually setting drug to Take and Refuse.\nFirst, we get posterior predictive samples of gender from the model from step 1. Notice that these are just samples of Male and Female, for which we will generate predictions based on the the second model.\n\n\nToggle code\npostPred_gender <- tidybayes::predicted_draws(\n  object  = fit_SP_GonIntercept,\n  newdata = tibble(Intercept = 1),\n  value   = \"gender\",\n  ndraws  = niter * 2\n  ) |> \n  ungroup() |> \n  mutate(gender = ifelse(gender, \"Male\", \"Female\")) |> \n  select(gender)\n\n# NB: in this case we could also have gotten this via: \n# rbinom(n=4000, p=rbeta(4000, 315+1, 700-315+1), size = 700)\n\npostPred_gender\n\n\n# A tibble: 4,000 × 1\n   gender\n   <chr> \n 1 Female\n 2 Female\n 3 Male  \n 4 Male  \n 5 Female\n 6 Male  \n 7 Female\n 8 Female\n 9 Male  \n10 Female\n# ℹ 3,990 more rows\n\n\nBased on these ‘sampled individuals’ we generate the prediction of the second model (predicting the a posteriori expected recovery, given gender and whether to take the drug or not).\n\n\nToggle code\n# posterior predictive samples for D=1\nposterior_DrugTaken <- tidybayes::epred_draws(\n  object  = fit_SP_RonGD,\n  newdata = postPred_gender |> mutate(drug = \"Take\"),\n  value   = \"taken\",\n  ndraws  = niter * 2\n) |> ungroup() |> \n  select(taken)\n\n# posterior predictive samples for D=0\nposterior_DrugRefused <- tidybayes::epred_draws(\n  object  = fit_SP_RonGD,\n  newdata = postPred_gender |> mutate(drug = \"Refuse\"),\n  value   = \"refused\",\n  ndraws  = niter * 2\n) |> ungroup() |> \n  select(refused)\n\n\nTo calculate the TCE we look at the difference in predicted recovery rate:\n\n\nToggle code\nCE_post <- cbind(posterior_DrugTaken, posterior_DrugRefused) |> \n  mutate(causal_effect = taken - refused) \n\nrbind(\n  aida::summarize_sample_vector(CE_post$taken, \"drug_taken\"),\n  aida::summarize_sample_vector(CE_post$refused, \"drug_refused\"),\n  aida::summarize_sample_vector(CE_post$causal_effect, \"causal_effect\")\n)\n\n\n# A tibble: 3 × 4\n  Parameter      `|95%`   mean `95%|`\n  <chr>           <dbl>  <dbl>  <dbl>\n1 drug_taken     0.690  0.832   0.973\n2 drug_refused   0.614  0.778   0.907\n3 causal_effect -0.0516 0.0543  0.142\n\n\nThis yields a point estimate (Bayesian mean posterior) and the usual uncertainty quantification in terms of credible intervals etc. In this case, we would not be compelled to conclude that the causal effect is substantial as the posterior estimate for this effect clearly encompasses non-negligible mass for the range of negative values. The orange line show the maximum-likelihood estimate of the causal effect.\n\n\nToggle code\nCE_post |> \n  ggplot(aes(x = causal_effect)) +\n  tidybayes::stat_halfeye() +\n  geom_vline(aes(xintercept = MLE_case1), color = project_colors[3])\n\n\n\n\n\n\n\n\n\n\n\nExercise 1: Alternative calculation of causal effect estimate\n\n\n\n\n\nThe last plot looks a bit ragged. This is because we approximate \\(P(G)\\) by actual samples of levels Male and Female. There is an alternative, though. Both approaches are correct. But they differ slightly in logic and execution, so you may benefit from having seen both.\nIn this new approach you do this:\n\nApproximate \\(P(G)\\) by taking samples from the expected value of \\(P(G=M)\\). Use the function tidybayes::epred_draws() for this.\nSample expected values of \\(P(R=1 \\mid G, D)\\) for all combinations of levels of \\(G\\) and \\(D\\).\nWeigh the predicted recovery rates from step 2 with the corresponding predictions of \\(P(G)\\) from step 1.\nCompute the causal effect from this.\n\nCompute the ususal summary statistics (posterior mean, credible interval) and plot the posterior for the estimated causal effect.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\n# get epred samples for intercept-only model:\npostPred_maleProportion <- tidybayes::epred_draws(\n  fit_SP_GonIntercept, \n  newdata = tibble(Intercept = 1),\n  value = \"maleProp\",\n  ndraws  = niter * 2\n  ) |> ungroup() |> \n  select(.draw, maleProp)\n\n#  get epred samples for the R ~ D, G model\n#   for all combinations of D and G\nposterior_DrugTaken <- tidybayes::epred_draws(\n  object  = fit_SP_RonGD,\n  newdata = tibble(gender = c(\"Male\", \"Male\", \"Female\", \"Female\"),\n                   drug   = c(\"Take\", \"Refuse\", \"Take\", \"Refuse\")),\n  value   = \"recovery\",\n  ndraws  = niter * 2\n) |> ungroup() |> \n  select(.draw, gender, drug, recovery)\n\n# weigh and compute the causal effect\nCE_post <- posterior_DrugTaken |> full_join( postPred_maleProportion) |> \n  mutate(weights = ifelse(gender == \"Male\", maleProp, 1-maleProp)) |> \n  group_by(`.draw`, drug) |> \n  summarize(predRecover = sum(recovery * weights)) |> \n  pivot_wider(names_from = drug, values_from = predRecover) |> \n  mutate(causal_effect = Take - Refuse) \n\n# produce summary statistics\nrbind(\n  aida::summarize_sample_vector(CE_post$Take, \"drug_taken\"),\n  aida::summarize_sample_vector(CE_post$Refuse, \"drug_refused\"),\n  aida::summarize_sample_vector(CE_post$causal_effect, \"causal_effect\")\n)\n\n\n# A tibble: 3 × 4\n  Parameter       `|95%`   mean `95%|`\n  <chr>            <dbl>  <dbl>  <dbl>\n1 drug_taken     0.792   0.832   0.866\n2 drug_refused   0.724   0.778   0.830\n3 causal_effect -0.00664 0.0542  0.121\n\n\nToggle code\n# plot the relevant posterior\nCE_post |> \n  ggplot(aes(x = causal_effect)) +\n  tidybayes::stat_halfeye()"
  },
  {
    "objectID": "practice-sheets/06-causal-inference.html#case-2-blood-pressure-as-a-mediator-1",
    "href": "practice-sheets/06-causal-inference.html#case-2-blood-pressure-as-a-mediator-1",
    "title": "07: Causal inference",
    "section": "Case 2: Blood pressure as a mediator",
    "text": "Case 2: Blood pressure as a mediator\nFor the second case, blood pressure as a mediator, the calculations are much easier. We just need to estimate \\(P(R \\mid D, B)\\), so a single regression model will do.\n\n\nToggle code\nfit_SP_RonBD <- brms::brm(\n  formula = recover ~ drug,\n  data    = data_SP_long,\n  family  = bernoulli(link = \"logit\"),\n  iter    = niter\n)\n\n\nThe coefficients of a logistic regression model relate to log-odds. Using the faintr package and the logistic transformation, we can calculate samples of the causal effect as follows:\n\n\nToggle code\nposterior_DrugTaken <- \n  faintr::extract_cell_draws(fit_SP_RonBD, drug == \"Take\") |> \n  pull(draws) |> \n  logistic()\n\nposterior_DrugRefused <- \n  faintr::extract_cell_draws(fit_SP_RonBD, drug == \"Refuse\") |> \n  pull(draws) |> \n  logistic()\n\nposterior_causalEffect <- \n  posterior_DrugTaken - posterior_DrugRefused\n\nrbind(\n  aida::summarize_sample_vector(posterior_DrugTaken, \"drug_taken\"),\n  aida::summarize_sample_vector(posterior_DrugRefused, \"drug_refused\"),\n  aida::summarize_sample_vector(posterior_causalEffect, \"causal_effect\")\n)\n\n\n# A tibble: 3 × 4\n  Parameter     `|95%`    mean `95%|`\n  <chr>          <dbl>   <dbl>  <dbl>\n1 drug_taken     0.737  0.780  0.821 \n2 drug_refused   0.787  0.825  0.863 \n3 causal_effect -0.105 -0.0457 0.0124\n\n\nHere is a density plot of the posterior samples. The orange line show the maximum-likelihood estimate of the causal effect.\n\n\nToggle code\ntibble(CE = posterior_causalEffect) |> \n  ggplot(aes(x = CE)) +\n  tidybayes::stat_halfeye() +\n  geom_vline(aes(xintercept = MLE_case2), color = project_colors[3])\n\n\n\n\n\n\n\n\n\n\n\nExercise 2: Using poterior predictives to estimate causal effects\n\n\n\n\n\nThe last section computed estimates of the relevant causal effect directly from the samples for model coefficients. This works well for logistic regression, but in other cases it may be more convenient to use samples from the posterior predictive distribution of the R ~ D, G model, similar to what we did in the first scenario.\nSo, use tidybayes::epred_draws() to get estimates of the causal effect.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\n# get posterior predictive samples & compute causal effect\nCE_post2 <- tidybayes::epred_draws(\n  fit_SP_RonBD,\n  newdata = tibble(drug   = c(\"Take\", \"Refuse\")),\n  ndraws  = niter * 2\n) |> \n  pivot_wider(id_cols = `.draw`, names_from = drug, values_from = `.epred`) |> \n  mutate(causal_effect = Take - Refuse)\n\n# produce summary statistics\nrbind(\n  aida::summarize_sample_vector(CE_post2$Take, \"drug_taken\"),\n  aida::summarize_sample_vector(CE_post2$Refuse, \"drug_refused\"),\n  aida::summarize_sample_vector(CE_post2$causal_effect, \"causal_effect\")\n)\n\n\n# A tibble: 3 × 4\n  Parameter     `|95%`    mean `95%|`\n  <chr>          <dbl>   <dbl>  <dbl>\n1 drug_taken     0.737  0.780  0.821 \n2 drug_refused   0.787  0.825  0.863 \n3 causal_effect -0.105 -0.0457 0.0124\n\n\nToggle code\n# plot posterior\nCE_post2 |> \n  ggplot(aes(x = causal_effect)) +\n  tidybayes::stat_halfeye() +\n  geom_vline(aes(xintercept = MLE_case2), color = project_colors[3])\n\n\n\n\n\n\n\n\n\n\n\n TODO: include direct effect computation for mediator case"
  },
  {
    "objectID": "practice-sheets/02a-hierarchical-models-tutorial.html",
    "href": "practice-sheets/02a-hierarchical-models-tutorial.html",
    "title": "Hierarchical regression models (tutorial)",
    "section": "",
    "text": "Here is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\n\n\nToggle code\n# install packages from CRAN (unless installed)\npckgs_needed <- c(\n  \"tidyverse\",\n  \"brms\",\n  \"rstan\",\n  \"rstanarm\",\n  \"remotes\",\n  \"tidybayes\",\n  \"bridgesampling\",\n  \"shinystan\",\n  \"mgcv\"\n)\npckgs_installed <- installed.packages()[,\"Package\"]\npckgs_2_install <- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]\nif(length(pckgs_2_install)) {\n  install.packages(pckgs_2_install)\n} \n\n# install additional packages from GitHub (unless installed)\nif (! \"aida\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/aida-package\")\n}\nif (! \"faintr\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/faintr\")\n}\nif (! \"cspplot\" %in% pckgs_installed) {\n  remotes::install_github(\"CogSciPrag/cspplot\")\n}\n\n# load the required packages\nx <- lapply(pckgs_needed, library, character.only = TRUE)\nlibrary(aida)\nlibrary(faintr)\nlibrary(cspplot)\n\n# these options help Stan run faster\noptions(mc.cores = parallel::detectCores())\n\n# use the CSP-theme for plotting\ntheme_set(theme_csp())\n\n# global color scheme from CSP\nproject_colors = cspplot::list_colors() |> pull(hex)\n# names(project_colors) <- cspplot::list_colors() |> pull(name)\n\n# setting theme colors globally\nscale_colour_discrete <- function(...) {\n  scale_colour_manual(..., values = project_colors)\n}\nscale_fill_discrete <- function(...) {\n   scale_fill_manual(..., values = project_colors)\n}\n\n\n\n\nToggle code\ndolphin <- aida::data_MT\nmy_scale <- function(x) c(scale(x))\n\n\nThis tutorial takes you through one practical example, showing the use of multilevel models. The main learning goals are:\n\nlearning how to implement multilevel linear models with brms including\nunderstanding random intercept models\nunderstanding random slope models\n\n\n\nOne way to motivate multi-level modeling is by noting that, without group-level terms, the model would be making strong (possibly) implausible independence assumptions.\nAs a motivating example, let us look at the probability of observing a straight trajectory predicted by response latency in the mouse tracking data set. Here a the plot for all data in the click group plus a logistic smooth term:\n\n\nToggle code\n# set up data frame\ndolphin_agg <- dolphin %>% \n  filter(correct == 1,\n         group == \"click\") %>% \n  mutate(straight = as.factor(ifelse(prototype_label == \"straight\", 1, 0)),\n         log_RT_s = my_scale(log(RT)),\n         AUC_s = my_scale(AUC))\n\ndolphin_agg$straight_numeric <- as.numeric(as.character(dolphin_agg$straight))\n\n# plot predicted values against data\nggplot(data = dolphin_agg,\n       aes(x = log_RT_s, y = straight_numeric)) +\n  geom_point(position = position_jitter(height = 0.02), alpha = 0.2) +\n  geom_smooth(method = \"glm\", color = project_colors[2],\n    method.args = list(family = \"binomial\"), \n    se = FALSE, fullrange = TRUE) +\n  ggtitle(\"overall relationship\") +\n  theme(legend.position = \"right\")\n\n\n\n\n\nThis picture suggest a negative relationship between the probability of observing straight trajectories (y) and people’s response times (x) (i.e. line goes down).\nBut this analysis looked at all responses at once and disregarded that responses came from groups of sources. For example, responses that come from one and the same participant are dependent on each other because participants (subject_id) might differ in characteristics relevant to the task, like how fast they move and how many times they move to the target in a straight trajectory. Another group of data points is related to different stimuli (exemplars). Different stimuli might have some inherent properties that lead to different response times and different proportions of straight trajectories. So analyzing the data without telling the model about these groups violates an important assumption of linear models. The independence assumption.\nLet’s look at these groups individually, starting by aggregating over over subject_ids and exemplars and plot the results.\n\n\nToggle code\n# aggregate over subjects\ndolphin_agg2 <- dolphin_agg %>% \n  group_by(subject_id) %>% \n  summarize(log_RT_s = mean(log_RT_s),\n            straights = sum(straight_numeric),\n            total = n()) \n\n# plot predicted values for subjects\nggplot(data = dolphin_agg2,\n       aes(x = log_RT_s, y = straights/total)) +\n  geom_point(size = 2, alpha = 0.5) +\n  # we use the geom_smooth function here as a rough proxy of the relationship \n  geom_smooth(method = \"glm\", \n              formula = y ~ x, color = project_colors[2],\n    method.args = list(family = \"binomial\"), \n    se = FALSE, fullrange = TRUE) +\n  ylab(\"Proportion of straight trajs\") +\n  ylim(0,1) +\n  xlim(-2.5,2.5) +\n  ggtitle(\"subject aggregates\") +\n  theme(legend.position = \"right\")\n\n\n\n\n\nHuh. That is interesting. So if we aggregate over subjects, i.e. each data point is one subject reacting to all exemplars, we get a positive relationship between response latency and the proportion of straight trajectories. The slower the reaction the more likely a straight trajectory. That could mean that those participants that are generally slower are also the ones that tend to move more often in a straight fashion. It also makes sense to some extent. Maybe those participants seem to wait until they have made their decision and then move to the target immediately, while other participants move upwards right away and make their decision on the fly during the decision.\nNow, let’s aggregate over exemplars:\n\n\nToggle code\n# aggregate over exemplars\ndolphin_agg3 <- dolphin_agg %>% \n  group_by(exemplar) %>% \n  summarize(log_RT_s = mean(log_RT_s),\n            straights = sum(straight_numeric),\n            total = n()) \n\n# plot predicted values for exemplars\nggplot(data = dolphin_agg3,\n       aes(x = log_RT_s, y = straights/total)) +\n  geom_point(size = 2, alpha = 0.5) +\n  geom_smooth(method = \"glm\", \n              formula = y ~ x, color = project_colors[2],\n    method.args = list(family = \"binomial\"), \n    se = FALSE, fullrange = TRUE) +\n  ylab(\"Proportion of straight trajs\") +\n  ylim(0,1) +\n  xlim(-2.5,2.5) +\n  ggtitle(\"stimuli aggregates\") +\n  theme(legend.position = \"right\")\n\n\n\n\n\nIf we look at the stimuli aggregates, i.e. each data point is one exemplar that all subjects have reacted to, we get a negative relationship between response latency and the proportion of straight trajectories. The quicker the reaction the more likely a straight trajectory. This could potentially reflect the difficulty of the categorization task. Maybe those exemplars that are inherently less ambiguous, for example the typical exemplars, don’t exhibit any response competition and are thus faster and more often straight.\nUltimately, we use our models to make a generalizing statement about a population. If our theory predicts a relationship between straight trajectories and response latency (without further nuance), we should find this relationship across the population of people AND the population of stimuli. But if we say, “there are more straight trajectories in faster responses”, this claim seems to be only true for within-participant behavior. So we need to inform our models about such groupings in our data, or we might overconfidently make predictions."
  },
  {
    "objectID": "practice-sheets/05a-model-comparison.html",
    "href": "practice-sheets/05a-model-comparison.html",
    "title": "06: Model comparison",
    "section": "",
    "text": "Here is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\n\n\nToggle code\n# install packages from CRAN (unless installed)\npckgs_needed <- c(\n  \"tidyverse\",\n  \"brms\",\n  \"rstan\",\n  \"rstanarm\",\n  \"remotes\",\n  \"tidybayes\",\n  \"bridgesampling\",\n  \"shinystan\",\n  \"mgcv\"\n)\npckgs_installed <- installed.packages()[,\"Package\"]\npckgs_2_install <- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]\nif(length(pckgs_2_install)) {\n  install.packages(pckgs_2_install)\n} \n\n# install additional packages from GitHub (unless installed)\nif (! \"aida\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/aida-package\")\n}\nif (! \"faintr\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/faintr\")\n}\nif (! \"cspplot\" %in% pckgs_installed) {\n  remotes::install_github(\"CogSciPrag/cspplot\")\n}\n\n# load the required packages\nx <- lapply(pckgs_needed, library, character.only = TRUE)\nlibrary(aida)\nlibrary(faintr)\nlibrary(cspplot)\n\n# these options help Stan run faster\noptions(mc.cores = parallel::detectCores())\n\n# use the CSP-theme for plotting\ntheme_set(theme_csp())\n\n# global color scheme from CSP\nproject_colors = cspplot::list_colors() |> pull(hex)\n# names(project_colors) <- cspplot::list_colors() |> pull(name)\n\n# setting theme colors globally\nscale_colour_discrete <- function(...) {\n  scale_colour_manual(..., values = project_colors)\n}\nscale_fill_discrete <- function(...) {\n   scale_fill_manual(..., values = project_colors)\n}\n\n\n\n\nToggle code\ndolphin <- aida::data_MT\nrerun_models = FALSE"
  },
  {
    "objectID": "practice-sheets/05a-model-comparison.html#normal-and-robust-regression-models",
    "href": "practice-sheets/05a-model-comparison.html#normal-and-robust-regression-models",
    "title": "06: Model comparison",
    "section": "Normal and robust regression models",
    "text": "Normal and robust regression models\nA normal regression model uses a normal error function.\n\n\nToggle code\nfit_n <- brm(\n  formula = y ~ x,\n  data = data_robust,\n  # student prior for slope coefficient\n  prior = prior(\"student_t(1,0,30)\", class = \"b\"),\n)\n\n\nWe will want to compare this normal regression model with a robust regression model, which uses a Student’s t distribution instead as the error function around the linear predictor:\n\n\nToggle code\nfit_r <- brm(\n  formula = y ~ x,\n  data = data_robust,\n  # student prior for slope coefficient\n  prior = prior(\"student_t(1,0,30)\", class = \"b\"),\n  family = student()\n)\n\n\nLet’s look at the posterior inferences of both models about the true (known) parameters of the regression line:\n\n\nToggle code\nprep_summary <- function(fit, model) {\n  tidybayes::summarise_draws(fit) |> \n    mutate(model = model) |> \n    select(model, variable, q5, mean, q95) |> \n    filter(grepl(variable, pattern = '^b'))  \n}\n\nrbind(prep_summary(fit_n, \"normal\"), prep_summary(fit_r, \"robust\"))\n\n\n# A tibble: 4 × 5\n  model  variable       q5  mean   q95\n  <chr>  <chr>       <num> <num> <num>\n1 normal b_Intercept  2.32  7.65 13.0 \n2 normal b_x          6.83 13.4  19.7 \n3 robust b_Intercept  1.81  2.49  3.22\n4 robust b_x          5.02  6.09  7.21\n\n\nRemember that the true intercept is 2 and the true slope is 4. Clearly the robust regression model has recovered the ground-truth parameters much better."
  },
  {
    "objectID": "practice-sheets/05a-model-comparison.html#leave-one-out-cross-validation",
    "href": "practice-sheets/05a-model-comparison.html#leave-one-out-cross-validation",
    "title": "06: Model comparison",
    "section": "Leave-one-out cross validation",
    "text": "Leave-one-out cross validation\nWe can use the loo package to compare these two models based on their posterior predictive fit. Here’s how:\n\n\nToggle code\nloo_comp <- loo_compare(list(normal = loo(fit_n), robust = loo(fit_r)))\nloo_comp\n\n\n       elpd_diff se_diff\nrobust    0.0       0.0 \nnormal -131.8      26.0 \n\n\nWe see that the robust regression model is better by ca. -132 points of expected log predictive density. The table shown above is ordered with the “best” model on top. The column elpd_diff lists the difference in ELPD of every model to the “best” one. In our case, th estimated ELPD difference has a standard error of about 26. Computing a \\(p\\)-value for this using Lambert’s \\(z\\)-score method, we find that this difference is “significant” (for which we will use other terms like “noteworthy” or “substantial” in the following):\n\n\nToggle code\n1 - pnorm(-loo_comp[2,1], loo_comp[2,2])\n\n\n[1] 0\n\n\nWe conclude from this that the robust regression model is much better at predicting the data (from a posterior point of view)."
  },
  {
    "objectID": "practice-sheets/05a-model-comparison.html#bayes-factor-model-comparison-with-bridge-sampling",
    "href": "practice-sheets/05a-model-comparison.html#bayes-factor-model-comparison-with-bridge-sampling",
    "title": "06: Model comparison",
    "section": "Bayes factor model comparison (with bridge sampling)",
    "text": "Bayes factor model comparison (with bridge sampling)\nWe use bridge sampling, as implemented in the formidable bridgesampling package, to estimate the (log) marginal likelihood of each model. To do this, we need also samples from the prior. To do this reliably, we need many more samples than we would normally need for posterior inference. We can update() existing fitted models, so that we do not have to copy-paste all specifications (formula, data, prior, …) each time. It’s important for bridge_sampler() to work that we save all parameters (including prior samples).\n\n\nToggle code\nif (rerun_models) {\n  # refit normal model\n  fit_n_4Bridge <- update(\n    fit_n,\n    iter = 5e5,\n    save_pars = save_pars(all = TRUE)\n  )\n  # refit robust model\n  fit_r_4Bridge <- update(\n    fit_r,\n    iter = 5e5,\n    save_pars = save_pars(all = TRUE)\n  )\n  normal_bridge <- bridge_sampler(fit_n_4Bridge, silent = T)\n  write_rds(normal_bridge, \"05-normal_bridge.rds\")\n  robust_bridge <- bridge_sampler(fit_r_4Bridge, silent = T)  \n  write_rds(robust_bridge, \"05-robust_bridge.rds\")\n} else {\n  normal_bridge <- read_rds(\"05-normal_bridge.rds\")  \n  robust_bridge <- read_rds(\"05-robust_bridge.rds\")\n}\n\nbf_bridge <- bridgesampling::bf(robust_bridge, normal_bridge)\n\n\nWe can then use the bf (Bayes factor) method from the bridgesampling package to get the Bayes factor (here: in favor of the robust regression model):\n\n\nToggle code\nbf_bridge\n\n\nEstimated Bayes factor in favor of robust_bridge over normal_bridge: 41136407426471809154394622543225576928422395904.00000\n\n\nAs you can see, this is a very clear result. If we had equal levels of credence in both models, after seeing the data, our degree of belief in the robust regression model should … well, virtually infinitely higer than our degree of belief in the normal model."
  },
  {
    "objectID": "01-basics.html",
    "href": "01-basics.html",
    "title": "Bayesian Regression: Theory & Practice",
    "section": "",
    "text": "The first part introduces the basics of BDA and linear regression modeling.\nHere are the slides on regression modeling.\nFollow up slides on priors and predictives.\nThere are five mostly self-contained tutorials, covering:\n\nbasics of data wrangling (in the tidyverse) and plotting (with ggplot2),\nsimple linear regression models (with brms),\nhow to specify priors in brms,\nwhy and how to inspect a model’s predictive distribution(s),\ncontrast coding of categorical predictor variables.\n\nThere is also a sheet with additional exercises (simple linear regression with metric and categorical variables).\nThere is also a sort of “cheat sheet” with common operations and functions when working with brms."
  },
  {
    "objectID": "03-GLM.html",
    "href": "03-GLM.html",
    "title": "Bayesian Regression: Theory & Practice",
    "section": "",
    "text": "This unit introduces generalized linear models, mixture models and distributional models.\nHere are slides for this session."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian Regression: Theory & Practice",
    "section": "",
    "text": "This site provides material for an intermediate level course on Bayesian linear regression modeling. The course presupposes some prior exposure to statistics and some acquaintance with R."
  },
  {
    "objectID": "index.html#intended-audience",
    "href": "index.html#intended-audience",
    "title": "Bayesian Regression: Theory & Practice",
    "section": "Intended audience",
    "text": "Intended audience\nThis course is designed for people who have completed a first, introductory course on data analysis, which has conveyed roughly the following:\n\nbasic knowledge of R and, ideally, the tidyverse\nbasic familiarity with Bayesian reasoning (prior, likelihood, posterior)\nsome prior exposure to regression modeling (Bayesian or otherwise)"
  },
  {
    "objectID": "index.html#scope",
    "href": "index.html#scope",
    "title": "Bayesian Regression: Theory & Practice",
    "section": "Scope",
    "text": "Scope\nThe aim of this course is to increase students overview over topics relevant for intermediate to advanced Bayesian regression modeling. The course focuses on Bayesian multi-level generalized linear models as implemented in the brms package. It covers, among other things, the following theoretical and practical aspects:\n\nprior and posterior model checking\ngeneralized linear models (ordered logit, multinomial, Poisson, Beta …)\nMCMC methods (HMC diagnostics)\ndistributional and non-linear models (GAMs, Gaussian processes)\nmodel comparison (Bayes factors, cross-validation)"
  },
  {
    "objectID": "index.html#additional-material",
    "href": "index.html#additional-material",
    "title": "Bayesian Regression: Theory & Practice",
    "section": "Additional material",
    "text": "Additional material\nAn even more basic introduction to data analysis (introducing R, tidyverse, Bayesian and, eventually, also frequentist statistics) is the webbook “An introduction to Data Analysis”. This course presupposes roughly the content covered in Chapters 2–9 and 12–13."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Bayesian Regression: Theory & Practice",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nPart of the hands-on material (wrangling, plotting, simple regression modeling) was used in a previous course, co-taught with the great Timo Roettger. My gratitude for his permission to build on it here. The tutorial on contrast coding was first authored by Polina Tsvilodub."
  },
  {
    "objectID": "04-MCMC.html",
    "href": "04-MCMC.html",
    "title": "Bayesian Regression: Theory & Practice",
    "section": "",
    "text": "The success of Bayesian statistics is in large part the fruit of very clever algorithms and efficient implementations for drawing samples from complex, high-dimensional posterior distributions. This unit covers:\n\nMarkov Chain Monte Carlo methods, in particular:\n\nsimple Metropolis-Hastings and\nHamiltonian Monte Carlo\n\ncommon notions and diagnostics for assessing the quality of MCMC samples, such as:\n\n\\(\\hat{R}\\)\nautocorrelation\neffective sample size\ntraceplots\ndivergent transitions\n\ncontrol parameters for brms model fits\n\nWe also take a peak at the Stan programming language.\nHere are the slides for this session"
  },
  {
    "objectID": "02-hierarchical-models.html",
    "href": "02-hierarchical-models.html",
    "title": "Bayesian Regression: Theory & Practice",
    "section": "",
    "text": "This unit introduces hierarchical regression models (also known as mixed models, or random-effect models).\nHere are slides for this session.\nThere are also tutorials covering multi-membership group-level effects and an alternative motivation for using multi-level modeling which is not “breaking the wrong independence assumptions”."
  },
  {
    "objectID": "05-model-comparison.html",
    "href": "05-model-comparison.html",
    "title": "Bayesian Regression: Theory & Practice",
    "section": "",
    "text": "The three main things we can do with models are:\n\ninferring (credible) parameter values (on the assumption that the model we use is good (enough))\nmaking predictions (from an ex ante (a priori) or ex post (a posteriori)) point of view\n\npredictions can also be used for model checking (also known as model criticism)\n\ncomparing which of several models is better (in some sense of “better”)\n\nWe have so far looked only at the first two “pillars of Bayesian data analysis”. In this unit we look at the third, model comparison.\nModel comparison is deeply related to the second point: making predictions. We compare models based on their ability to predict data well enough. But not exclusively! We might also rely on other aspects, such as whether a model makes fewer spurious assumptions, i.e., is simpler, more economical or more parsimonious.\nAs usual, there is not one criterion for model comparison that everybody unanimously agrees to as the best. As usual, this is likely because “goodness of a model” is a multi-dimensional concept. What counts as a good model for science (knowledge gain; theoretical understanding) need not be the same as for engineering or application (getting high-quality predictions in the most efficient manner).\nThis unit therefore centers on two important tools for Bayesian model comparison that lie at opposite ends of a continuous spectrum, namely Bayes factors and leave-one-out (LOO) cross validation. Bayes factors take the most extreme point of view of ex ante predictions: we compare models without any updating on the relevant data. LOO-CV, on the other hand, take the (almost) most extreme point of view of ex post predictions, comparing models that are trained on all of the relevant data, except one single data observations.\nIn the practical session, we explore how these different perspectives can give rise to opposite results. This is not a puzzle or paradox, and maybe not even something to quarrel about. It is a natural reflex of comparing the same objects based on a different task, namely making predictions before training, or after.\nHere are slides for session 6."
  },
  {
    "objectID": "xx-priorPredictives-categPredictors.html",
    "href": "xx-priorPredictives-categPredictors.html",
    "title": "Bayesian Regression: Theory & Practice",
    "section": "",
    "text": "Part 2: Categorical predictors\nMore on this topic can be found in this chapter of the webbook."
  }
]